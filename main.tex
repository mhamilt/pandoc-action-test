%------------------------------------------------------------
% Pull in global LaTex files

\documentclass[11pt,twoside,a4paper,english]{book}

\usepackage[tc]{titlepic}

\usepackage{setspace}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{cases}
%\usepackage{epic}
%\usepackage{graphpap}
%\usepackage{makeidx}
\usepackage{framed}
\usepackage{url}
\usepackage{cases}

\usepackage{babel}
\usepackage[useregional]{datetime2}


%%%%%%%%%%%%%%%%%
%%% SET PAGE MARGINS %%%
%%%%%%%%%%%%%%%%%
\usepackage[hmargin={2cm,2cm},tmargin=3cm,bmargin=3cm,headheight=1cm,headsep=0.5cm,footskip=1.5cm]{geometry}
%%%%%%%%%%%%%%%%%%
%%% BEGIN PAGE HEADER %%%
%%%%%%%%%%%%%%%%%%
%\pagestyle{fancy}
%\fancyhead{}
% \fancyfoot{}
% \fancyfoot[R]{Universit{\`a di Bologna}}
% \fancyfoot[L]{Ciclo 1, 2021/22}
%\fancyhead[R] {\thepage}
% \renewcommand{\headrulewidth}{0.3pt}
% \renewcommand{\footrulewidth}{0.3pt}
% \fancyhfoffset{0.7cm}



\DeclareMathOperator{\atantwo}{atan2}

\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\al}{\alpha}
\newcommand{\bt}{\beta}
\newcommand{\om}{\omega}
\newcommand{\gm}{\gamma}
\newcommand{\lb}{\lambda}
\newcommand{\lm}{\lambda_-}
\newcommand{\lp}{\lambda_+}
\newcommand{\thw}{\theta_w}
\newcommand{\thp}{\theta_\phi}
\newcommand{\tht}{\theta}

\newcommand{\etp}{e_{t+}}
\newcommand{\etm}{e_{t-}}

\newcommand{\esp}{e_{x+}}
\newcommand{\esm}{e_{x-}}

\newcommand{\dsp}{\delta_{x+}}
\newcommand{\dsm}{\delta_{x-}}
\newcommand{\dsd}{\delta_{x\cdot}}
\newcommand{\dss}{\delta_{xx}}
\newcommand{\dssss}{\delta_{xxxx}}

\newcommand{\dtp}{\delta_{t+}}
\newcommand{\dtm}{\delta_{t-}}
\newcommand{\dtd}{\delta_{t\cdot}}
\newcommand{\dtt}{\delta_{tt}}
\newcommand{\dtttt}{\delta_{tttt}}
\newcommand{\dspm}{\delta_{x\pm}}

\newcommand{\mtp}{\mu_{t+}}
\newcommand{\mtm}{\mu_{t-}}
\newcommand{\mtd}{\mu_{t\cdot}}
\newcommand{\mtt}{\mu_{tt}}

\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\innp}[1]{\left\langle#1\right\rangle}
\newcommand{\virg}[1]{``#1''}

\setstretch{1.15}

\begin{document}

\begin{titlepage}
\begin{center}
 {\huge\bfseries Numerical Simulation of Mechanical Vibrations\\}
 \vspace{1.5cm}
 {\Large\bfseries Michele Ducceschi}\\[5pt]
 michele.ducceschi@unibo.it\\[14pt]
 \end{center}\vspace{2cm}
\vfill

{These are the lecture notes for the course ``Simulazione Numerica di Sistemi Dipendenti dal Tempo'', Dept. Of Engineering (DIN), Universit{\`a} di Bologna, I Ciclo 2021-2022. } \\[2cm]
\vfill


\emph{Copyright Notice: The entire manuscript is Copyright by Michele Ducceschi. All Rights Reserved. The notes may not be copied or duplicated in whole or part by any means without express prior agreement in writing or unless specifically noted.}\\[2cm]
\vfill


Version 1.4 : \today 
\vfill

Log: 
\begin{itemize}
    \item minor edits to content and style 
    \item new chapter on numerical methods for the wave equation
\end{itemize}


\end{titlepage}
% \section*{\underline{Lecture 1}}
% \renewcommand*\thesubsection{1.\arabic{subsection}}


\tableofcontents

\chapter{Introduction}\label{chap:Intro}

\noindent This course is about the analysis and simulation of mechanical vibrations. Since a kind of oscillation is involved, the systems of interest here are time-dependent These systems appear in many branches of the sciences, including mechanical engineering, acoustics, vibroacoustics, and others. In this chapter, Newton's law for a system comprising one degree of freedom will be reviewed, along with useful concepts used in later chapters. Discrete calculus will also be introduced. 




\section{Newton's law for mechanical oscillations}



We shall  begin the discussion by studying the evolution of a simple vibrating object, comprising a mass $m$  and a subjected to a force field originating from a suitable potential, 
\begin{equation}\label{eq:PhiF}
    F = - \frac{d\phi}{dx}.
\end{equation}
Here $\phi = \phi(x): \mathbb{R} \rightarrow \mathbb{R} \in \mathcal{C}^1$ is the potential (assumed to be differentiable), and $x = x(t): \mathbb{R}^+_0 \rightarrow \mathbb{R} \in \mathcal{C}^2$ is the displacement (assumed to be differentiable twice), measured from some convenient reference position. Here, $t \geq 0$ is time. Newton's law gives
\begin{equation}\label{eq:SHO}
    m \frac{d^2 x}{dt^2} = - \frac{d\phi}{dx}.
\end{equation}
Equation \eqref{eq:SHO} is an example of \emph{ordinary differential equation} (ODE). The equation is  \emph{autonomous}, intended as the absence of explicit dependence on time $t$, except as via the argument of $x$. The equation is, in general, \emph{nonlinear}. Linearity is expressed here as a superposition principle: if $x_1(t)$ and $x_2(t)$ are both solutions to \eqref{eq:SHO}, then, under linear conditions, $x_3(t) = a_1 x_1(t) + a_2 x_2(t)$ (with $a_1$, $a_2$ being constants) is also a solution. Since the second time derivative on the left-hand side of \eqref{eq:SHO} is linear, linearity is obtained when
\begin{equation}
    \frac{d\phi(x_3)}{d x_3} = a_1 \frac{d\phi(x_1)}{dx_1} + a_2 \frac{d\phi(x_2)}{dx_2},
\end{equation}
which is solved for $\phi = c x^\alpha$, where $c$ is a constant, and $\alpha \in \{0,2 \}$ (the case $\alpha =0$ being the trivial case of zero force).


To be complete, \eqref{eq:SHO} requires the specification of two \emph{initial conditions}, usually given as the initial displacement and velocity, such that
\begin{equation}\label{eq:SHO_ICs}
    x(t=0) = x_0, \,\, \frac{dx}{dt}(t=0) = v_0,
\end{equation}
and since the independent variable is the time $t$, \eqref{eq:SHO} plus \eqref{eq:SHO_ICs} specify an \emph{initial value problem} (IVP). When initial conditions are imposed, the system possesses one \emph{unique solution}, if appropriate Lipschitz-continuity arguments are satisfied. (There is no need to study these in detail, we will just assume that the examples provided here have a unique solution). 


% It is sometimes useful to rewrite \eqref{eq:SHO} as a \emph{first-order} system, i.e. a system where the highest order derivative in time is one. This is accomplised via the introduction of the velocity $v(t)$, such that
% \begin{equation}
% \frac{dx}{dt} = v, \,\,\, \frac{dv}{dt} = - \frac{d\phi}{dx},
% \end{equation}
% and clearly one needs to solve here for both $v(t)$ and $x(t)$. Using vector notation, the system can be written compactly as
% \begin{equation}\label{eq:FirstOrder}
% \frac{d{\bf x}}{d t} = - {\bf f}({\bf x})
% \end{equation}
% In this case, $[x(t), v(t)]^\intercal = {\bf x}(t): \mathbb{R}^+_0 \rightarrow \mathbb{R}^2$ is the vector of the unknown variables (to be solved for), and $[-v, \frac{d\phi}{dx}]^\intercal={\bf f}({\bf x}): \mathbb{R}^2 \rightarrow \mathbb{R}^2$ specifies the time evolution. It will be sometimes more practical to work with the second-order form \eqref{eq:SHO}, sometimes with the first-order form \eqref{eq:FirstOrder}. This last form is also known as the \emph{state-space} form, here not comprising input. For the one-dimensional oscillator, the size of the state is 2 (position and velocity), though clearly form \eqref{eq:FirstOrder} expresses generally an $n-$dimensional state-space system. Numerical integrators for IVPs are generally expressed as discretisations of \eqref{eq:FirstOrder}, and this is the reason why such form was introduced here. We will come back to it in due time. 


\subsection{Energy Analysis}\label{sec:EnAnGen}

As anticipated, all through this course, we will rely heavily on energy arguments. The fact that systems present some kind of energy balance is a fundamental principle of physics, bearing significant  consequences in the analysis of both the continuous systems, and the numerical approximations used to simulate them. In the simple, one dimensional case \eqref{eq:SHO}, the work $W$ done by a force pushing on $m$ is
\begin{equation}
    W = \int_0^x F \, \dif x
\end{equation}
where it is assumed that the initial position of the body, at the time $t=0$, is 0. Using this definition in \eqref{eq:SHO} gives
\begin{equation}
    \int_0^x m \frac{d^2 x}{dt^2} \dif x = - \int_0^x \frac{d\phi}{dx} \dif x.
\end{equation}
To integrate, one uses $dx = \frac{dx}{dt} dt$, giving
\begin{equation}\label{eq:EnAnGen}
    \int_0^t m \frac{d^2 x}{dt^2} \frac{d x}{dt} \dif t = - \int_0^t \frac{d\phi}{dx} \frac{dx}{dt} \dif t,
\end{equation}
and, using simple identities, one gets
\begin{equation}\label{eq:En1}
    \int_0^t \frac{d}{dt}\left( \frac{m}{2}\left(\frac{dx}{dt}\right)^2 + \phi  \right) \dif t = 0.
\end{equation}
Since $x(t) \in \mathcal{C}^2$, $\phi(x) \in \mathcal{C}^1$, the equation is solved by taking
\begin{equation}\label{eq:En2}
    \frac{m}{2}\left(\frac{dx}{dt}\right)^2 + \phi = H_0,
\end{equation}
where $H_0$ (a constant) is the total energy of the system. It is convenient to identify the \emph{kinetic} and \emph{potential} components of the energy, given, respectively, by
\begin{equation}
    E_k \triangleq \frac{m}{2}\left(\frac{dx}{dt}\right)^2, \quad E_p \triangleq \phi.
\end{equation}
The expression for $H_0$ is determined by the initial conditions, and hence
\begin{equation}\label{eq:EnCons}
    H_0 = \frac{m v_0^2}{2} + \phi(x_0),
\end{equation}
and the identity $H(t) = H_0$ holds $\forall t \geq 0$. 

\subsection{Bounds on solution growth}\label{sec:BoundsSHO}

Energy, as seen, is conserved. It is remarked that the energy is also \emph{non-negative}, i.e. $H(t) \geq 0$ $\forall t$, when $\phi$ itself is non-negative. This fact leads to the important result of \emph{boundedness of the solutions}. In practice, since the kinetic and potential energies are \emph{both} non-negative, one has
\begin{equation}\label{eq:bnds}
    0 \leq E_k \leq H_0, \quad 0\leq E_p\leq H_0.
\end{equation} 
For the kinetic term, one has, in all cases,
\begin{equation}\label{eq:EgyBoundVel}
    \left|\frac{dx(t)}{dt}\right| \leq \sqrt{2 H_0/m}\,\,\, \forall t,
\end{equation} 
and thus the velocity of the system is \emph{always} bounded in terms of the initial energy. The displacement $x(t)$ itself may or may not be bounded. Consider the case of a quadratic potential, as in the left panel of Fig. \ref{fig:phis}: $\phi = \frac{K x^2}{2}$. This case (as shown above) corresponds to the linear case, i.e. the simple harmonic oscillator with stiffness constant $K$. From \eqref{eq:bnds}, one has $|x|\leq \sqrt{2 H_0/K}$. Hence,  the displacement is bounded. When the potential is not quadratic, a nonlinear system is obtained. We shall consider the nonlinear oscillator later on. Here, qualitatively, two nonlinear potentials are shown in the center and right panels of Fig. \ref{fig:phis}. Bounded motion is obtained whenever the potential attains infinity for large values of $|x|$. However, if the potential $\phi$ is bounded by some finite constant, unbounded motion can be observed.
% \item $\phi = 0.5\left((\tanh(x) + 1)x + (\tanh(-x) + 1)x^2 + c\right)$, where $c$ is a constant such that $\text{min}\,\phi(x) = 0$. This case is nonlinear. However, since $\phi \rightarrow \infty$ as $x \rightarrow \pm \infty$, the displacement is bounded. 
% \item  $\phi = \frac{1}{1+e^{-x}} + (\tanh(-x) + 1)x^2 + c$. This is also a nonlinear case, but in this case it is not possible to bound $x$ for  sufficiently large values of $H_0$, since the potential is itself bounded when $x \rightarrow \infty$.
\begin{figure}
    \includegraphics[width = \linewidth]{Figures/phiGraphs.png}
    \caption{Potential functions corresponding to a quadratic function $\phi  \propto x^2$ (left panel), and non-quadratic functions (center and right panels). Total energy is represented as dashed or dash-dotted lines, the latter corresponding to unbounded motion. }\label{fig:phis}
\end{figure}

To understand these claims, it can be useful to visualise the trajectories in the \emph{phase plane}. This is a plane whose axes are $x = x(t)$ and $y = dx(t)/dt$, see Fig. \ref{fig:phasePorts}. For the quadratic case, trajectories are ellipses, and are hence symmetric about both the $x$ and $y$ axes. Periodicity is evident by inspection of the phase portraits, which appear as closed loops. The energy components also oscillate periodically, in this case at one single frequency. 
For the central panel of Fig. \ref{fig:phis}, trajectories are now symmetric only about the $x$ axis: the nonlinearity is such that symmetry is broken in $x$, as evident from panel 2 of Fig. \ref{fig:phis}. However, motion is still periodic, via the combination of a number frequencies with a common factor (the fundamental frequency). For the right panel of Fig. \ref{fig:phis}, motion may be  bounded or unbounded, depending on the value of $H_0$, as seen in panel 3 of  Fig. \ref{fig:phasePorts}. 
\begin{figure}
    \centering
    1.\\ ~\\ \fbox{\includegraphics[width = 0.8\linewidth]{Figures/BndUnbnd1.png}} \\~\\
    2.\\~\\ \fbox{\includegraphics[width = 0.80\linewidth]{Figures/BndUnbnd2.png}} \\~\\
    3.\\ ~\\\fbox{\includegraphics[width = 0.80\linewidth]{Figures/BndUnbnd3.png}}
    \caption{Phase portraits and energy components for: 1. quadratic potential; 2. central panel of Fig. \ref{fig:phis}; 3. right panel of Fig. \ref{fig:phis}. The energy components are kinetic (dashed-dotted), potential (dashed), and total (solid, conserved).}\label{fig:phasePorts}
\end{figure}

When motion is periodic, the period can be estimated from the energy, since
\begin{equation}\label{eq:dxdtEn}
    \frac{dx}{dt} = \pm\sqrt{\frac{2}{m}}\sqrt{H_0-\phi}.
\end{equation}
The plus or minus signs depends on which side of the phase portrait the particle is (i.e. whether it is found in the $y\geq0$ or $y<0$ half-plane). Considering the plus sign, and inverting, one has
\begin{equation}
    \dif t = \sqrt{\frac{m}{2}}\frac{\dif x}{\sqrt{H_0-\phi}}.
\end{equation}
Thus, half the period is obtained integrating between $x_1$ and $x_2$, which are the points in the phase plane where $dx/dt = 0$. Denoting the period $\tau$, one has
\begin{equation}
    \tau = \sqrt{2m} \int_{x_1}^{x_2}\frac{\dif x}{\sqrt{H_0-\phi}}.
\end{equation}
In general, this equation does not have a closed-form solution. Some cases are exceptional, including the case of simple harmonic motion. For that, $x_1 = -x_2$, and $H_0=H(t) =\frac{K x_2^2}{2}$, where the last equality holds since $dx/dt|_{x=x_2} = 0$. Hence, the period in obtained as the integral over one quadrant (one quarter of a loop):
\begin{equation}
    \tau = 2\sqrt{2m} \int_0^{x_2}\frac{\dif x}{\sqrt{\frac{K x_2^2}{2}-\frac{K x^2}{2}}} = 4 \sqrt{\frac{m}{K}} \arctan\left( \frac{x}{\sqrt{x_2^2-x^2}}\right)\big|_0^{x_2} = 2\pi \sqrt{\frac{m}{K}}.
\end{equation}
It is convenient to introduce the \emph{radian frequency} $\omega_0 = \sqrt{K/m} = 2\pi f_0$, where $f_0$ is a linear frequency (in Hz). Hence, one has
\begin{equation}\label{eq:tau}
    \tau = \frac{2\pi}{\omega_0} = \frac{1}{f_0}.
\end{equation} 

\subsection{Periodicity of orbits via Fourier series}

For Case 1 of the previous section (quadratic potential), periodicity of the solution is expressed as 
\begin{equation}\label{eq:Fou1}
    x(t) = a \cos(\omega_0 t) + b \sin(\omega_0 t) = A \cos(\omega_0 t - \varphi)  = C_+ e^{j\omega_0 t} + C_- e^{-j \omega_0 t}
\end{equation}
where
\begin{equation}\label{eq:Fou2}
    A^2 = a^2 + b^2, \,\, a = A \cos\varphi, \,\, b = A\sin\varphi, \,\, C_+ = \frac{1}{2}(a+jb), \,\, C_- = \frac{1}{2}(a-jb)
\end{equation}
Note that the complex exponential notation in \eqref{eq:Fou1} yields indeed a real solution, when $C_+,C_-$ are selected as in \eqref{eq:Fou2}. The expression for $x(t)$ is periodic, with period $\tau$ given in \eqref{eq:tau}. 



When the potential is not quadratic, the dynamics is nonlinear, as observed in Case 2 and (partly) 3 of the previous subsection. Since periodic motion still exists, this can be obtained as a combination of frequencies, all multiples of a fundamental frequency. A generalisation of \eqref{eq:Fou1} can then be given as
\begin{equation}\label{eq:Fou3}
x(t) = \sum_{m=0}^M \left( a_m \cos(m \omega_0 t) + b_m \sin(m \omega_0 t) \right),
\end{equation}
which can be turned into  amplitude-phase or complex exponential expressions analogous to those in \eqref{eq:Fou1}, using identities similar to \eqref{eq:Fou2}. The upper bound $M$ in the sum is in theory infinity, though one chooses a finite $M$ in any practical application.
Expression \eqref{eq:Fou3} is called a \emph{Fourier series} expansion, which is valid and uniquely determined for any periodic signal $x(t)$, with period $\tau = 2\pi / \omega_0$. If one happens to know $x(t)$ for a given system (for example, via a measurement), then the Fourier components can be extracted as
\begin{equation}\label{eq:FouComps}
a_0 = \frac{1}{\tau}\int_0^\tau x(t) \, \dif t, \,\, a_{m\neq 0} = \frac{2}{\tau}\int_0^\tau x(t)\cos(m\omega_0 t) \, \dif t, \,\, b_{m} = \frac{2}{\tau}\int_0^\tau x(t)\sin(m\omega_0 t) \, \dif t
\end{equation}
These expressions are proven easily when considering the \emph{orthogonality} of the Fourier components (easily obtained via direct integration), i.e. 
\begin{subequations}\label{eq:OrthoFou}
\begin{align}
\int_{0}^\tau \cos(m\omega_0 t)\cos(n \omega_0 t) \dif t &= \frac{\tau}{2}\delta_{m,n} \,\,\, (m>0, n\geq 0) \label{eq:OrthoFou1} \\
\int_{0}^\tau \sin(m\omega_0 t)\sin(n \omega_0 t) \dif t &= \frac{\tau}{2}\delta_{m,n} \,\,\, (m>0, n\geq 0) \label{eq:OrthoFou2} \\
\int_{0}^\tau \cos(m\omega_0 t)\sin(n \omega_0 t) \dif t &= 0  \label{eq:OrthoFou3}
\end{align}
\end{subequations}
Multiplying \eqref{eq:Fou3} by, say, $\sin(n\omega_0 t)$, integrating, and using \eqref{eq:OrthoFou2}, one obtains the last expression in \eqref{eq:FouComps}. The other expressions in \eqref{eq:FouComps} are obtained analogously. 
A picture of the Fourier components for Cases 1,2 and 3 is given in Fig. \ref{fig:Fou1}, where one sees that simple harmonic motion is indeed characterised by a single frequency of vibration.
\begin{figure}
\includegraphics[width = 0.33\linewidth]{Figures/Fou1.png}
\includegraphics[width = 0.33\linewidth]{Figures/Fou2.png}
\includegraphics[width = 0.33\linewidth]{Figures/Fou3.png}
\caption{Output spectra of Case 1, 2, 3. Initial conditions: $x_0 = 0.75$, $v_0 = 0.5$.}\label{fig:Fou1}
\end{figure}
% There is a fundamental difference between \eqref{eq:Fou1} (for the linear oscillator), and \eqref{eq:Fou3} (valid in the nonlinear case): the former expression depends on two constants, whereas the latter depends on $2M$ constants (in theory, an infinite amount). Clearly, the two constants in the linear case are uniquely determined from the initial conditions, but in the nonlinear case there is no hope to determine the $2M$ constants from the initial conditions alone. 

It may be useful to express the Fourier series in complex exponential form. In that case, one has
\begin{equation}\label{eq:FourierSeriesComplex}
x(t) = \sum_{m=-M}^M c_m e^{jm\omega_0 t},
\end{equation}
where $c_0 = a_0$, $c_m = \frac{1}{2}\left( a_m - j b_m\right)$ for $m>0$, $c_m = \frac{1}{2}\left( a_m + j b_m\right)$ for $m<0$.














\section{Time Difference Operators}


This section introduces the notation and the principles of difference calculus, upon which the method of \emph{finite differences} is constructed. Though finite differences are used as a method for computer-aided solution of differential equations, this method has surprisingly long roots, reaching as far as the work of Courant, Friedrichs, and Lewy in 1928 (before digital computers were even available). The underlying principle of finite differences is straightforward, and it involves the discrete approximation of  differential operators. In digital applications, whether e.g. performing a measurement, or in computer-aided simulation, time is most often discretised by means of a \emph{sample rate}, $f_s$. In practice, one usually knows (or is interested in knowing) the state of a system at discrete time intervals, of length $k$ seconds, the time step. The relationship between sample rate and time step is simply
\begin{equation}
    k f_s = 1,
\end{equation}
The aim of finite differences is to compute a \emph{time series} $x^n$ approximating the ``true'' solution $x(t)$ of a given model problem. The index $n \in \mathbb{N}_0$ in $x^n$ is shorthand for $t = t_n \triangleq nk$, i.e. $n$ is the time index, approximating the true solution $x(t)$ at the time $t_n = nk$.  The fundamental relationship between the approximate time series $x^n$ and the true solution $x(t)$ is as follows:
\begin{equation}\label{eq:ErrDef}
    x(t_n) - x^n  = E^n ,
\end{equation}
where $E^n$ is a time series defining the \emph{absolute error} of the approximation. In general, $E^n \neq 0$, and, generally, it will not  be possible to obtain an exact expression for $E^n$ (since $x(t)$ is generally unknown!) However, the study of finite differences is almost entirely devoted to the design of schems for which the error remains provably \emph{bounded}  by a given power of $k$, and we shall of course spend considerable effort in studying such schemes.


\subsection{Shift, time and averaging operators}

Given the time series $x^n$, the identity, forward and backward shift operators are given as
\begin{equation}
    {1}{x}^n = { x}^n, \quad \etp {x}^n = { x}^{n+1}, \quad \etm { x}^n = { x}^{n-1}.
\end{equation}
From these, one may define the time difference operators, all approximating the first time derivative, as
\begin{subequations}
    \begin{align}
        \dtp & = \frac{(\etp - 1)}{k} \approx \frac{d}{dt},     \\ 
        \dtm & = \frac{(1 - \etm)}{k}\approx \frac{d}{dt},      \\ 
        \dtd & = \frac{(\etp - \etm)}{2k}\approx \frac{d}{dt} . 
    \end{align}
\end{subequations}
An approximation to the second time derivative is constructed from the above as
\begin{equation}
    \dtt  = \dtp\dtm\approx \frac{d^2}{dt^2}.
\end{equation}
Averaging operators (all approximating the identity) are also used throughout the text, and are
\begin{subequations}
    \begin{align}
        \mtp & = \frac{(\etp + 1)}{2} \approx 1,    \\ 
        \mtm & = \frac{(1 + \etm)}{2} \approx 1,    \\ 
        \mtd & = \frac{(\etp + \etm)}{2} \approx 1. 
    \end{align}
\end{subequations}
Whilst these expressions look at least reasonable, Taylor series arguments can be used to infer the \emph{order} of the approximation. Hence, the difference operators are applied to the  continuous function $x(t)$, and a Taylor expansion is applied. For the forward difference operator, one gets
\begin{align*}
    \dtp x(t_n) = \frac{x(t_n+k) - x(t_n)}{k} & \approx \frac{x(t_n)+ k \frac{dx(t)}{dt}|_{t=t_n}+\frac{k^2}{2}\frac{d^2x(t)}{dt^2}|_{t=t_n}-x(t_n)}{k} \\&= \frac{dx(t_n)}{dt}+\frac{k}{2}\frac{d^2x(t_n)}{dt^2}
\end{align*}
Since ${d^2x(t_n)}/{dt^2}$ is a value independent of $k$, in the limit of high sample rate the expression above reduces to ${dx(t_n)}/{dt}$. The rate at which such approximation is satisfied is linear in $k$, so that applying definition \eqref{eq:ErrDef} one gets
\begin{equation}\label{eq:Errs1}
    \frac{dx(t_n)}{dt} - \dtp x(t_n) = O(k).
\end{equation} 
The ``big-Oh'' notation $O(k^p)$ means that the rate of the approximation goes as $k^p$. For the current case, $p=1$. Using similar arguments, it is easy to show that
\begin{subequations}\label{eq:Errs}
\begin{align}
     \frac{dx(t_n)}{dt} - \dtm x(t_n) &= O(k), \\   
     \frac{dx(t_n)}{dt} - \dtd x(t_n) &= O(k^2),\\   
     \frac{d^2x(t_n)}{dt^2} - \dtt x(t_n) &= O(k^2),
\end{align}
\end{subequations}
and so on. These identities define the \emph{truncation errors} of the finite difference approximations. It is seen, then, that some operators have a higher accuracy than others. Generally, these employ a larger \emph{stencil}, that is, the footprint in time of a finite difference operator including the coefficients. So, $\dtm$, $\dtp$, $\mtm$, $\mtp$ all have a stencil of width 1. $\dtd$, $\mtd$, $\dtt$ have a stencil of width 2. See also Fig. \ref{fig:stencil}. 


It is of course possible to construct operators with  higher accuracy. As an example, consider the following central approximation to the first time derivative, 
\begin{equation}
    \bar \delta_{t\cdot} = \frac{\etm^2 - 8 \etm + 8\etp - \etp^2}{12k}.
\end{equation} 
It is easy (though a little tedious) to show that this approximation is $O(k^4)$. At this point of the discussion, one may be tempted to think that the construction of higher-order schemes for the solution of a model problem amounts to merely employing difference operators with the appropriate stencil. Things are, of course, more complicated than this: primarily, operators with wide stencils in time tend to yield \emph{unstable} simulations, as will be seen in forthcoming examples. Other problems exist: for instance, it is not clear how to initialise an operator  with a stencil of width $M$, when the model problem is of order $N < M$. A similar problem arises when discretising differential operators in space for boundary-values problems, where one must set values for the ``ghost points'' (i.e. points located outside the boundary of the grid). 

Constructing higher-order schemes is of course possible, and sometimes desirable, and these words of caution suffice for the moment as cursory understanding of the underlying difficulties. 

When solving differential equations, different definitions of errors are employed: these are the \emph{local truncation error} (LTE) and the \emph{global error}. The truncation errors given by \eqref{eq:Errs1} and \eqref{eq:Errs} are only useful to determine the order of the approximation of a difference operator, but ensuring that the differential operators of a difference scheme are approximated to an order $p$, does not automatically ensure that the scheme is $p^{th}$ order accurate, if in fact convergent at all.

 
\begin{figure}
    \includegraphics[width= \linewidth]{Figures/stencils.pdf}
    \caption{Stencil width and coefficients (units of $k$) of various difference operators approximating the first time derivative}\label{fig:stencil}
\end{figure}


\section{Frequency domain analysis and stability of LTI systems}\label{sec:FreqDomAn}

Traditional analysis techinques for time-dependent problems rely on frequency domain analysis, both in the continuous and discrete cases.

\subsection{Laplace and $z$ transforms}

For the continuous function $x(t)$, the Laplace transform is obtained as
\begin{equation}\label{eq:LapT}
    \hat x(s) = \int_{\{-\infty,0\}}^{\infty} x(t) e^{-st} \dif t \triangleq \mathcal{L}\{x\}(s),
\end{equation}
where $s = j\omega + \sigma \in \mathbb{C}$ is a complex variable. The lower bound in the integral means that the transform can be defined to be two-sided (starting from $-\infty$), or one-sided (starting from 0), the latter allowing the incorporation of initial conditions. The closely related $z$ transform is a discrete version of \eqref{eq:LapT}, i.e. 
\begin{equation}\label{eq:ZT}
    \hat x(z) = \sum_{n = \{-\infty,0 \}}^{\infty} x^n z^{-n} \triangleq \mathcal{Z}\{x \}(z)
\end{equation}
for a complex number $z \in \mathbb{C}$. In the analysis of linear, time invariant (LTI) systems, both continuous and discrete, one usually computes $s$ and $z$ from the given model problem, via direct substitution of the transforms. Then, the stability of the underlying system may be inferred by direct inspection of $s$ and $z$, as will be stated below.  Applying the Laplace transform to the time derivative of $x(t)$ gives:
\begin{equation}\label{eq:LPtemp1}
    \mathcal{L}\left\{\frac{d x}{dt } \right\} = \int_{-\infty}^\infty\frac{dx(t)}{dt}e^{-st} \dif t = s \int_{-\infty}^{\infty} x(t) e^{-st} \dif t =  s\mathcal{L}\{x\},
\end{equation}
where integration by parts was used, and where it was assumed that $x(t)$ dies out fast enough as  $t\rightarrow \pm \infty$. The shift operators under the two-sided $z$ transform become
\begin{equation}\label{eq:LPtemp2}
    \mathcal{Z}\left\{e_{t\pm} x^n \right\} = \sum_{n = -\infty}^{\infty} x^{n\pm 1}z^{-n} = z^{\pm 1} \sum_{n = -\infty}^{\infty} x^{n\pm 1}z^{-(n\pm 1)} = z^{\pm 1} \mathcal{Z}\{x^n \}.
\end{equation}
Using analogous arguments as  for \eqref{eq:LPtemp1} and \eqref{eq:LPtemp2}, one can show that higher-order derivatives and differences transform as
\begin{equation}\label{eq:TransLZ}
    \mathcal{L}\left\{\frac{d^p x}{dt^p } \right\} = s^p \mathcal{L}\{x\}, \quad \mathcal{Z}\left\{e^p_{t\pm} x^n \right\} = z^{\pm p} \mathcal{Z}\left\{x^n \right\}.
\end{equation}
In practice, it is often useful to substitute simpler expressions than the transforms, via an \emph{ansatz}. So, one may employ the test solutions
\begin{equation}\label{eq:ansatz}
    x(t) = \hat x e^{st}, \quad x^n = \hat x z^n,
\end{equation}
for appropriate constant complex amplitudes $\hat x$. It is immediate to verify that derivatives and differences of these test solutions transform in the same way as the derivatives and differences of the Laplace and $z$ transforms in \eqref{eq:TransLZ}. For this reason, for LTI systems, the use of the test solutions yields ultimately  the same qualitative analysis as the substitution of the full transforms, and we shall make use of such solutions accordingly. Since time $t$ (and, accordingly, the time index $n$) increases toward infinity, boundedness of $x(t)$ and $x^n$ in \eqref{eq:ansatz} can be achieved if and only if $\text{Re}(s) \leq 0$, and $|z|\leq 1$, respectively. This is, in essence, the idea of stability in the frequency domain.



The fact that the transforms of derivatives and differences amount to mere multiplications in the frequency domain is a powerful analysis tool allowing for great simplifications. In theory, once the problem is solved in the frequency domain, one can transform back to the time domain,  by computing the inverse transforms. This approach, however, presents some drawbacks. Most notably, inverse transforms are difficult to compute, and closed-form solutions are available only in a few cases. Second, these techniques do not generalise  to nonlinear, time variant systems (though some exceptions exisit, such as Volterra kernels). Frequency domain techniques remain a popular analysis tool nonetheless, since most nonlinear systems reduce to linear under suitable conditions, and one may infer (at least qualitatively) some useful properties of the model systems under study. 


Laplace transforms (and inverses) may be difficult to compute, and usually one resorts to table look-ups. A couple of useful transorm pairs, used later on, are listed here as
\begin{equation}\label{eq:LaplTtable}
    \mathcal{L}\left\{e^{-ct}\sin(a t)u(t)\right\}(s) = \frac{a}{(s+c)^2 + a^2}, \quad \mathcal{L}\left\{e^{-ct}\cos(a t)u(t)\right\}(s) = \frac{s+c}{(s+c)^2 + a^2}.
\end{equation}
The function $u(t)$ is the step function at time $t=0$ (i.e. $u(t<0)=0, u(t\geq 0) = 1)$, and thus the identities above apply equally to the two-sided and one-sided transforms. 






\subsection{Fourier and discrete time Fourier transforms}
When one considers $\sigma=0$ in \eqref{eq:LapT}, and $z = e^{j\omega k}$ in \eqref{eq:ZT} (in the two-sided form), the continuous \emph{Fourier transform} and the \emph{discrete time Fourier transform} (DTFT) are obtained. These are useful to compute the magnitude and the phase of the solutions (i.e. the spectrum), and will also be used throughout. The definitions of these transforms are as: 
\begin{equation}
    \hat x(\omega) = \int_{-\infty}^{\infty} x(t) e^{-j\omega t} \dif t \triangleq \mathcal{F}\{x\}(\omega),\qquad \hat x(\omega) = \sum_{n = -\infty}^{\infty} x^n e^{-j\omega k n} \triangleq \mathcal{X}\{x^n \}(\omega).
\end{equation}
The \virg{hat} notation was used in both definitions, but the meaning is different. We remark, however, that the discrete time Fourier transform is a continuous function of the radian frequency $\omega$, and is thus not to be confused with the discrete Fourier transform (which is evaluated at discrete frequency bins). A couple of useful transform pairs are given here as
\begin{equation}\label{eq:FouTtable}
    \mathcal{F}\left\{x(t)\sin(at)\right\}(\omega) = \frac{\hat x(\omega-a)-\hat x(\omega+a)}{2j}, \quad \mathcal{F}\left\{e^{-ct}u(t)| c \geq 0\right\}(\omega) = \frac{1}{\sqrt{2\pi}(c+j\omega)}.
\end{equation}
In the second transform, $u(t)$ is again the step function as in \eqref{eq:LaplTtable}. 

\subsection{Frequency-domain intepretation of time difference operators}\label{sec:FDtransformations}

The action of the difference operators on a time series $x^n$ may be interpreted in the $z$ domain as a \emph{transformation}. In \eqref{eq:LPtemp2}, it was seen that $\mathcal{Z}\left\{e_{t\pm} x^n\right\} = z^{\pm 1} \mathcal{Z}\left\{x^n\right\}$. 
The action of other difference operators may be constructed from such identity. To evaluate the frequency response, $z$ is limited to the unit circle, i.e. $z=e^{j\omega k}$, i.e. the DTFT $\mathcal X\left\{x^n \right\}$. Consider, for example, the second difference operator. This is
\begin{equation}\label{eq:DTFTdtt}
\mathcal{X}\left\{\dtt x^n\right\} = \frac{e^{j\omega k}-2+e^{-j\omega k}}{k^2}\mathcal{X}\left\{x^n\right\}=\frac{2}{k^2}\left(\cos(\omega k)-1 \right)\mathcal{X}\left\{x^n\right\} = -\frac{4}{k^2} \sin^2\left(\frac{\omega k}{2}\right)\mathcal{X}\left\{x^n\right\}
\end{equation}
Analogously, one has
\begin{subequations}
\begin{align}
\mathcal{X}\left\{\mtd x^n\right\} &= \cos(\omega k) \mathcal{X}\left\{x^n \right\}, \\
\mathcal{X}\left\{\mtt x^n\right\} &= \frac{1}{2}\left(\cos(\omega k) + 1\right)\mathcal{X}\left\{x^n \right\} \\
\mathcal{X}\left\{\dtd x^n\right\} &= \frac{j}{k}\sin(\omega k) \mathcal{X}\left\{x^n \right\}
\end{align}
\end{subequations}
These identities will prove useful in the study of the stability of LTI systems via \emph{von Neumann} analysis. 




\subsection{Recursion polynomials}
\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{Figures/SchurCohn.pdf}
\caption{The Schur-Cohn stability region, i.e. the region of the $(b,c)-$plane for which the roots of \eqref{eq:TempSchurCohn} have magnitude less than unity.}\label{fig:SchurCohn}
\end{figure}
Finite difference schemes, as will be seen shortly, produce a recursion in time. When such recursion has constant coefficients, substitution of \emph{ansatz} \eqref{eq:ansatz} results in a complex equation of the form
\begin{equation}
    \sum_{n=0}^M a_n z^n = 0,
\end{equation}
where the $a_n$'s are constant coefficients, and $M$ is the problem order. Often, for the problems of interest here, $M=2$, and thus the summation above reduces to (after rescaling by $a_2 \neq 0$)
\begin{equation}\label{eq:TempSchurCohn}
    z^2 + b z + c = 0.
\end{equation}
The solutions (i.e. the poles) are given by $z_\pm = \frac{-b \pm \sqrt{b^2-4 c}}{2}$. Useful bounds on the coefficients $b,c$ can be derived when one considers $|z_\pm|<1$ (i.e. when the poles are within the unit circle). These are
\begin{equation}\label{eq:SchurCohnStab}
    |c| = |z_+ z_-| =  |z_+| |z_-| < 1, \quad |b|<1+|c|,
\end{equation}
which are necessary and sufficient conditions to enforce $|z_\pm|<1$, and are known as \emph{Schur-Cohn stability test}, see also Fig. \ref{fig:SchurCohn}.







% \section{Newton's Law for mechanical oscillations}



% Though not all time-dependent process involve some kind of oscillation, we shall nonetheless begin the discussion by studying the evolution of a simple vibrating object, comprising a mass $m$  and a subjected to a force field originating from a suitable potential, 
% \begin{equation}\label{eq:PhiF}
%     F = - \frac{d\phi}{dx}.
% \end{equation}
% Here $\phi = \phi(x): \mathbb{R} \rightarrow \mathbb{R}^+_0 \in \mathcal{C}^0$ is the potential (assumed to be non-negative, and continuous), and $x = x(t): \mathbb{R}^+_0 \rightarrow \mathbb{R} \in \mathcal{C}^2$ is the displacement (assumed to be differentiable twice), measured from some convenient reference position. Here, $t \geq 0$ is time. Newton's law gives
% \begin{equation}\label{eq:SHO}
%     m \frac{d^2 x}{dt^2} = - \frac{d\phi}{dx}.
% \end{equation}
% Equation \eqref{eq:SHO} is an example of \emph{ordinary differential equation} (ODE). The equation is  \emph{autonomous}, intended as the absence of explicit dependence on time $t$, except as via the argument of $x$. The equation is, in general, \emph{nonlinear}. Linearity is expressed here as a superposition principle: if $x_1(t)$ and $x_2(t)$ are both solutions to \eqref{eq:SHO}, then, under linear conditions, $x_3(t) = x_1(t) + x_2(t)$ is also a solution. Since the second time derivative on the left-hand side of \eqref{eq:SHO} is linear, linearity is obtained when
% \begin{equation}
%     \frac{d\phi(x_3)}{d x_3} = \frac{d\phi(x_1)}{dx_1} + \frac{d\phi(x_2)}{dx_2},
% \end{equation}
% which is solved for $\phi = c x^\alpha$, where $c \geq 0$ is a constant, and $\alpha \in \{0,2 \}$ (the case $\alpha =0$ being the trivial case of zero force).


% To be complete, \eqref{eq:SHO} requires the specification of two \emph{initial conditions}, usually given as the initial displacement and velocity, such that
% \begin{equation}\label{eq:SHO_ICs}
%     x(t=0) = x_0, \,\, \frac{dx}{dt}(t=0) = v_0,
% \end{equation}
% and since the independent variable is the time $t$, \eqref{eq:SHO} plus \eqref{eq:SHO_ICs} specify an \emph{initial value problem} (IVP). When initial conditions are imposed, the system possesses one \emph{unique solution}, if appropriate Lipschitz-continuity arguments are satisfied. (There is no need to study these in detail, we will just assume that the examples provided here have a unique solution). 


% % It is sometimes useful to rewrite \eqref{eq:SHO} as a \emph{first-order} system, i.e. a system where the highest order derivative in time is one. This is accomplised via the introduction of the velocity $v(t)$, such that
% % \begin{equation}
% % \frac{dx}{dt} = v, \,\,\, \frac{dv}{dt} = - \frac{d\phi}{dx},
% % \end{equation}
% % and clearly one needs to solve here for both $v(t)$ and $x(t)$. Using vector notation, the system can be written compactly as
% % \begin{equation}\label{eq:FirstOrder}
% % \frac{d{\bf x}}{d t} = - {\bf f}({\bf x})
% % \end{equation}
% % In this case, $[x(t), v(t)]^\intercal = {\bf x}(t): \mathbb{R}^+_0 \rightarrow \mathbb{R}^2$ is the vector of the unknown variables (to be solved for), and $[-v, \frac{d\phi}{dx}]^\intercal={\bf f}({\bf x}): \mathbb{R}^2 \rightarrow \mathbb{R}^2$ specifies the time evolution. It will be sometimes more practical to work with the second-order form \eqref{eq:SHO}, sometimes with the first-order form \eqref{eq:FirstOrder}. This last form is also known as the \emph{state-space} form, here not comprising input. For the one-dimensional oscillator, the size of the state is 2 (position and velocity), though clearly form \eqref{eq:FirstOrder} expresses generally an $n-$dimensional state-space system. Numerical integrators for IVPs are generally expressed as discretisations of \eqref{eq:FirstOrder}, and this is the reason why such form was introduced here. We will come back to it in due time. 


% \subsection{Energy Analysis}\label{sec:EnAnGen}

% As anticipated, all through this course, we will rely heavily on energy arguments. The fact that systems present some kind of energy balance is a fundamental principle of physics, bearing significant  consequences in the analysis of both the continuous systems, and the numerical approximations used to simulate them. In the simple, one dimensional case \eqref{eq:SHO}, the work $W$ done by a force pushing on $m$ is
% \begin{equation}
%     W = \int_0^x F dx 
% \end{equation}
% where it is assumed that the initial position of the body, at the time $t=0$, is 0. Using this definition in \eqref{eq:SHO} gives
% \begin{equation}
%     \int_0^x m \frac{d^2 x}{dt^2} dx = - \int_0^x \frac{d\phi}{dx} dx.
% \end{equation}
% To integrate, one uses $dx = \frac{dx}{dt} dt$, giving
% \begin{equation}\label{eq:EnAnGen}
%     \int_0^t m \frac{d^2 x}{dt^2} \frac{d x}{dt} dt = - \int_0^t \frac{d\phi}{dx} \frac{dx}{dt}dt,
% \end{equation}
% and, using simple identities, one gets
% \begin{equation}\label{eq:En1}
%     \int_0^t \frac{d}{dt}\left( \frac{m}{2}\left(\frac{dx}{dt}\right)^2 + \phi  \right)dt = 0.
% \end{equation}
% Since $x(t) \in \mathcal{C}^2$, $\phi(x) \in \mathcal{C}^0$, the equation is solved by taking
% \begin{equation}\label{eq:En2}
%     \frac{m}{2}\left(\frac{dx}{dt}\right)^2 + \phi = H_0,
% \end{equation}
% where $H_0$ (a constant) is the total energy of the system. It is convenient to identify the \emph{kinetic} and \emph{potential} components of the energy, given, respectively, by
% \begin{equation}
%     E_k \triangleq \frac{m}{2}\left(\frac{dx}{dt}\right)^2, \quad E_p \triangleq \phi.
% \end{equation}
% The expression for $H_0$ is determined by the initial conditions, and hence
% \begin{equation}\label{eq:EnCons}
%     H_0 = \frac{m v_0^2}{2} + \phi(x_0),
% \end{equation}
% and the identity $H(t) = H_0$ holds $\forall t \geq 0$. 

% \subsection{Bounds on solution growth}\label{sec:BoundsSHO}

% Energy, as seen, is conserved. It is remarked that the energy is also \emph{non-negative}, i.e. $H(t) \geq 0$ $\forall t$, because of the assumption that $\phi$ itself be non-negative. This fact leads to the important result of \emph{boundedness of the solutions}. In practice, since the kinetic and potential energies are \emph{both} non-negative, one has
% \begin{equation}\label{eq:bnds}
%     0 \leq E_k \leq H_0, \quad 0\leq E_p\leq H_0.
% \end{equation} 
% For the kinetic term, one has, in all cases,
% \begin{equation}\label{eq:EgyBoundVel}
%     \left|\frac{dx(t)}{dt}\right| \leq \sqrt{2 H_0/m}\,\,\, \forall t,
% \end{equation} 
% and thus the velocity of the system is \emph{always} bounded in terms of the initial energy. The displacement $x(t)$ itself may or may not be bounded. Consider the case of a quadratic potential, as in the left panel of Fig. \ref{fig:phis}: $\phi = \frac{K x^2}{2}$. This case (as shown above) corresponds to the linear case, i.e. the simple harmonic oscillator with stiffness constant $K$. From \eqref{eq:bnds}, one has $|x|\leq \sqrt{2 H_0/K}$. Hence,  the displacement is bounded. When the potential is not quadratic, a nonlinear system is obtained. We shall consider the nonlinear oscillator later on. Here, qualitatively, two nonlinear potentials are shown in the center and right panels of Fig. \ref{fig:phis}. Bounded motion is obtained whenever the potential attains infinity for large values of $|x|$. However, if the potential $\phi$ is bounded by some finite constant, unbounded motion can be observed.
% % \item $\phi = 0.5\left((\tanh(x) + 1)x + (\tanh(-x) + 1)x^2 + c\right)$, where $c$ is a constant such that $\text{min}\,\phi(x) = 0$. This case is nonlinear. However, since $\phi \rightarrow \infty$ as $x \rightarrow \pm \infty$, the displacement is bounded. 
% % \item  $\phi = \frac{1}{1+e^{-x}} + (\tanh(-x) + 1)x^2 + c$. This is also a nonlinear case, but in this case it is not possible to bound $x$ for  sufficiently large values of $H_0$, since the potential is itself bounded when $x \rightarrow \infty$.
% \begin{figure}
%     \includegraphics[width = \linewidth]{Figures/phiGraphs.png}
%     \caption{Potential functions corresponding to a quadratic function $\phi  \propto x^2$ (left panel), and non-quadratic functions (center and right panels). Total energy is represented as dashed or dash-dotted lines, the latter corresponding to unbounded motion. }\label{fig:phis}
% \end{figure}

% To understand these claims, it can be useful to visualise the trajectories in the \emph{phase plane}. This is a plane whose axes are $x = x(t)$ and $y = dx(t)/dt$, see Fig. \ref{fig:phasePorts}. For the quadratic case, trajectories are ellipses, and are hence symmetric about both the $x$ and $y$ axes. Periodicity is evident by inspection of the phase portraits, which appear as closed loops. The energy components also oscillate periodically, in this case at one single frequency. 
% For the central panel of Fig. \ref{fig:phis}, trajectories are now symmetric only about the $x$ axis: the nonlinearity is such that symmetry is broken in $x$, as evident from panel 2 of Fig. \ref{fig:phis}. However, motion is still periodic, via the combination of a number frequencies with a common factor (the fundamental frequency). For the right panel of Fig. \ref{fig:phis}, motion may be  bounded or unbounded, depending on the value of $H_0$, as seen in panel 3 of  Fig. \ref{fig:phasePorts}. 
% \begin{figure}
%     \centering
%     1.\\ ~\\ \fbox{\includegraphics[width = 0.85\linewidth]{Figures/BndUnbnd1.png}} \\~\\
%     2.\\~\\ \fbox{\includegraphics[width = 0.85\linewidth]{Figures/BndUnbnd2.png}} \\~\\
%     3.\\ ~\\\fbox{\includegraphics[width = 0.85\linewidth]{Figures/BndUnbnd3.png}}
%     \caption{Phase portraits and energy components for: 1. quadratic potential; 2. central panel of Fig. \ref{fig:phis}; 3. right panel of Fig. \ref{fig:phis}. The energy components are kinetic (dashed-dotted), potential (dashed), and total (solid, conserved).}\label{fig:phasePorts}
% \end{figure}

% When motion is periodic, the period can be estimated from the energy, since
% \begin{equation}\label{eq:dxdtEn}
%     \frac{dx}{dt} = \pm\sqrt{\frac{2}{m}}\sqrt{H_0-\phi}.
% \end{equation}
% The plus or minus signs depends on which side of the phase portrait the particle is (i.e. whether it is found in the $y\geq0$ or $y<0$ half-plane). Considering the plus sign, and inverting, one has
% \begin{equation}
%     dt = \sqrt{\frac{m}{2}}\frac{dx}{\sqrt{H_0-\phi}}.
% \end{equation}
% Thus, half the period is obtained integrating between $x_1$ and $x_2$, which are the points in the phase plane where $dx/dt = 0$. Denoting the period $\tau$, one has
% \begin{equation}
%     \tau = \sqrt{2m} \int_{x_1}^{x_2}\frac{dx}{\sqrt{H_0-\phi}}.
% \end{equation}
% In general, this equation does not have a closed-form solution. Some cases are exceptional, including the case of simple harmonic motion. For that, $x_1 = -x_2$, and $H_0=H(t) =\frac{K x_2^2}{2}$, where the last equality holds since $dx/dt|_{x=x_2} = 0$. Hence, the period in obtained as the integral over one quadrant (one quarter of a loop):
% \begin{equation}
%     \tau = 2\sqrt{2m} \int_0^{x_2}\frac{dx}{\sqrt{\frac{K x_2^2}{2}-\frac{K x^2}{2}}} = 4 \sqrt{\frac{m}{K}} \arctan\left( \frac{x}{\sqrt{x_2^2-x^2}}\right)\big|_0^{x_2} = 2\pi \sqrt{\frac{m}{K}}.
% \end{equation}
% It is convenient to introduce the \emph{radian frequency} $\omega_0 = \sqrt{K/m} = 2\pi f_0$, where $f_0$ is a linear frequency (in Hz). Hence, one has
% \begin{equation}\label{eq:tau}
%     \tau = \frac{2\pi}{\omega_0} = \frac{1}{f_0}.
% \end{equation} 

% \subsection{Periodicity of solutions. Fourier series.}

% For Case 1 of the previous section (quadratic potential), periodicity of the solution is expressed as 
% \begin{equation}\label{eq:Fou1}
%     x(t) = a \cos(\omega_0 t) + b \sin(\omega_0 t) = A \cos(\omega_0 t - \varphi)  = C_+ e^{j\omega_0 t} + C_- e^{-j \omega_0 t}
% \end{equation}
% where
% \begin{equation}\label{eq:Fou2}
%     A^2 = a^2 + b^2, \,\, a = A \cos\varphi, \,\, b = A\sin\varphi, \,\, C_+ = \frac{1}{2}(a+jb), \,\, C_- = \frac{1}{2}(a-jb)
% \end{equation}
% Note that the complex exponential notation in \eqref{eq:Fou1} yields indeed a real solution, when $C_+,C_-$ are selected as in \eqref{eq:Fou2}. The expression for $x(t)$ is periodic, with period $\tau$ given in \eqref{eq:tau}. 



% When the potential is not quadratic, the dynamics is nonlinear, as observed in Case 2 and (partly) 3 of the previous subsection. Since periodic motion still existis, this can be obtained as a combination of frequencies, all multiples of a fundamental frequency. A generalisation of \eqref{eq:Fou1} can then be given as
% \begin{equation}\label{eq:Fou3}
% x(t) = \sum_{m=0}^M \left( a_m \cos(m \omega_0 t) + b_m \sin(m \omega_0 t) \right),
% \end{equation}
% which can be turned into  amplitude-phase or complex exponential expressions analogous to those in \eqref{eq:Fou1}, using identities similar to \eqref{eq:Fou2}. The upper bound $M$ in the sum is in theory infinity, though one chooses a finite $M$ in any practical application.
% Expression \eqref{eq:Fou3} is called a \emph{Fourier series} expansion, which is valid and uniquely determined for any periodic signal $x(t)$, with period $\tau = 2\pi / \omega_0$. If one happens to know $x(t)$ for a given system (for example, via a measurement), then the Fourier components can be extracted as
% \begin{equation}\label{eq:FouComps}
% a_0 = \frac{1}{\tau}\int_0^\tau x(t) \, dt, \,\, a_{m\neq 0} = \frac{2}{\tau}\int_0^\tau x(t)\cos(m\omega_0 t) \, dt, \,\, b_{m} = \frac{2}{\tau}\int_0^\tau x(t)\sin(m\omega_0 t) \, dt
% \end{equation}
% These expressions are proven easily when considering the \emph{orthogonality} of the Fourier components (easily proven via direct integration), i.e. 
% \begin{subequations}\label{eq:OrthoFou}
% \begin{align}
% \int_{0}^\tau \cos(m\omega_0 t)\cos(n \omega_0 t) dt &= \frac{\tau}{2}\delta_{m,n} \,\,\, (m>0, n\geq 0) \label{eq:OrthoFou1} \\
% \int_{0}^\tau \sin(m\omega_0 t)\sin(n \omega_0 t) dt &= \frac{\tau}{2}\delta_{m,n} \,\,\, (m>0, n\geq 0) \label{eq:OrthoFou2} \\
% \int_{0}^\tau \cos(m\omega_0 t)\sin(n \omega_0 t) dt &= 0  \label{eq:OrthoFou3}
% \end{align}
% \end{subequations}
% Multiplying \eqref{eq:Fou3} by, say, $\sin(m\omega_0 t)$, integrating, and using \eqref{eq:OrthoFou2}, one obtains the last expression in \eqref{eq:FouComps}. The other expressions in \eqref{eq:FouComps} are obtained analogously. 
% A picture of the Fourier components for Cases 1,2 and 3 is given in Fig. \ref{fig:Fou1}, where one sees that simple harmonic motion is indeed characterised by a single frequency of vibration.
% \begin{figure}
% \includegraphics[width = 0.33\linewidth]{Figures/Fou1.png}
% \includegraphics[width = 0.33\linewidth]{Figures/Fou2.png}
% \includegraphics[width = 0.33\linewidth]{Figures/Fou3.png}
% \caption{Output spectra of Case 1, 2, 3. Initial conditions: $x_0 = 0.75$, $v_0 = 0.5$.}\label{fig:Fou1}
% \end{figure}
% There is a fundamental difference between \eqref{eq:Fou1} (for the linear oscillator), and \eqref{eq:Fou3} (valid in the nonlinear case): the former expression depends on two constants, whereas the latter depends on $2M$ constants (in theory, an infinite amount). Clearly, the two constants in the linear case are uniquely determined from the initial conditions, but in the nonlinear case there is no hope to determine the $2M$ constants from the initial conditions alone. 


% An analytical development based on expansion \eqref{eq:Fou3} exists, and is known as the \emph{harmonic balance method}, but the development becomes unwieldy even for the simple case of the cubic oscillator, and is therefore not recommended. Other analytical approximations are available, such as the \emph{method of multiple scales}, the \emph{method of averaging}, etc, but these are again limited to leading-order corrections of the linear solutions. Numerical methods are really the only tool availble to the analyst interested in the long-term evolution of nonlinear systems. 

\chapter{The Simple Harmonic Oscillator}\label{chap:SHO}

Simple harmonic motion is obtained under linear condition, that is
\begin{equation}
    \phi = \frac{Kx^2}{2}.
\end{equation}
Under such choice, \eqref{eq:SHO} becomes
\begin{equation}\label{eq:SHOreal}
    \frac{d^2 x}{dt^2} = - \omega_0^2 x,
\end{equation}
where the radian natural frequency $\omega_0 = \sqrt{K/m}$ was introduced. There are multiple reasons to be wanting to study the simple harmonic oscillator here: a variety of mechanical systems can be approximated by \eqref{eq:SHOreal}, see e.g. Fig. \ref{fig:oscExamples}. Furthermore, as will be seen in later lectures, distributed systems may themselves be approximated as a bank of parallel oscillators, each one corresponding to one ``mode'' of vibration. Finally, the simple harmonic oscillator possesses exact (i.e. closed-form) solutions, both analytically and numerically, and is thus a very useful test case for the numerical schemes that will be introduced below. It was seen that the solution is expressed as the sum of two periodic functions, i.e.
\begin{equation}
    x(t) = a \cos(\omega_0 t) + b \sin{\omega_0 t}.
\end{equation}
The constants $a,b$ are uniquely determined from the intial conditions. Setting $x(t=0)=x_0$ and $\frac{dx}{dt}(x=0)=v_0$, one gets
\begin{equation}\label{eq:SHOexact}
    x(t) = x_0 \cos(\omega_0 t) + \frac{v_0}{\omega_0}\sin(\omega_0 t)
\end{equation}
The energy components are obtained explictly as
\begin{equation}
    E_k = \frac{m}{2}\left(x_0\omega_0\sin(\omega_0 t) - v_0 \cos(\omega_0 t) \right)^2, \quad E_p = \frac{K}{2}\left( x_0 \cos(\omega_0 t) + \frac{v_0}{\omega_0}\sin(\omega_0 t) \right)^2.
\end{equation}
Summing the two expressions together, one gets
\begin{equation}
    H(t) = \frac{m v_0^2}{2} + \frac{K x_0^2}{2} = H_0,
\end{equation}
i.e. \eqref{eq:EnCons}. 
\begin{figure}
    \includegraphics[width=\linewidth]{Figures/OscillatorsExamples.pdf}
    \caption{Examples of harmonic oscillators. The mass-spring system (a), and the series RLC circuit (b) are usually given as examples of harmonic oscillators. The case of a cantilever beam with a point mass (c), and the pendulum (d) may  be approximated as harmonic oscillators in the case of small vibrations.}\label{fig:oscExamples}
\end{figure}



% \subsubsection{Stability}

% The stability of the model problem \eqref{eq:SHOreal} was already shown via energy arguments, in Sec. \ref{sec:BoundsSHO}. It can be appreciated that energy analysis handles equally well linear and nonlinear cases.  For the simple harmonic oscillator, stability may be drawn using frequency domain techniques as well, as anticipated in Sec. \ref{sec:FreqDomAn}. Thus, assuming a trial solution of the form $x = e^{st}$ gives
% \begin{equation}
% s^2  x = - \omega_0^2  x \quad \rightarrow \quad s = \pm j \omega_0
% \end{equation}
% In this case, $s$ is purely imaginary and the solutions are oscillating.  


\section{A finite difference scheme}

Consider the time series $x^n$, approximating the true solution $x(t)$ of \eqref{eq:SHOreal}. As a first example of a working finite difference scheme, consider
\begin{equation}\label{eq:Scheme1}
    \dtt x^n = -\omega_0^2 x^n.
\end{equation}
Expanding out the operator, one gets 
\begin{equation}\label{eq:FDbasic}
    x^{n+1} = x^n(2-\omega_0^2 k^2) - x^{n-1},
\end{equation}
and hence the update requires one multiply and one sum. Clearly, it is convenient to store the value $2-\omega_0^2 k^2$ offline, so that one need not  recompute this value at each time step. The update \eqref{eq:FDbasic} is \emph{explicit}: by this term, we denote a scheme such that
\begin{equation}
    x^{n+1} = f(x^n,x^{n-1}), \qquad \text{(explicit scheme)}
\end{equation}
We will soon encounter schemes that are instead \emph{implicit}, i.e. 
\begin{equation}\label{eq:ImplSchemeDef}
    g(x^{n+1})= f(x^n,x^{n-1}), \qquad \text{(implicit scheme)}
\end{equation}
where $g$ is generally a nonlinear function in  $x^{n+1}$.


Though scheme \eqref{eq:Scheme1} looks reasonable, there is no guarantee (for the moment) that the computed solutions are indeed an approximate form of the true solution $x(t)$. In some cases, as will be seen shortly, the time series computed by $\eqref{eq:Scheme1}$ diverges, in some other cases, it remains bounded. The next few sections will explain the idea of \emph{convergence}, and the closely linked idea of \emph{stability}. A notion of stability may be formalised as follows. We say that a scheme is stable in stability region $\Lambda \subseteq \mathbb{R}^+$ if, for any positive time constant $\tau \leq n k$, there exist a positive index $M$ such that
\begin{equation}\label{eq:StabDef}
    |x^n| \leq C_\tau \sum_{m = 0}^M |x^m|
\end{equation}
for a constant $C_\tau > 0$ independent of $k \in \Lambda$. In practice, stability is defined as a bound on the time series including the first $M+1$ steps. 

\subsection{Stability via frequency domain analysis}\label{sec:FreqDomSHO}

As anticipated earlier, frequency-domain techinques may be employed in the analysis of stability of linear, time-invariant discrete systems, such as \eqref{eq:Scheme1}. For that, an \emph{ansatz} of the form \eqref{eq:ansatz} is substituted:
\begin{equation}
    x^n = \hat x z^{n}.
\end{equation}
In this equation, notation is a little mixed-up, since on the left-hand side $n$ denotes the time index, whereas on the right-hand side it is an exponent! In practice, we keep the same apex notation in both cases, for the sake of notation, but the meaning is very different. We get
\begin{equation}
    \hat x z^n\left( z - (2-\omega_0 k^2) + z^{-1}\right) = 0, \quad \rightarrow \quad z_{\pm} = \frac{2 - \omega_0^2 k^2 \pm \omega_0^2 k^2\sqrt{1 - \frac{4}{\omega_0^2 k^2}}}{2}.
\end{equation}
Hence, the solution is
\begin{equation}\label{eq:sol_z_SHO}
    x^n =  A_+ z^n_+ + A_- z^n_-,
\end{equation}
for complex constants $A_\pm$. We assume that the scheme is started using two starting values $x^0, x^1$ (obtained from $x_0$, $v_0$ of the continuous problem). Then
\begin{equation}
    x^0 = A_+ + A_-, \quad x^1 = A_+ z_+ + A_- z_-.
\end{equation}
From these, the complex constants are obtained as
\begin{equation}\label{eq:ApAm}
    A_+ = \frac{x^0z_- - x^1}{z_- - z_+},\,\,\, A_- = \frac{x^1 - x^0z_+ }{z_- - z_+}.
\end{equation}
If the square root in $z_\pm$ is a real number, than $z_-$ has magnitude larger than unity, and the solution $x^n$ will therefore grow exponentially over time: this is \emph{instability}. On the other hand, when the square root in imaginary, then $z_\pm$ become oscillating, and $z_\pm$ are complex conjugates. This condition is obtained as
\begin{equation}\label{eq:StabCondSHO}
    k < \frac{2}{\omega_0}, 
\end{equation}
which is an upper bound on the time step, once the natural frequency of the oscillator is set. In the case of oscillating solutions, one has
\begin{equation}
    z_\pm = r e^{\pm j \theta},
\end{equation}
with $r = 1$, and $\tan\theta = \left(\omega_0^2k^2\sqrt{\frac{4}{\omega_0^2 k^2}-1}\right)/(2-\omega_0^2 k^2)$.

To check stability, definition \eqref{eq:StabDef} is applied, to give
\begin{equation}\label{eq:SHObound} 
    |x^n| = |A_+ r^n e^{j \theta n} + A_- r^n e^{-j \theta n}| \leq |A_+| + |A_-| \leq \frac{|x^0|+|x^1|}{|\sin \theta|} \triangleq C_\tau \sum_{m=0}^1 |x^m|,
\end{equation}
and thus the absolute value of the solution at the time $n>1$ is bounded in terms of the values at $n=0,1$, with bounding constant $C_\tau = 1/|\sin\theta|$. (The first inequality in the above was obtained via the triangle inequality. Then, the fact that $|r|=|e^{\pm j\theta}|=1$ was used, and finally the values from \eqref{eq:ApAm} were substituted in). A numerical check of the current bound is given in Fig. \ref{fig:SHObounds}.
\begin{figure}
    \includegraphics[width = \linewidth]{Figures/boundsSHO.png}
    \caption{Simple Harmonic Oscillator. Numerical check on bound \eqref{eq:SHObound}. The sample rate used in the examples is $f_s = 2000$ Hz. Starting values are $x^0 = x^1 = 1$. Dashed line line is bound \eqref{eq:SHObound}, characteristic frequency as indicated, with $\omega_{max}=2/k$.}\label{fig:SHObounds}
\end{figure}
Note that the same condition may be arrived at via \emph{von Neumann} analysis. Recall the DTFT of the $\dtt$ operator, as per \eqref{eq:DTFTdtt}. Transforming \eqref{eq:Scheme1} in the frequency domain accordingly, one gets
\begin{equation}
\left(-\frac{4}{k^2}\sin^2\left( \frac{\omega k}{2}\right) + \omega_0^2\right)\mathcal{X}\left\{ x^n\right\} = 0.
\end{equation}
One must impose  $0 \leq \sin^2\left( \frac{\omega k}{2}\right) \leq 1$, which is possible if and only if \eqref{eq:StabCondSHO} holds. If this condition is violated, one has that the frequency $\omega$ becomes pure imaginary, thus the sine becomes a hyerbolic sine, with unbounded growth (instability).




\subsubsection{Stability via energy analysis}



The discussion of Sec. \ref{sec:EnAnGen} suggests that, if the model problem can be shown to have a conserved total energy, with non-negative kinetic and potential terms, then the solution can be bounded in terms of the energy. It may be tempting to try to find a discrete version of \eqref{eq:EnAnGen}, valid in the discrete case. To that end, \eqref{eq:Scheme1} is multiplied by $m \dtd x^n$, to get
\begin{equation}\label{eq:SHOen1}
    m \dtd x^n (\dtt x^n +\omega_0^2 x^n) = 0
\end{equation}
A couple of useful identities (that will be used throughout) are given here:
\begin{equation}\label{eq:IdsFD}
    \dtd x^n \, \dtt x^n = \dtp \left( \frac{(\dtm x^n)^2}{2}\right), \,\, \dtd x^n \, x^n = \dtp \left( \frac{x^n \etm x^n}{2}\right).
\end{equation}
These are proven by simple algebra. Using these identities in \eqref{eq:SHOen1}, one gets
\begin{equation}
    \dtp \left( \frac{m}{2}{(\dtm x^n)^2} + \frac{K}{2} {x^n \etm x^n} \right) = 0,
\end{equation} 
that is a discrete counterpart of \eqref{eq:En2}. Multiplication by $m$ was here used so to yield units of energy in the expression within the brakets, though of course one may obtain conserved energy per unit mass via multiplication by $\dtd x^n$ alone. It is easy, in the above, to recognise a discrete approximation to the continuous energy balance \eqref{eq:En1}. In this case, one may define an \emph{interleaved} time series $\mathfrak{h}^{n-1/2}$, corresponding to the discrete conserved energy:
\begin{equation}\label{eq:DiscEnSHO}
    \mathfrak{h}^{n-1/2} \triangleq \frac{m}{2}{(\dtm x^n)^2} + \frac{K}{2} {x^n \etm x^n} = \mathfrak{h}^{1/2}.
\end{equation}
In light of the discussion in the continuous case, one may of course use energy conservation as a means to bound the growth of solutions over time. The problem here, is that $\mathfrak{h}^{n-1/2}$ may \emph{not} be positive, since the potential energy is of indefinite sign. Instances leading to negative energy overall are a manifestation of instability, and must be avoided. It may be useful, then, to bound the potential term in the energy expression. Using
\begin{equation}
    x^n \etm x^n  =  (\mtm x^n)^2 - \frac{k^2}{4}\left(\dtm x^n \right)^2,
\end{equation}
the total energy is 
\begin{equation}\label{eq:ModEnSHO}
    \mathfrak{h}^{n-1/2} = \frac{m\left(\dtm x^n \right)^2}{2} \left(1 - \frac{\omega_0^2 k^2}{4}\right) + \frac{K (\mtp x^n)^2}{2} \geq \frac{m\left(\dtm x^n \right)^2}{2} \left(1 - \frac{\omega_0^2 k^2}{4}\right),
\end{equation}
and thus the total energy will be non-negative if and only if \eqref{eq:StabCondSHO} is satisfied. Fig. \ref{fig:EnConsSHO} shows the energy components and error for scheme \eqref{eq:Scheme1}. The energy components are given as per \eqref{eq:DiscEnSHO}, and it is remarked that the potential term \emph{does} become negative at times. It is the \emph{overall} energy that is positive. Of course, the equivalent expression in \eqref{eq:ModEnSHO} has modified expressions for the kinetic and potential energies, that are always non-negative under stability condition \eqref{eq:StabCondSHO}.
\begin{figure}
    \includegraphics[width=\linewidth,clip,trim={5cm 0.0cm 5cm 0cm}]{Figures/EnergyErr.png}
    \caption{Energy behaviour of scheme \eqref{eq:Scheme1}. Left: energy components. Kinetic (dashed), potential (dash-dotted), total (solid). Right: energy error $\mathfrak{h}^{n-1/2}/\mathfrak{h}^{1/2}-1$. The oscillator is initialised with $x_0=v_0=1$, and $\omega_0=100$ rad/s.}\label{fig:EnConsSHO}
\end{figure}




\subsubsection{Consistency and accuracy}

We are now introducing the idea of   \emph{local truncation error} (LTE), denoted here $\varepsilon^n$. Applying the finite difference scheme to the true solution $x(t)$ yields a definiton of the LTE as
\begin{equation}\label{eq:LTEdef}
    \dtt x(t_n) + \omega_0^2 x(t_n) = \varepsilon^n.
\end{equation}
Using Taylor series arguments, as per \eqref{eq:Errs}, one gets
\begin{equation}
    \left( \frac{d^2 x(t)}{dt} + \omega_0^2 x(t)\right)|_{t=t_n} + O(k^2) = \varepsilon^n,
\end{equation}
and since $x(t)$ is the true solution, one recovers $\varepsilon^n = O(k^2)$. The behaviour of the LTE as a function of $k$ describes the idea of \emph{consistency}: a scheme is said to be consistent if
\begin{equation}
    \lim_{k\rightarrow 0}\varepsilon^n = 0.
\end{equation}
In practice, consistent schemes are such that the local error becomes small as $k$ is decreased. Usually, $\varepsilon = O(k^p)$, and one may conclude that the scheme is $p^{th}$-order accurate. Of course, this is not entirely true, since the question of accuracy is tightly bound to the ideas of stability and convergence: higher-accurate schemes may \emph{never} converge for a given model problem. The idea of accuracy is only going to be meaningful when a scheme is provably stable in some manner. As an example, consider a fourth-order accurate difference operator discretising the second time derivative:
\begin{equation}\label{eq:FourthOrderdtt}
    \bar\delta_{tt} x(t) = \left(\frac{-\etp^2 + 16 \etp - 30 + 16\etp - \etp^2}{12k^2}\right) x(t) = \frac{d^2 x}{dt^2} + O(k^4).
\end{equation}
Though technically ``higher'' accurate, this approximation is always unstable, even for the simple problem of a free particle ($\phi = 0$ in \eqref{eq:PhiF}). Using the test solution $x^n = \hat x z^n$ for this test case, one has
\begin{equation}
    \hat x z^n \left(-z^2 + 16 z - 30 + 16 z^{-1} - z^{-2}\right) = 0,
\end{equation}
and it is easy to verify that there exist one (real) root $z \approx 13.9282$ for which clearly $|z|>1$. The scheme is unstable, and the higher accuracy of error of $\bar\delta_{tt}$ has no real advantage. 


\subsubsection{Convergence}

In turn, what we are really interested in is the evolution of the global error $E^n$, which must remain bounded. For stable, consistent schemes, the global error $E^n$ can be expected to maintain the same trend as the local truncation error $\varepsilon^n$, though this claim would require a formal proof. As an example, the output of scheme \eqref{eq:Scheme1} is compared against the exact solution given in \eqref{eq:SHOexact}. The initial conditions in the continuous system are set as $x_0 =1$, $v_0 = 0$, giving $x(t) = \cos(\omega_0 t)$. The initial conditions in the discrete scheme are given as $x^0 = x_0 = 1$, $x^1 = \cos(\omega_0 k)$ (which discretises the exact solution at the time $t=k$). Fig. \ref{fig:SHOerrs} (a double log plot) shows that indeed slopes of 2 are recovered.


\begin{figure}
    \includegraphics[width = \linewidth]{Figures/OscError.pdf}
    \caption{Global error of scheme \eqref{eq:Scheme1}. Here, $nk = 1$. Initial conditions are given as $x_0 = 1$, $v_0 = 0$, giving $x(t) = \cos(\omega_0 t)$. The numerical initial conditions are $x^0 = 1$, $x^1 = \cos(\omega_0 k)$. The three lines correspond to $\omega_0 = 100$ rad/s (solid), $\omega_0 = 200$ rad/s (dashed), $\omega_0 = 300$ rad/s (dash-dotted). Sample rate $f_s = 2000$ Hz.}\label{fig:SHOerrs}
\end{figure}



The behaviour of the global error as a function of the time step $k$ encapsulates the idea of convergence. A scheme is convergent if 
\begin{equation}\label{eq:convDef}
    \lim_{k\rightarrow 0} (x(t_n)-x^n) = \lim_{k\rightarrow 0} E^n = 0.
\end{equation}
In practice, as the time step is decreased, the global error goes to zero. We will come back to the idea of convergence later on. In general, a stable and consistent method is also convergent (this should be proven rigourously, and we will postpone this discussion until later sections).  


\subsubsection{Initialisation}\label{sec:Init}
In the previous subsection, the scheme was initialised exactly using knowledge coming from the exact solution $x(t)$ as per \eqref{eq:SHOexact}. Of course, generally an exact solution is not available, and schemes must be initialised in some other manner. Since we usually know the initial position and velocity of the oscillator (in continuous time) $x_0, v_0$, we would like to know how to use this information to extract suitable initial values for the time series, i.e. $x^0$, $x^1$.
\begin{figure}[hbt]
    \centering
    \includegraphics[width = 1\linewidth]{Figures/OscErrorOrder.pdf}
    \caption{Global error of scheme \eqref{eq:Scheme1}. Initialisation with first-order accurate (markers) and second-order accurate (lines) initial conditions. Here, $nk = 1$. Initial conditions are given as $x_0 = 0.5$, $v_0 = 0.5$. The numerical initial conditions are $x^0 = x_0$, $x^1 = kv_0 + x_0 - 0.5 k^2 \omega_0^2 x_0$ (lines), $x^0 = x_0$, $x^1 = kv_0 + x_0$ (markers). The three cases correspond to $\omega_0 = 100$ rad/s, $\omega_0 = 200$ rad/s, $\omega_0 = 300$ rad/s.}\label{fig:SHOerrsOrders}
\end{figure}
Obviously, one can set 
\begin{equation}
    x^0=x_0
\end{equation}
For $x^1$, one possible solution is to use a simple forward difference to compute it from $v_0$ and $x^0$, i.e. 
\begin{equation}
    \dtp x^0 = v_0 \,\,\, \rightarrow \,\,\, x^1 = x^0 + kv_0.
\end{equation}
This approximation to the initial conditions is only \emph{first-order accurate.} This is easily proven via Taylor series arguments. One may be tempted then to use the centered difference $\dtd$, instead of the forward difference $\dtp$, since it yields a second-order accurate approximation to the time derivative. Of course, this is not possible directly, since the stencil of $\dtd$ is too large: we do not know what $x^{-1}$ is (in fact, this value is not defined). However, consider the following expression for the centered time difference:
\begin{equation}\label{eq:dtdInit}
    \dtd = \dtp - \frac{k}{2} \dtt = \frac{d}{dt} + O(k^2).
\end{equation}
Of course, the value of $\dtt$ is not substituted directly, rather via \eqref{eq:Scheme1}, i.e. $\dtt = -\omega_0^2$. Thus, a second-order accurate intial condition can be given as
\begin{equation}
    \dtd x^0 = v_0 \,\,\, \rightarrow \,\,\, x^1 = x^0 + kv_0 - \frac{k^2}{2}\om_0^2x^0.
\end{equation}
Fig. \ref{fig:SHOerrsOrders} shows the error plots, computed against the exact solution \eqref{eq:SHOexact}, displaying the expected  trends in the limit of high sample rate. Note that, for lower values of the sample rate, the error of the first-order accurate scheme may in fact be lower than the second-order accurate scheme, though in the limit of vanishing time step the correct trends are recovered, and convergence is of course faster for the second-order accurate schemes.  

Higher order accurate approximations are of course possible. Here is a list, obtained using Taylor-series arguments. 
\begin{align}\label{eq:HigherOrderICsOscillator}
\dtp x^0 &= v_0 \quad \text{first order}\\
\left(\dtp -\frac{k}{2}\dtt \right) x^0 &= v_0 \quad \text{second order}\\
\left(\dtp -\frac{k}{2}\dtt - \frac{k^2}{6}\dtp \dtt \right) x^0 &= v_0 \quad \text{third order}\\
\left(\dtp -\frac{k}{2}\dtt - \frac{k^2}{6}\dtp \dtt - \frac{k^3}{24}\dtt^2 \right) x^0 &= v_0 \quad \text{fourth order}
\end{align}
In the expressions above, substituting $(\dtt)^p = (-\omega_0)^p$ gives a way to compute $x^1$, knowing $x^0$ and $v_0$. 


\subsection{Frequency warping and modified equation techniques}\label{eq:ModEqTechniques}

The discussion about accuracy has so far dealt with the idea of order-accuracy, i.e. how the global error $E^n$ behaves as the time step $k$ is decreased. It was seen that finite difference scheme usually behave in such a way that $|E^n| = O(k^p)$, where $p \geq  1$ is the order of accuracy. This is, of course, one way of looking at how well a scheme performs. For the oscillator, it may be useful to measure the degree of accuracy in the frequency domain. Solution \eqref{eq:sol_z_SHO} suggests that the solutions are oscillating with natural frequency
\begin{equation}\label{eq:ErrFreqs}
    \omega = \frac{1}{k}\arctan{\frac{\omega_0^2k^2\sqrt{\frac{4}{\omega_0^2 k^2}-1}}{(2-\omega_0^2 k^2)}} = \omega_0 \left(1 + \frac{\omega_0^2 k^2}{24} +  \frac{3 \omega_0^4 k^4}{640} + O(\omega_0^6k^6) \right),
\end{equation} 
and hence the natural frequency computed by  scheme \eqref{eq:Scheme1} is second-order accurate compared to the natural frequency of the continuous system. See also left panel of Fig. \ref{eq:FreqWarping}. The error in frequency can be quite audible, as $\omega_0$ approaches the limit of stability $\omega_{max}=2/k$. In practice, the cent deviation from \eqref{eq:ErrFreqs} is given by
\begin{equation}
    1200 \log_{2}\frac{\omega}{\omega_0} = 1200\log_2\left(1 + \frac{\omega_0^2 k^2}{24} +  \frac{3 \omega_0^4 k^4}{640} + O(\omega_0^6k^6) \right), 
\end{equation}
and this is shown in the right panel of Fig. \ref{eq:FreqWarping}.
\begin{figure}
    \includegraphics[width=\linewidth,clip,trim={1cm 0.0cm 1cm 0cm}]{Figures/FreqWarp.pdf}
    \caption{Frequency warping (left) and cent deviation (right) of scheme \eqref{eq:Scheme1}. In the left panel, three natural frequencies are shown: 100 rad/s (solid), 200 rad/s (dahsed), 300 rad/s (dash-dotted). The right panel shows cent deviation up to $O(\omega_0^6k^6)$ (solid), and exact (dashed).}\label{eq:FreqWarping}
\end{figure}
The frequency warping effects are quite evident. It may be preferable, then, to construct schemes with a higher accuracy. This, of course, cannot be done by merely using difference operators with a larger stencil, such as the one given in \eqref{eq:FourthOrderdtt}: this would result in unstable behaviour! A different approach, known as \emph{modified equation method}, can be constructed starting from Taylor-series arguments. One has
\begin{equation}\label{eq:dttExpasion}
    \dtt = \sum_{l=1}^{\infty}\frac{2k^{2(l-1)}}{(2l)!}\frac{d^{2l}}{dt^{2l}}= \frac{d^2}{dt^2} + \frac{k^2}{12}\frac{d^4}{dt^4} + \frac{k^4}{360}\frac{d^6}{dt^6} + O(k^6).
\end{equation}
Considering for the moment the expansion up to the term $l=2$, it is natural to add a term $-\frac{k^2}{12}\dtt \dtt x^n$ on the left-hand side of \eqref{eq:Scheme1}, in order to cancel the $O(k^2)$ error. Then, one may use $\dtt = -\omega_0^2$ twice, to get 
\begin{equation}
    \dtt x^n = \frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} \right) x^n,
\end{equation}
and of course the local truncation error $\varepsilon^n$ is now $O(k^4)$. Considering now the next term in the series, proportional to $k^4$, and using $\dtt = -\omega_0^2$ three times, one gets
\begin{equation}
    \dtt x^n = \frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} - \frac{\omega_0^6k^6}{320}\right) x^n,
\end{equation}
and this approximation is $O(k^6)$. Of course, one may go on and add more and more terms to the series. The expansion can be rewritten conveniently as
\begin{equation}
    \frac{1}{k^2}\left(-\omega_0^2k^2 + \frac{\omega_0^4 k^4}{12} - \frac{\omega_0^6 k^6}{320} + ...\right) = \frac{2}{k^2}\left(-1 + \underbrace{1 - \frac{\omega_0^2k^2}{2} + \frac{\omega_0^4k^4}{4!} - \frac{\omega_0^6k^6}{6!} + ...}_{\cos(\omega_0k)} \right),
\end{equation}
and thus, adding infinite terms to the series results in
\begin{equation}\label{eq:SHOexactNum}
    \dtt x^n =  \frac{2}{k^2}\left(-1 + \cos(\omega_0 k) \right)x^n.
\end{equation}
Since the series expansion converges to a known function in this case, we can say that scheme \eqref{eq:SHOexactNum} solves \eqref{eq:SHOreal} \emph{exactly}. Of course, this is somewhat too strong a statement: following the discussion in Sec. \ref{sec:Init}, we know that the scheme is only going to be as accurate as its initial conditions in this case. However, scheme \eqref{eq:SHOexactNum} does not introduce errors in the frequency domain. 


\section{Loss}\label{sec:LossSHO}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.9\linewidth,clip]{Figures/PhaseSpLoss.png}
    \caption{Time evolution and phase portraits of lightly damped oscillator ((a),(b)), and overdamped oscillator ((c),(d)). The natural frequency is $\omega_0=100$ rad/s, and the decay times are $\tau_{60}=5$ s (lightly damped), 0.05 s (overdamped). Initial conditions are $x_0=-0.01$ m, $v_0=0.04$ m/s.}\label{fig:dampedOsc}
\end{figure}
We are now going to study the behaviour of the oscillator in the presence of dissipative forces. These are always present in some form in real systems. Terms proportional to the velocity are usually a good starting point to model viscous damping. For the oscillator, a modification of \eqref{eq:SHOreal} can then be given as
\begin{equation}\label{eq:SHOLoss}
    \frac{d^2 x}{dt^2} = -\omega_0^2 x - 2c \frac{dx}{dt}.
\end{equation}
Here, $c \geq 0$ is a loss parameter (assumed constant, and measured in s$^{-1}$). We remark that \eqref{eq:SHOLoss} is still a linear, time invariant system, and we may infer its stability properties via \emph{ansatz} \eqref{eq:ansatz}. Thus, 
\begin{equation}\label{eq:LossAnsatz}
    \hat x e^{st}\left(s^2 + 2c s + \omega_0^2 \right) = 0, \,\, \text{implying}\,\, s_{\pm} = -{c} \pm \sqrt{{c}^2 - \omega_0^2}.
\end{equation}
The qualitative behaviour of the oscillator will depend on the value of $c$ compared to $\omega_0$, i.e. whether the square root in the expression for $s_\pm$ is real or pure imaginary. Two cases of interest may be extracted as follows:
\begin{enumerate}
    \item $c<\omega_0$. In this case, the oscillator is only lightly damped, and $s_\pm = -c \pm j \sqrt{\omega_0^2-c^2}$. Remembering the definition of $s$ in \eqref{eq:LapT}, one may extract $\sigma = - c$, $\omega = \sqrt{\omega_0^2-c^2}$. The solution to \eqref{eq:SHOLoss} may then be written as $x(t) = A_+ e^{s_+ t} + A_- e^{s_- t} = e^{-c t}\left(A_+ e^{j \omega t} + A_- e^{-j\omega t} \right)$. As per usual, the complex constants $A_+,A_-$ are determined from the intial conditions. The solution is in this case the product of an oscillating solution, times an exponentially damped envelope. The frequency of vibration is $\omega = \omega_0\sqrt{1 - c^2/\omega_0^2}\approx \omega_0\left(1 - \frac{c^2}{2\omega_0^2} \right)$ and is thus lower than the natural frequency of the undamped oscillator. In this case, motion is not strictly periodic, since the mass is never really going to extend as far at each oscillation because of the energy given away to losses. However, it still makes sense to speak of period of vibration, as $\tau = 2\pi / \omega.$ 
    \item $c>\omega_0$. In this case, $s_\pm$ are both real, and negative, i.e. $s_\pm < 0$. Thus, the solutions are still ``physical'' in that they die out exponentially as time increases. However, the mass does not oscillate. This case is sometimes referred to as \emph{overdamped oscillator.}
\end{enumerate}
Fig. \ref{fig:dampedOsc} shows illustrative examples of such cases. Note that, in phase space, the orbits spiral toward the centre, and motion is strictly speaking not periodic.
A third case (\emph{critically damped}) is obtained whenever $c=\omega_0$. This case serves as a mathematical ``boundary'' between the overdamped and lightly damped cases, and is never really realised in practice. A useful quantity to quantify loss is the \emph{decay time} $\tau_{60}$. This is defined as the time taken by the oscillator to reduce its amplitude of vibration by 60 dB. In the lightly damped case, the amplitude envelope is simply $e^{-ct}$. Thus, the implicit definition of $\tau_{60}$ is obtained as
\begin{equation}\label{eq:tau60}
    -60 = 20 \log_{10}e^{-c\tau_{60}}, \,\, \text{and upon inversion: } \,\, \tau_{60} = \frac{3}{c} \ln(10).
\end{equation}
This shows that the loss constant $c$ is most easily interpreted as a function of the decay time.



\subsection{Energy analysis}


Qualitative results on stability can be obtained via energy analysis. Multiplying \eqref{eq:SHOLoss} by $m \frac{dx}{dt}$ and using the same indentities as for \eqref{eq:En1}, we get
\begin{equation}\label{eq:EnBalLoss}
    \frac{d}{dt}\left( \frac{m}{2} \left(\frac{dx}{dt}\right)^2 + \frac{K x^2}{2}   \right) = -Q(t) \triangleq - 2mc \left( \frac{dx}{dt} \right)^2 \leq 0,
\end{equation}
implying that the total energy is not increasing. Here, $Q(t) \geq 0$ is the power dissipated by the oscillator. Thus, bounds \eqref{eq:bnds}  hold here as well. It is somewhat harder to draw more quantitative results here, without knowledge on the form of $x(t).$ However, it is easy to draw trajectories in phase space: instead of a closed loop, the mass now spirals toward the centre as a result of losses. 


\subsection{A finite difference scheme}

As a discretisation for \eqref{eq:SHOLoss} is obtained as
\begin{equation}\label{eq:FDSHOLoss}
    \dtt x^n = -\omega_0^2 x^n -2c \dtd x^n,
\end{equation}
yielding an update equation
\begin{equation}
    \left(ck+1\right) x^{n+1} = (2-\omega_0^2 k^2) x^n + (ck - 1)x^n.
\end{equation}
Using the definition of local truncation error $\varepsilon^n$, as per \eqref{eq:LTEdef}, one gets $\varepsilon^n = O(k^2)$, $\forall n$, showing that the LTE is second-order accurate. Stability may be inferred using either frequency-domain analysis, or energy methods. Application of the former via the ansatz $x^n = \hat x z^n$ gives
\begin{equation}\label{eq:SolFdLoss}
    \hat x z^n \left((1+ck)z - (2-\omega_0^2 k^2) -(ck-1)z^{-1}\right), \,\, z_\pm = \frac{2-\omega_0^2k^2 \pm \sqrt{(2-\omega_0^2k^2)^2 - 4(1-ck)(1+ck)}}{2(1+ck)},
\end{equation}
and, using the Schur-Cohn condition \eqref{eq:SchurCohnStab}, $|z_\pm|<1$ if and only if
\begin{equation}
    k < \frac{2}{\omega_0},
\end{equation}
that is the same as \eqref{eq:StabCondSHO}. The same condition may be arrived at via energy analysis. To that end, multiply \eqref{eq:FDSHOLoss} by $\dtd x^n$, 
\begin{equation}
    \dtd x^n \, \dtt x^n = - \dtd x^n \, \omega_0^2 x^n - 2c (\dtd x^n)^2.
\end{equation}
Using identities \eqref{eq:IdsFD}, and multiplying by the mass $m$ to restore units of energy, one gets
\begin{equation}
    \dtp \left( \frac{m}{2}{(\dtm x^n)^2} + \frac{K}{2} {x^n \etm x^n} \right) = - 2mc (\dtd x^n)^2 \leq 0,
\end{equation}
which is a discrete counterpart of \eqref{eq:EnBalLoss}. Thus, the discrete energy is non-increasing, and when the total energy is itself non-negative, boundedness of the solution results. Thus, the stability condition \eqref{eq:StabCondSHO} is recovered in this case as well. Of course, \eqref{eq:SHObound}  holds in this case too. The numerical decay time may be established via knowledge of the solutions $z_\pm$ in \eqref{eq:SolFdLoss}. Assuming oscillating behaviour, $z_\pm$ are complex conjugates with absolute value
\begin{equation}
    |z_\pm| = \sqrt{\frac{1-ck}{1+ck}}.
\end{equation}
Thus, the numerical decay time index $n_{60}$ is given by
\begin{equation}
    -60 = 20\log_{10}\left({\frac{1-ck}{1+ck}}\right)^{n_{60}/2},\,\,\,\, n_{60}k = \frac{6k \ln(10)}{\ln \frac{1+ck}{1-ck}}\approx \tau_{60} - c k^2 \ln(10),
\end{equation}
showing that the numerical decay time (in seconds) is $O(k^2)$ compared to the exact decay time $\tau_{60}$ defined in \eqref{eq:tau60}.

\subsection{Higher-order schemes}
Higher-order accurate schemes may of course be obtained in this case as well. To that end, consider the definition of the LTE, as
\begin{equation}
    \left(\dtt+2c\dtd \right) x(t_n) = - \omega_0^2 x(t_n) + \varepsilon^n,
\end{equation}
where $x(t_n)$ is the true solution. Expanding in a Taylor series, one has
\begin{equation}
    \left(\frac{d^2}{dt^2} + 2c \frac{d}{dt} \right)\left(1 + \frac{k^2}{6}\frac{d^2}{dt^2}\right)x(t_n) - \frac{k^2}{12}\frac{d^4}{dt^4}x(t_n) + O(k^4) = - \omega_0^2 x(t_n) + \varepsilon^n.
\end{equation}
This suggests the use the following modified scheme, in order to cancel the terms proportional to $k^2$:
\begin{equation}
    \left(\dtt + 2c \dtd \right)\left(1 - \frac{k^2}{6}\dtt\right)x^n + \frac{k^2}{12}\dtt\dtt x^n = -\omega_0^2 x^n.
\end{equation}
Since $\dtt + 2c \dtd  = -\omega_0^2 + O(k^2)$, the scheme above can be written as (to the order $O(k^4)$):
\begin{equation}
    \left(\dtt + 2c \dtd \right)x^n - \frac{k^2}{6}(-\omega_0^2) \dtt x^n + \frac{k^2}{12}\dtt \dtt x^n = -\omega_0^2 x^n.
\end{equation}
The hard bit left is to find a suitable approximation to $\dtt \dtt$, involving at most a stencil of width 2. This can be accomplised in the following way
\begin{equation}\label{eq:dttsq}
    \dtt \dtt \approx \left(-\omega_0^2 \etm -2c \dtm \right)\left(-\omega_0^2 \etp -2c \dtp \right) = \omega_0^4 + 4c \omega_0^2 \dtd + 4c^2 \dtt.
\end{equation}
Putting it all together, one obtains a fourth-order accurate approximation to \eqref{eq:SHOLoss} as
\begin{equation}
    \left(1 + \frac{k^2}{6}(\omega_0^2 + 2c^2) \right)\dtt x^n = -\omega_0^2\left(1 + \frac{\omega_0^2 k^2}{12} \right) x^n - 2c \left(1 + \frac{\omega_0^2k^2}{6} \right)\dtd x^n.
\end{equation}
Higher-order accurate schemes can may be obtained this way, i.e. finding approximations to $\dtt^p$, involving only operators of width 2. A sketch of the idea is given briefly here. From \eqref{eq:dttsq}, one may construct $\dtt^3$ in the following way:
\begin{align*}
    \dtt\dtt\dtt \approx \left( \omega_0^4 + 4c \omega_0^2 \dtd + 4c^2 \dtt\right)\dtt \approx                                              
    \left( \omega_0^4\etm + 4c \omega_0^2 \dtm + 4c^2 (-\omega_0^2\etm -2c\dtm)\right)\dtt \approx                                          \\\left( \omega_0^4\etm + 4c \omega_0^2 \dtm + 4c^2 (-\omega_0^2\etm -2c\dtm)\right)(-\omega_0^2\etp -2c\dtp)= \\
    (-\omega_0^6 + 4c^2 \omega_0^4) + \left(-6 c \omega_0^4 + 16 c^3 \omega_0^2 \right)\dtd + \left( -8 c^2 \omega_0^2 + 16 c^4\right)\dtt, 
\end{align*}
showing that $\dtt^3$ can be approximated using a stencil of width 2. One may of course use the modified equation technique described above to any desired order. Luckily, the oscillator with loss also posseses an exact solution, where ``exact'' is intended in the same way as for \eqref{eq:SHOexact} (i.e. exact up to the accuracy order of the intial conditions). Considering again the continuous equation with loss, \eqref{eq:SHOLoss}, under the following transformation
\begin{equation}
    X(t) = e^{ct} x(t),
\end{equation}
one gets
\begin{equation}
    \frac{d^2X}{dt^2} + \left(\omega_0^2-c^2 \right) X = 0.
\end{equation}
Thus, the exact scheme \eqref{eq:SHOexactNum} for the undamped oscillator can be applied to the transformed variable $X$. When transformed back to $x$, this gives
\begin{equation}\label{eq:SHO2}
    % \left(\dtt + \frac{2e^{-c k}}{k^2} \left(e^{c k}-\cos\left(\sqrt{\omega_0^2-c^2} k\right)\right) + \frac{e_{t-}}{k^2} (e^{-2c k}-1)\right)x^n = 0.
    \dtt x^n = \left(-\frac{2}{k^2}\left(1 - \cos\left((\sqrt{\omega_0^2-c^2} \, k\right) \right)- \frac{\etp(e^{ck}-1) + \etm(e^{-ck}-1)}{k^2}\right)x^n
\end{equation}
This scheme solves \eqref{eq:SHOLoss} exactly, in particular, the frequency of oscillation and the numerical decay time are exact.



\section{Forced Oscillations}

The equation of the oscillator including loss and source terms is given as
\begin{equation}\label{eq:SHOForced}
    \frac{d^2 x}{dt^2} = -\omega_0^2 x - 2c \frac{dx}{dt} + f(t),
\end{equation}
where $f(t)$ is a time-dependent force per unit mass. Energy analysis leads here to the following energy balance
\begin{equation}
\frac{d}{dt}\left( \frac{m}{2} \left(\frac{dx}{dt}\right)^2 + \frac{K x^2}{2}   \right) = - Q(t) + P(t),
\end{equation}
where $Q(t) = 2mc \left(\frac{dx}{dt}\right)^2$ is the dissipated power, and where $P(t) = m\frac{dx}{dt}f(t)$ is the injected power.




In this case, the system is still linear, but it is not time invariant. Solutions via transform techniques can be obtained, involving the use of the one-sided Laplace transform \eqref{eq:LapT} so to incorporate the effects of the initial conditions and  of the external forcing. In this case, substitution of the simpler \emph{ansatz} \eqref{eq:ansatz} is not possible, because of the presence of the forcing term. Considering $t\geq 0$, the application of the one-sided Laplace transform in \eqref{eq:SHOForced} gives
\begin{equation}
    \int_0^{\infty} \left( \frac{d^2 x}{dt^2} + \omega_0^2 x + 2c \frac{dx}{dt} - f(t)\right)e^{-st} \, \dif t = 0.
\end{equation}
Using integration by parts, one gets (remember that we defined $x_0 = x(0), v_0 = dx(0)/dt$):
\begin{equation}
    \hat x(s)\left( s^2 + 2cs + \omega_0^2\right) = (s+2c)x_0 + v_0 + \hat{f}(s),
\end{equation}
and thus
\begin{equation}
    \hat x(s) = \frac{(s+2c)x_0 + v_0 + \hat{f}(s)}{s^2 + 2cs + \omega_0^2}.
\end{equation}
The expression above may be decomposed into the \emph{transient} and \emph{forced response}. The transient is given by the contribution of the initial conditions only, without external forcing, so:
\begin{equation}
    \hat x(s) = \hat x_{tr}(s) + \hat x_{fr}(s) = \frac{(s+c)x_0 + (v_0 + cx_0)}{s^2 + 2cs + \omega_0^2} + \frac{\hat{f}(s)}{s^2 + 2cs + \omega_0^2}.
\end{equation}
This shows that the contributions of the transient and of the forced response are independent of each other: they add up in the final response. The solution in the time domain is obtained upon inversion of $\hat x(s).$  It is best to write the denominator of $\hat x(s)$ as $(s+c)^2 + \left(\sqrt{\omega_0^2-c^2}\right)^2$, since this is the form reported in \eqref{eq:LaplTtable}.
% \begin{align}
% \mathcal{L}^{-1}\left\{\frac{1}{(s+c)^2 + (\sqrt{\omega_0^2-c^2})^2}\right\} &= \frac{e^{-ct}}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,t\right), \\
% \mathcal{L}^{-1}\left\{\frac{s}{(s+c)^2 + (\sqrt{\omega_0^2-c^2})^2}\right\} &= e^{-ct}\left(\cos\left(\sqrt{\omega_0^2-c^2}\,\,t\right) - \frac{c}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,t\right) \right). 
% \end{align}
Using these (with $a = \sqrt{\omega_0^2-c^2}$), the solution to the transient is
\begin{equation}
    x_{tr}(t) = e^{-ct}\left(x_0 \cos\left(\sqrt{\omega_0^2-c^2}\,\,t\right) + \frac{v_0 + c x_0}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,t\right)\right),
\end{equation}
which is of course the same as \eqref{eq:LossAnsatz} (we did not go through the substitution of the initial conditions there, but the result is the same). For the forced response, we may employ the \emph{convolution} property of the Laplace transform, which states that
\begin{equation}
    \mathcal{L}^{-1}(\hat a(s)\hat b(s)) = \int_{0}^t a(t-u)b(u) \, \dif u,
\end{equation}
and thus, using $\hat a = 1/((s+c)^2 + (\sqrt{\omega_0^2-c^2})^2)$, $\hat b = \hat f$, one gets
\begin{equation}\label{eq:steadyStSHO}
    x_{fr}(t) = \int_0^t \frac{e^{-c(t-u)}}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,(t-u)\right) f(u) \, \dif u. 
\end{equation}
This integral is not generally computable analytically. However, it also encapsulates the idea that the forced response to \emph{any} forcing can be obtained as the convolution with the \emph{impulse response}. To that end, considering $f(t)=\delta(t-t_0)$, with delta being Dirac's delta here, and denoting as per usual the initial time by $t_0$, one gets from \eqref{eq:steadyStSHO}:
\begin{equation}\label{eq:GreenSHO}
    G(t|t_0) = \frac{e^{-c(t-t_0)}}{\sqrt{\omega_0^2-c^2}}\sin\left(\sqrt{\omega_0^2-c^2}\,\,(t-t_0)\right), \,\,\,\, t\geq t_0,
\end{equation}
and $G$ is zero for $t<t_0$. The symbol $G(t|t_0)$ denotes the \emph{Green's function} of the harmonic oscillator, i.e. the  response to a Dirac impulse at $t=t_0$. In particular, for zero initial conditions, the forced response is also the total response of the system. 
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/ForcedOsc.png}
    \caption{Time evolution of forced oscillator. Dashed-dotted line is response to initial conditions, dahsed line is Green's function \eqref{eq:GreenSHO}, solid line is total response. The oscillator is activated with a Dirac impulse at $t_0=0$. Initial conditions are $x_0=-0.01$ m, $v_0=0.04$ m/s. The natural frequency of the oscillator is $\omega_0 = 100$ rad/s, and the decay time is $\tau_{60} = 5$ s.}
\end{figure}


The stability of system \eqref{eq:SHOForced} may be adapted to include the effects of external forcing. We are not going to bother as much here, and we will assume that the solution will not \virg{blow up} in a finite time if the source remains bounded. 
In particular, if the source is itself bounded, we remark that integral \eqref{eq:steadyStSHO} remains bounded. 





\subsection{Response to harmonic input forcing}

Consider now a harmonic input of the form
\begin{equation}\label{eq:harmoForceLinear}
    f(t) = F e^{j\omega t},
\end{equation}
with $F \in \mathbb{C}$ being a complex forcing amplitude, and $\omega \in \mathbb{R}^+_0$ being the input forcing radian frequency (not to be confused with the natural frequency of the oscillator). We are only going to assume that the oscillator will eventually fall into a steady-state here, since from the previous discussion we know that the transient response will die out after sufficient time has elapsed, and since the forcing is of harmonic type. According to \eqref{eq:steadyStSHO}, one may compute the steady-state via the convolution integral. However, in this case one may equivalently assume that the steady state vibrates at the frequency of the input, thus
\begin{equation}
    x(t) = X e^{j\omega t},
\end{equation}
where we removed the index $st$ since we are now assuming that the transient has completely died out, and thus we can identify the whole solution $x(t)$ of \eqref{eq:SHOForced} with the steady-state. $X$ is here a complex amplitude. Substituting into the equation of motion results in
\begin{equation}\label{eq:TransXF}
    \frac{X}{F} = \frac{1}{\omega_0^2-\omega^2+2jc\omega} =  \frac{\omega_0^2-\omega^2-2jc\omega}{(\omega_0^2-\omega^2)^2+4c^2\omega^2}.
\end{equation}
Since $X,F$ are complex constants, the tangent of the phase angle  is obtained as the ratio between the imaginary and real parts, i.e. 
\begin{equation}
    \tan \left(\angle \frac{X}{F}\right) = -\frac{2c\omega}{\omega_0^2 - \omega^2}
\end{equation}
One should pay attention to the sign of the denominator when inverting the tangent function. Hence, the phase starts out at $0$ when $\omega \approx 0$; then it reaches $-\pi/2$ when $\omega \approx \omega_0$, and then approaches $-\pi$ as $\omega \rightarrow \infty$. The absolute value of the transfer function is obtained as
\begin{equation}\label{eq:XFlinearOsc}
    \left|\frac{X}{F}\right|  = \left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2},
\end{equation}
which has a maximum at $\omega = \omega_0 \left(1-2(c/\omega_0)^2 \right)^{1/2}$. The maximum is 
\begin{equation}
    \left|\frac{X}{F}\right|(\omega = \omega_0 \left(1-2(c/\omega_0)^2 \right)^{1/2}) \approx \frac{1}{2c\omega_0},
\end{equation}
where factors of the order $O(c^4)$ were disregarded. The \emph{bandwidth} of the transfer function is defined as the interval in frequency occurring between the frequencies $\omega_+,\omega_-$ that are found at $\frac{1}{\sqrt{2}}$ the maximum (these are the \emph{half-power points}, since power is the square of the absolute value). To obtain $\omega_\pm$, one simply uses this defintion, hence:
\begin{equation}
    \left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2} = \left(2\sqrt{2} c\omega_0\right)^{-1}.
\end{equation}
Solving for $\omega$, and disregarding small terms, one gets
\begin{equation}
    \omega_\pm \approx \omega_0\left(1 \pm {c} \right), \quad \rightarrow (\omega_+ - \omega_-) \approx 2c.
\end{equation}
This shows that, for small damping values, one may recover the value of the decay time from the frequency response, after measuring the bandwith of the peak in the transfer function. As a cautionary note, the transfer function $X/F$ was obtained here in the case of sinusoidal input forcing, by \virg{scanning} the frequency axis. The same transfer function may however be obtained via the impulse response \eqref{eq:GreenSHO}. To show this, it is sufficient to compute a Fourier transform and, again, we may resort to tables for this. Considering the transforms \eqref{eq:FouTtable} (with $a = \sqrt{\omega_0^2 - c^2}$, $x(t) = e^{-ct}$), one gets for the impulse response \eqref{eq:GreenSHO}
% \begin{equation}
% \mathcal{F}\left\{f(t)\sin(at)\right\}(\omega) = \frac{\hat f(\omega-a)-\hat f(\omega+a)}{2i}, \quad \mathcal{F}\left\{e^{-ct}| t \geq 0, c \geq 0\right\}(\omega) = \frac{1}{\sqrt{2\pi}(c+j\omega)}.
% \end{equation}
% Using these two identities it is possible to transform the impulse response \eqref{eq:GreenSHO}, using $f(t) = e^{-ct}$, $a = \sqrt{\omega_0^2-c^2}$. The result is
\begin{equation}
    \mathcal{F}\left\{G(t|t_0=0)\right\}(\omega) = \frac{1}{\sqrt{2\pi}}\frac{1}{(\omega_0^2-\omega^2)+2jc\omega} = \frac{1}{\sqrt{2\pi}}\frac{\omega_0^2-\omega^2-2jc\omega}{(\omega_0^2-\omega^2)^2+4c^2\omega^2},
\end{equation}
that is the same as \eqref{eq:TransXF} (up to a constant factor). Thus, knowledge of the impulse response is equivalent to \virg{scanning} the frequency axis one frequency at a time. This equivalence is often employed experimentally, where the impulse response is obtained via deconvolution of appropriate sine sweeps.


\subsubsection{Mechanical impedance}

Though \eqref{eq:TransXF} expresses the general relationship between output displacement and input forcing, it may be preferable to obtain the transfer function between output velocity and input forcing. Thus,
\begin{equation}
    \frac{dx(t)}{dt} = j\omega e^{j\omega t} \triangleq V e^{j\omega t}. 
\end{equation}
Using this in \eqref{eq:TransXF} results in
\begin{equation}
    \frac{V}{F} = \frac{j\omega(\omega_0^2 - \omega^2)-2c\omega^2}{(\omega_0^2-\omega^2)^2+4c^2\omega^2}.
\end{equation}
The ratio $V/F$ (often denoted $Y$) is the  \emph{mechanical admittance.} The inverse $F/V$ (often denoted $Z$) is the \emph{mechanical impedance.} The reason why it may be preferable to work with the impedance (or the admittance), rather than $X/F$, is that velocity and force and power-conjugated quantities: in an energy framework, knowledge of the impedance/admittance allows to describe mechanical systems in an energy-consistent manner, as we will see in due course. For the phase angle, one has
\begin{equation}
    \tan \left(\angle \frac{V}{F}\right) = \frac{\omega_0^2 - \omega^2}{(-2c\omega)}.
\end{equation}
Thus, in this case the phase angle starts out at $\pi/2$ when $\omega \approx 0$, then it goes to zero when $\omega \approx \omega_0$, and then it approaches $-\pi/2$ as $\omega \rightarrow \infty$. The absolute value is given by
\begin{equation}
    \left| \frac{V}{F} \right| = \omega \left((\omega_0^2-\omega^2)^2+4c^2\omega^2\right)^{-1/2}.
\end{equation}
Differentiating with respect to $\omega$, one gets a maximum at $\omega = \omega_0$. The maximum is 
\begin{equation}
    \left|\frac{V}{F}\right|(\omega = \omega_0) = (2c)^{-1}.
\end{equation}
\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/ImpedanceSHO.png}
    \caption{Absolute values and phase angles of the transfer functions for the linear oscillator. The natural frequency of the oscillator is $\omega_0 = 100$ rad/s, the decay times are set as $\tau_{60}=\left\{1.0,1.2,1.4,1.6,1.8,2.0\right\}$ s}\label{fig:LinearTransFunctPlots}
\end{figure}



\subsubsection{Finite difference schemes}
\begin{figure}[hbt]
    \centering
    \includegraphics[width = 0.85\linewidth, clip, trim={1cm 0cm 1cm 0cm}]{Figures/SlopesForced.pdf}
    \caption{Error slopes of scheme \eqref{eq:shoFDForced}. The oscillator is activated with a Dirac delta at $t_0=0$. Initial conditions are $x_0=-0.01$ m, $v_0=0.04$ m/s. The natural frequencies of the oscillator are selected as $\omega_0 = 100$ rad/s (solid), $\omega_0 = 200$ rad/s (dashed), $\omega_0 = 300$ rad/s (dash-dotted), and the decay time is $\tau_{60} = 5$ s.}\label{fig:ErrSlopesForced}
\end{figure}
Finite difference schemes may be obtained in this case by any discretisation of $f(t)$, of the desired accuracy. For second-order accuracy, $f^n = \left\{f(t_n),\mtd f(t_n), \mtt f(t_n) \right\}$, are all valid discretisations. Hence, a suitable second-order accurate scheme is obtained as
\begin{equation}\label{eq:shoFDForced}
    \dtt x^n = -\omega_0^2 x^n -2c\dtd x^n + f^n
\end{equation}
Initialisation should be performed in such a way that second-order accuracy is preserved. In this respect, one may set $x^0 = x_0$. Then,
\eqref{eq:dtdInit} is used again, to get
\begin{equation}
    (\dtp - \frac{k}{2}\dtt)x^0 = v_0.
\end{equation}
One has
\begin{equation}
    \dtt x^0 \approx -\omega_0^2 x_0 - 2c \dtp x^0 + f^0.
\end{equation}
Using these, one can extract $x^1$ as 
\begin{equation}
    x^1 = x^0 + \frac{k v_0 + \frac{k^2}{2}\left(-\omega_0^2 x^0 + f^0 \right)}{1+kc}
\end{equation}
When approximating a Dirac delta at $t_0=0$, one should use $f^0 = 2/k$. Error slopes for scheme \eqref{eq:shoFDForced} are shown in Fig.  \ref{fig:ErrSlopesForced}.


\chapter{Nonlinear Oscillators}

The harmonic oscillator equation \eqref{eq:SHOreal} is a useful starting point to model physical vibrations. However, it is in most cases only an approximation of the actual physical behaviour of oscillating systems. Examples are given in Fig. \ref{fig:oscExamples} (c) and (d). The pendulum is, in fact, a useful case study to approach the study of nonlinearities. Considering the pendulum sketched in panel $(d)$ of Fig. \ref{fig:oscExamples}, and assuming that the rope has length $l$, balancing torques with respect to the axis of rotation gives
\begin{equation}
    \frac{d^2\theta}{dt^2} = - \frac{g}{l}\sin \theta \approx -\frac{g}{l}\left(\theta - \frac{\theta^3}{6} \right),
\end{equation}
which clearly is a nonlinear equation in $\theta$. Here, $g$ is the acceleration due to gravity. Note that the equation is independent of the mass $m$.

\section{Unforced, Lossless Duffing Equation}

The pendulum example suggests that it may be useful to study the behaviour of an equation of the kind
\begin{equation}\label{eq:DuffNoLoss}
    \frac{d^2 x}{dt^2} = -\omega_0^2 x - \gamma x^3,
\end{equation}
where we are going to assume that $|\gamma|/\omega_0^2 \triangleq |\epsilon|$ is small. This equation is known as \emph{Duffing equation}, and it is used to model the nonlinear oscillations of a variety of oscillating systems, whenever linear analysis results insufficient. The equation may be rescaled in time so to yield a natural parametrisation in terms of the small parameter $\epsilon$. To that end, consider the change of time scale $\bar t = \omega_0 t$, giving $\frac{d}{dt} = \omega_0 \frac{d} {d \bar t}$. Using scaled time, one gets
\begin{equation}
 \frac{d^2 x}{d\bar t^2} = - x - \epsilon x^3.
\end{equation}
In the following, the dimensional form \eqref{eq:DuffNoLoss} will be retained, and we shall use $\gamma$ as ``small'' parameter in the analysis. We shall resume to the $\epsilon$ notation for the forced oscillator, in Sec. \ref{sec:ForcedDuffing}.
 
 
From Sec. \ref{sec:EnAnGen}, one may show that \eqref{eq:DuffNoLoss} conserves a mechanical energy of the kind
\begin{equation}\label{eq:EnContDuffNoLoss}
    H_0 = \frac{m}{2}\left(\frac{dx}{dt} \right)^2 +  \frac{K x^2}{2} + \phi \triangleq \frac{m}{2}\left(\frac{dx}{dt} \right)^2 + \frac{K x^2}{2} + \frac{m\gamma x^4}{4},
\end{equation}
where multiplication by $m$ was performed so to restore the physical units of energy, and where $K = m\omega_0^2$. In the above, the linear part was separated from the nonlinear potential $\phi$, as it may be advantageous to perform numerical integration via such splitting, as will be seen below. The sign of $\gamma$ determines whether the systems displays \emph{hardening} ($\gamma > 0$) or \emph{softening} ($\gamma < 0$) behaviour. 
\begin{figure}
    \includegraphics[width=\linewidth]{Figures/PhasePortrDuff.pdf}
    \caption{Phase portraits of lossless Duffing oscillator \eqref{eq:DuffNoLoss}. Left: softening behaviour ($\gamma <0)$, dots indicate the saddles located at $x_s = \pm \omega_0/|\gamma|^{1/2}$. Right: hardening behaviour ($\gamma > 0$).}\label{fig:PhasePortDuff}
\end{figure}
Under hardening conditions, the potential energy is non-negative $\forall x \in \mathbb{R}$, hence motion is always periodic, with period of oscillation depending on the amplitude of motion, as will be seen shortly. Under softening behaviour, motion can be qualitatively very different. It may be useful to draw a more detailed analysis of the phase portrait there than what we already did in Sec. \ref{sec:EnAnGen}. To that end, we are going to compute the \emph{equilibrium points} of the system, defined as the points in phase space where both the velocity and the acceleration are zero. Hence, setting to zero the right-hand side of \eqref{eq:DuffNoLoss} gives
\begin{equation}
    -x\left(\omega_0^2 + \gamma x^2\right) = 0,
\end{equation}
showing that there is only one equilibrium point at $(x,dx/dt)=(0,0)$ for the hardening case, and three equilibrium points, $(x,dx/dt) = \left\{(0,0), (\pm x_s,0)\right\}$, with $x_s = \omega_0/|\gamma|^{1/2}$ for the softening case. The equilibria are denoted as dots in Fig. \ref{fig:PhasePortDuff}. For the softening case, motion may be bounded or unbounded, depending on the amplitude of the motion. Clearly, the equilibria at $\pm x_s$ represent some kind of boundary between these types of motion. To see this, it may be useful to expand in a Taylor series the right-hand side of  \eqref{eq:DuffNoLoss} around $\pm x_s$, keeping only linear terms. (This means that we are starting the system using small initial conditions around the equilibria, and we observe the behaviour of the oscillator). Hence
\begin{equation}
    \frac{d^2 x}{dt^2} \approx -\omega_0^2 x -3\gamma x x_s^2 = -\left(\omega_0^2 + 3 \gamma x_s^2\right) = 2\omega_0^2 x, 
\end{equation}
which is solved by $x(t) = A e^{\sqrt{2}\omega_0 t} + B e^{-\sqrt{2}\omega_0 t}$, with $A,B \in \mathbb{R}$. Thus, around the equilibria at $x=\pm x_s$ the mass is either drawn exponentially toward the equilibrium, or away from it: there is no oscillation. If we are interested in computing the region of existence of oscillating solutions, then we may start from the knowledge of the equilibrium points, and compute the trajectories of constant energy that intesect such points in phase space. The total energy is given in \eqref{eq:EnContDuffNoLoss}. Setting $dx/dt=0$, and $x = \pm x_s$, one has
\begin{equation}
    H_0 = \frac{K x_s^2}{2} + \frac{m\gamma x_s^4}{4} = \frac{m \omega_0^4}{2|\gamma|} + \frac{m \gamma \omega_0^4}{4|\gamma|^2} = \frac{m \omega_0^4}{4|\gamma|} = - \frac{m \omega_0^4}{4\gamma}
\end{equation}
From \eqref{eq:dxdtEn}, one has
\begin{equation}
    \frac{dx(x)}{dt} = \pm \sqrt{\frac{|\gamma|}{2}}\left(x^2-x_s^2 \right),
\end{equation}
showing that the curves separating bounded from unbounded motion are parabolas in the $(x,dx/dt)$-plane. These are sketched in Fig. \ref{fig:PhasePortDuff}. Hence, if motion is started with initial conditions within the region included inside the intersecting parabolas, motion will be periodic; else, it will be unbounded.



\subsection{The Lindstedt-Poincar{\'e} Method}


When solutions are periodic, one may be interested in computing the period of the vibration. When studying the simple harmonic oscillator, one is taught that the period is independent of the amplitude of the motion. This is in fact not true, since for any physical oscillator, such as the pendulum, higher-order correction terms in the Taylor expansion of the acceleration are such that the period depends on the amplitude of the motion. Many analytical methods are available in the case of a single scalar nonlinear equations such as \eqref{eq:DuffNoLoss}, to compute approximate amplitude-frequency relationships. Here, we are going to use a technique, due to Lindstedt and Poincar{\'e}. At a very basic level, this method assumes that the solution may be expanded out as
\begin{equation}\label{eq:ExpGmX}
    x(t) = y_0(t) + \gamma y_1(t)  + O(\gamma^2),
\end{equation}
where $y_0,y_1$ are unknown functions of $t$ (to be solved for). One of course may add more terms to the expansion above, though we are only going to consider the first two terms in this example. Then, \eqref{eq:DuffNoLoss} is \emph{rescaled} in time via the non-dimensional time $\bar t = t / \tau$, where $\tau$ is a time constant to be determined. Since the motion is periodic, periodicity is enforced here as
\begin{equation}\label{eq:PeriodDuffNoLoss}
    x(\bar t + 2\pi) = x(\bar t),
\end{equation}
The choice of a period of $2\pi$ is completely arbitrary, since ultimately a different choice for the period merely amounts to rescaling the unknown time constant $\tau$. Then, $\tau$ is also expanded out in powers of $\gamma$, so that
\begin{equation}\label{eq:ExpGmTau}
    \tau = \tau_0 + \gamma \tau_1 + O(\gamma^2).
\end{equation}
Using scaled time, \eqref{eq:DuffNoLoss} becomes
\begin{equation}
    \frac{d^2 x}{d \bar t^2} = -\tau^2\omega_0^2 x - \tau^2\gamma x^3.
\end{equation}
We are going to assume that the initial conditions are $x(0) = x_0$, $dx(0)/dt = 0$, without loss of generality. Using expansions \eqref{eq:ExpGmX}, \eqref{eq:ExpGmTau} in the above, one gets
\begin{equation}
    \frac{d^2}{d\bar t^2}\left(y_0 + \gamma y_1 \right) = - \omega_0^2 \left(\tau_0 + \gamma \tau_1\right)^2 \left(y_0 + \gamma y_1 \right) - \gamma \left(\tau_0 + \gamma \tau_1\right)^2 \left(y_0 + \gamma y_1 \right)^3.
\end{equation}
When the products are expanded out, and terms up to $O(\gamma)$ are kept, comparing terms with equal powers of $\gamma$ gives the following
\begin{subequations}
    \begin{align}
          \frac{d^2 y_0}{d\bar t^2} &= -\omega_0^2 \tau_0^2 y_0, \label{eq:LindPoin1}\\
          \frac{d^2 y_1}{d\bar t^2} &= -\omega_0^2 \left(\tau_0^2 y_1 + 2 \tau_1\tau_0 y_0\right) - \tau_0^2 y_0^3. \label{eq:LindPoin2}
    \end{align}
\end{subequations}
The first equation is solved immediately via the techniques already detailed in Chapter \ref{chap:SHO}. Hence
\begin{equation}
y_0(\bar t) = x_0 \cos(\omega_0 \tau_0 \bar t),
\end{equation}
where we assumed initial conditions of the kind $y_0(0) = x_0$, $dy_0(0)/d\bar t = 0$ (i.e. the initial conditions for $x(t)$). Enforcing \eqref{eq:PeriodDuffNoLoss} gives
\begin{equation}
x_0 \cos(\omega_0\tau_0(\bar t+2\pi)) = x_0 \cos(\omega_0\tau_0\bar t),
\end{equation}
which sets $\tau_0 = 1/\omega_0$. Substituting the values of $\tau_0$, $y_0$ in \eqref{eq:LindPoin2} gives
\begin{equation}\label{eq:tempDuff1}
\frac{d^2 y_1}{d\bar t^2} = - y_1 - 2x_0 \omega_0\tau_1 \cos(\bar t) - \frac{x_0^3}{\omega_0^2}\cos^3(\bar t) .
\end{equation}
This is a harmonic oscillator equation, with an external, time-dependent term. Solutions to the equation are given by a superposition of the solution to the \emph{homogeneous} equation (i.e. with all external terms set to zero), plus a particular solution to the \emph{non-homogeneous} equation. The homogeneous equation is solved immediately via the techniques already detailed in Chapter \ref{chap:SHO}. For the non-homogeneous equation, we are going to use some intuition. First, the cubic term is written as
\begin{equation}
\cos^3 (\bar t) = \frac{\left(e^{j\bar t} + e^{-j\bar t}\right)^3}{8} = \frac{e^{3j\bar t} + e^{-3j\bar t}}{8} + \frac{3 e^{j\bar t} + 3  e^{-j\bar t}}{8} = \frac{1}{4}\cos(3 \bar t) + \frac{3}{4}\cos(\bar t).
\end{equation}
Hence, \eqref{eq:tempDuff1} becomes
\begin{equation}\label{eq:tempDuff2}
\frac{d^2 y_1}{d\bar t^2} = - y_1 - \left(2x_0\omega_0\tau_1 + \frac{3 x_0^3}{4\omega_0^2}\right)\cos(\bar t) - \frac{x_0^3}{4\omega_0^2}\cos(3\bar t).
\end{equation}
For a particular solution, an educated guess may be
\begin{equation}
y_1(\bar t) = A(\bar t) \cos(\bar t) + B(\bar t) \sin(\bar t) + C(\bar t) \cos(3 \bar t) + D(\bar t) \sin(3 \bar t).
\end{equation} 
Inserting this guess into \eqref{eq:tempDuff2}, and equating coefficients, results in
\begin{subequations}
\begin{align}
\frac{d^2 A}{d\bar t^2} + 2 \frac{d B}{d\bar t} &= - \left(2x_0\omega_0\tau_1 + \frac{3 x_0^3}{4\omega_0^2}\right), \\
\frac{d^2 B}{d\bar t^2} - 2 \frac{d A}{d\bar t} &= 0, \\
\frac{d^2 C}{d\bar t^2} + 6 \frac{d D}{d\bar t} - 8 C &= -\frac{x_0^3}{4\omega_0^2}, \\
\frac{d^2 D}{d\bar t^2} - 6 \frac{d C}{d\bar t} - 8 D &= 0.
\end{align}
\end{subequations}
This is solved by $A = 0$, $B = -\frac{\bar t}{2}\left( 2x_0\omega_0\tau_1 + \frac{3 x_0^3}{4\omega_0^2}\right )$, $C = \frac{x_0^3}{32\omega_0^2}$, $D=0$. Hence, the full solution of \eqref{eq:tempDuff2}, consistents with the initial conditions $y_1(0) = dy_1(0)/d\bar t=0$, is
\begin{equation}
y_1(\bar t) = \frac{x_0^3}{32\omega_0^2}\left(-\cos(\bar t) + \cos(3 \bar t) \right) - \frac{\bar t}{2}\left(2x_0\omega_0\tau_1 + \frac{3x_0^3}{4\omega_0^2} \right)\sin(\bar t).
\end{equation}
The last term in the expression (proportional to time $\bar t$), is called a \emph{secular} term, which grows over time and which does not allow periodic motion to take place. Hence, its coefficient must be set to zero, yielding ultimately an expression for $\tau_1$ as
\begin{equation}
\tau_1 = -\frac{3a^2}{8\omega_0^3}. 
\end{equation}
Putting all together, one has
\begin{equation}\label{eq:LinPoinSol}
x(t) = x_0\left(1 + \frac{\gamma x_0^2}{32\omega_0^2} \right) \cos(\omega t) - \frac{\gamma x_0^3}{32 \omega_0^2}\cos(3\omega t) + O(\gamma^2), 
\end{equation}
with $\omega = \omega_0\left(1 + \frac{3\gamma x_0^2}{8 \omega_0^2}\right)$. This shows that the solution presents at least one higher partial, with a period three times as short as the fundamental period, and that the fundamental frequency depends on the amplitude of the initial conditions, $x_0$. Furthermore, for hardening behaviour, the fundamental frequency increases with the amplitude, and the converse is true for the softening case.


\subsection{Method of multiple scales}\label{sec:MultScales}

Another popular analytical method is known as the \emph{method of multiple scales.} This method assumes the same expansion \eqref{eq:ExpGmX} for $x(t)$. However, time is not rescaled in terms of a constant $\tau$ to be determined, rather, multiple time scales are defined as 
\begin{equation}
\tau_n = \gamma^n t, \quad n \geq 0.
\end{equation}
Then, from the chain rule, one has $d/dt = \partial/\partial{\tau_0} + \gamma \partial/\partial {\tau_1} + O(\gamma^2)$. Hence, the solution is assumed to be a function of the multiple time scales, as
\begin{equation}\label{eq:MultScalesExp}
x(\tau_0,\tau_1) = y_0(\tau_0,\tau_1) + \gamma y_1(\tau_0,\tau_1) + O(\gamma^2),
\end{equation}
and this will coincide with the true solution for $\tau_0 = t$, $\tau_1 = \gamma t$. Substituting this expansion into the equation of motion, and equating equal powers of $\gamma$, one obtains
\begin{subequations}
    \begin{align}
          \frac{\partial^2 y_0}{\partial \tau_0^2} + \omega_0^2 y_0 &= 0, \label{eq:MultScales1}\\
          \frac{\partial^2 y_1}{\partial\tau_0^2} + \omega_0^2 y_1 &=- 2\frac{\partial^2 y_0}{\partial \tau_0 \partial \tau_1} - y_0^3. \label{eq:MultScales2}
    \end{align}
\end{subequations}
The solution to \eqref{eq:MultScales1} can be obtained immediately using the techniques already discussed at the end of Sec. \ref{sec:EnAnGen}. It is convenient, in this case, to use a complex exponential form, such that
\begin{equation}\label{eq:MultScTemp1}
y_0 = A (\tau_1) e^{j\omega_0 \tau_0} + \text{cc},
\end{equation}
where cc denotes the complex conjugate. Substituting this expression into \eqref{eq:MultScales2}, one gets
\begin{equation}
\frac{\partial^2 y_1}{\partial\tau_0^2} + \omega_0^2 y_1 = -2\left(j\omega_0 \frac{dA}{d\tau_1}e^{j\omega_0\tau_0} + \text{cc} \right) - \left(A^3 e^{3j\omega_0 \tau_0} + 3 |A|^2 A e^{j\omega_0 \tau_0} + \text{cc} \right),
\end{equation}
Just like we saw for the Lindstedt-Poincar{\'e} method, secular terms will appear if the coefficients of the forcing terms oscillating at $\omega_0$ are not set to zero. Hence, one must set
\begin{equation}
2j\omega_0\frac{dA}{d\tau_1} + 3|A|^2 A = 0.
\end{equation}
Using $A = \alpha e^{j\beta \tau_1}$, one has 
\begin{equation}
\beta = \frac{3 \alpha^2}{2\omega_0}\gamma t .
\end{equation}
Hence, using this in \eqref{eq:MultScTemp1}, one gets
\begin{equation}
y_0 = \alpha \left(e^{j\left(\frac{3 \alpha^2 \gamma}{2\omega_0}+ \omega_0 \right)t} + \text{cc} \right) = 2\alpha \cos\left(\frac{3 \alpha^2 \gamma t}{2\omega_0}+ \omega_0 t\right).
\end{equation}
Using $y_0(0) = x_0$, $dy_0(0)/dt = 0$, one gets $\alpha = x_0/2$, from which
\begin{equation}
x(t) = x_0 \cos\left(\omega t\right) + O(\gamma),
\end{equation}
with $\omega = \omega_0\left(1 + \frac{3\gamma x_0^2}{8 \omega_0^2}\right)$. This is the same as \eqref{eq:LinPoinSol}, up to terms proportional to $\gamma$. Adding more terms in expansion \eqref{eq:MultScalesExp} allows to obtain further correction terms in the above equation. 







\subsection{Closed-form solution in terms of Jacobi elliptic functions}

The Lindstedt-Poincar{\'e} method, just like the method of multiple scales, (along with other analogous approximate analytical mehtods) are powerful tools allowing to draw quantitative conclusions regarding nonlinear oscillating systems. When more terms are added to the expansion, more accurate solutions can be obtained, though the algebra quickly becomes unwieldy. The Duffing equation \eqref{eq:DuffNoLoss} possesses in fact an analytic solution, generalising the solution of the simple harmonic oscillator. The special functions that solve the Duffing equation are called \emph{Jacobi elliptic functions.} Consider the following relationship, relating $t$ to the angle $\phi$
\begin{equation}
t = \int_0^\phi \frac{\dif \theta}{\sqrt{1-m\sin^2\theta}}.
\end{equation}
Then, the two Jacobi functions $\text{cn}$ and $\text{sn}$ are defined as
\begin{equation}
\text{cn}(t;m) = \cos\phi, \quad \text{sn}(t;m) = \sin \phi.
\end{equation}
Here, $0<m<1$ is a parameter. It is seen that, in the limit of $m\rightarrow 0$, the elliptic function reduce to the ordinary trigonometric functions. Differentiating under the integral sign gives
\begin{equation}
\frac{dt}{d\phi} = \frac{1}{\sqrt{1-m\sin^2\phi}}. 
\end{equation}
Then
\begin{equation}
\frac{d\text{cn}(t;m)}{dt} = \frac{d\cos\phi}{d\phi}\left(\frac{dt}{d\phi}\right)^{-1} = -\sin\phi \sqrt{1-m\sin^2\phi} \triangleq - \text{sn}(t;m)\text{dn}(t;m),
\end{equation}
where the Jacobi function $\text{dn}(t;m) = \sqrt{1-m\,\text{sn}^2(t;m)}$ was introduced. With these definitions, it is easy to verify the following identites:
\begin{subequations}
\begin{align}
\text{sn}^2+\text{cn}^2 &= 1, \\
\text{dn}^2 &= 1 - m \, \text{sn}^2,\\
\frac{d\text{cn}}{dt}&= -\text{sn}\,\text{dn} \\
\frac{d\text{sn}}{dt} &= \text{cn}\,\text{dn} \\
\frac{d\text{dn}}{dt} &= - m\, \text{sn}\,\text{cn}.
\end{align}
\end{subequations}
Using these identities, one can easily show that
\begin{equation}
x(t) = x_0 \, \text{cn}\left( \sqrt{\omega_0^2 + \gamma x_0^2} \, t ; \, \frac{\gamma x_0^2}{2\gamma x_0^2 + 2\omega_0^2}\right)
\end{equation}
solves \eqref{eq:DuffNoLoss} exactly, with intial conditions $x(0) = x_0$, $dx(0)/dt = 0$. This solution is useful in order to check the order accuracy of the numerical schemes given below (the Jacobi elliptic functions exist in Matlab).




\section{Finite Difference Schemes}


Numerical integration of \eqref{eq:DuffNoLoss} may be performed using a number of schemes. To start with, we are now going to consider  the hardening case ($\gamma > 0$), which poses less difficulties in terms of energy analysis. The softening case will be treated below using arguments similar to those encountered in the continuous case, i.e. by checking whether the numerical schemes possess closed orbits in numerical phase space, and selecting initialisation parameters accordingly. Three  schemes are given as
\begin{subnumcases}{\dtt x^n+\omega_0^2 x^n = \label{eq:subnum}}
 - \gamma (x^n)^3 \label{eq:schemesDuffingA}\\
 - \gamma (x^n)^2 \mtd x^n \label{eq:schemesDuffingB}\\
 - \gamma \mtd\left((x^n)^2\right)\mtd x^n \label{eq:schemesDuffingC}
\end{subnumcases}
It is remarked that the three schemes are second-order accurate, in that the local truncation error is $\varepsilon^n = O(k^2)$ for all schemes. The first scheme is a trivial implementation of the difference operators. The second and third schemes are slightly more involved, employing an implicit discretisation of the nonlinearity. Expanding out the operators, the updates are as
\begin{subequations}
\begin{align}
x^{n+1} &= \left(2-k^2\omega_0^2 -k^2\gamma (x^n)^2\right) x^n - x^{n-1}, \\
\left(1+\frac{k^2 \gamma (x^n)^2}{2}\right)x^{n+1} &= \left(2-k^2\omega_0^2\right)x^n -\left(1+\frac{k^2 \gamma (x^n)^2}{2}\right)x^{n-1},  \label{eq:DuffLinearlyImpl} \\
g(x^{n+1}) &= (2-\omega_0^2k^2)x^n - \left(1+\frac{\gamma k^2}{4}(x^{n-1})^2\right)x^{n-1} \triangleq f(x^n,x^{n-1}),  \label{eq:DuffImpl}
\end{align}
\end{subequations}
where 
\begin{equation}\label{eq:gdefDuffImpl}
g(x^{n+1}) = \frac{\gamma k^2}{4}(x^{n+1})^3 + \frac{\gamma k^2 x^{n-1}}{4}(x^{n+1})^2+\left(1+\frac{\gamma k^2(x^{n-1})^2}{4} \right)x^{n+1}.
\end{equation}
The first scheme, as expected, is fully explicit. The second scheme is \emph{linearly implicit}, in that division by a state-dependent coefficient is required at each time step. The third scheme is $\emph{fully implicit}$, in that one needs to solve a cubic equation at each time step. In this simple case, the computational requirements of the first two schemes are mostly comparable. The third scheme (written in the form \eqref{eq:ImplSchemeDef}) is consirably more expensive to run, since a nonlinear algebraic equation needs to be performed at each time step. Aside from efficiency requirements, the schemes have considerably different energy properties. Multiplying all three schemes  by $m \dtd x^n$, gives an energy balance as
\begin{align}\label{eq:EnergySchemesDuffing}
\dtp \mathfrak{h}^{n-1/2} \triangleq \dtp \left( \frac{m(\dtm x^n)^2}{2} +\frac{K x^n \etm x^n }{2} + \phi^{n-1/2}\right)  = 0,
\end{align}
where $\phi$ has the following forms for the three schemes:
\begin{subnumcases}{\phi^{n-1/2} = }
\phi^{1/2}+\sum_{p=2}^{n} m k \gamma (\dtd x^p) (x^p)^3, \\ 
\frac{m\gamma}{4}(x^{n})^2\etm (x^{n})^2, \label{eq:NonlinEnDuff2}\\
\frac{m\gamma}{4}\mtm(x^{n})^4.
\end{subnumcases}
The energy expression in \eqref{eq:EnergySchemesDuffing} (regardless of the particular form for $\phi$) is a discrete counterpart of \eqref{eq:EnContDuffNoLoss}. The energy of scheme \eqref{eq:schemesDuffingA} is obtained here as pure integration from the beginning of time ($\phi^{1/2}$ being a consistent constant of integration), and is unavailable in compact form involving only two steps of the state $x^n$. Hence, nothing can be said about the sign of the energy overall, which may become negative. For schemes \eqref{eq:schemesDuffingB} and \eqref{eq:schemesDuffingC} a conserved, non-negative nonlinear energy exists, and thus, non-negativity of the total energy \eqref{eq:EnergySchemesDuffing} is obtained under the same arguments as for the simple harmonic oscillator. In particular, \eqref{eq:StabCondSHO} hold here too. This is easily shown via the following argument:
\begin{equation}
\mathfrak{h}^{n-1/2} = \frac{m(\dtm x^n)^2}{2} +\frac{K x^n \etm x^n }{2} + \phi^{n-1/2} \geq \frac{m(\dtm x^n)^2}{2} +\frac{K x^n \etm x^n }{2},
\end{equation}
where the last inequality follows from non-negativity of $\phi$. Hence, the total energy of the nonlinear oscillator is bounded from below by the energy of the simple harmonic oscillator.
\begin{figure}
\includegraphics[width=\linewidth]{Figures/PhasePortrDuffNumerical.pdf}
\caption{Numerical phase portraits for scheme \eqref{eq:schemesDuffingB}. Left: softening behaviour ($\gamma <0$); dots indicate the saddles located at $(0,0),(\pm x_s,\pm x_s)$ with $x_s = \pm \omega_0/|\gamma|^{1/2}$. Right: hardening behaviour ($\gamma > 0$). }\label{fig:NumPhaseSpDuff}
\end{figure}


\subsection{Softening Case Analysis in Numerical Phase Space}

When $\gamma < 0$, the energy analysis of the finite difference schemes needs to be extended so to yields conditions under which the system is oscillating. One may again start with the calculation of the equilibrium points in numerical phase space, that can be conveniently set as the $(x^n,x^{n-1})-$plane. We are going to consider here the linearly implicit scheme \eqref{eq:schemesDuffingB}, though a similar analysis may be carried out for the fully implicit scheme as well.  

To start with, one looks for the points which satisfy concurrently $\dtm x^n = 0$ and $\dtt x^n = 0$. These are solved by $x^n = x^{n-1}$, and $\omega_0^2 x^n = -\gamma (x^n)^3$, $\forall n$. This gives the equilibrium points $(x^n,x^{n-1}) = \left\{(0,0),(\pm x_s,\pm x_s) \right\}$, where $x_s= \omega_0/|\gamma|^{1/2}$. Expanding the numerical scheme around $x_s$, for small amplitudes, gives $\dtt x^n \approx - (\omega_0^2 + 3 \gamma x_s^2) = 2\omega_0^2$, which is solvable in the $z$ domain using $x^n = z^n$, and yielding one exponentially growing and one exponentially decaying solution, indicating that the equilibria at $(\pm x_s, \pm x_s)$ are saddles. 

Just like the continuous case, the conserved numerical energy yields the trajectories which bound the region of the phase space in which oscillatory motion takes place. The energy is given in \eqref{eq:EnergySchemesDuffing}, where the form for $\phi$ is as per \eqref{eq:NonlinEnDuff2}. Since energy is conserved, the boundary trajectories are such that $\mathfrak{h}^{n-1/2}=\mathfrak{h}^{1/2}=\omega_0^4/4|\gamma|$, because this is the value of the energy when  $x^n = x^{n-1} = \pm x_s$. A form for $x^{n-1}$ when $x^{n-1} \neq x^n$ can then be derived by solving $\mathfrak{h}^{n-1/2} - \omega_0^4/4|\gamma| = 0$, with  $\mathfrak{h}^{n-1/2}$ as per \eqref{eq:EnergySchemesDuffing}. This is a quadratic equation in $x^{n-1}$, as a function of $x^n$. Solving the quadratic equation, one gets
\begin{equation}
x^{n-1}_\pm = \frac{\mp  \sqrt{\frac{|\gamma|}{2k^2}} (x^n)^2 + \left(\frac{1}{k^2}-\frac{\omega_0^2}{2}\right)x^n \pm \sqrt{\frac{1}{2k^2|\gamma|}}\omega_0^2}{\frac{1}{k^2}+\frac{\gamma}{2}(x^n)^2}
\end{equation}
Within the interval $I = \left\{ x^n :-x_s \leq x^n \leq x_s\right\}$, one has $x^{n-1}_+ \geq x^{n-1}_-$, where the equality holds at $x^{n}=\pm x_s$. Hence, oscillating solutions are obtained for $x^n \in I, x^{n-1}_- \leq x^{n-1} \leq x^{n-1}_+.$ Outside of this region, solutions grow unbounded. Fig. \ref{fig:NumPhaseSpDuff} reports a sketch of the numerical phase space, for the softening and the hardening cases, indicating the regions of oscillation.


\subsection{Newton-Raphson Solution of Implicit Schemes}

Scheme \eqref{eq:schemesDuffingC} is an example of a fully implicit scheme. The update must be computed via a suitable root finding method: Newton-Raphson is a most popular one. First, existence and uniqueness of update \eqref{eq:DuffImpl} should be proven: for a cubic, one real solution always exists, though three solutions may exist concurrently for a given parameter set, and for particular values of $x^{n-1}$, $x^n$. In fact, this is never the case here. A proof may be obtained via computation of the derivative of \eqref{eq:gdefDuffImpl} with respect to the update $x^{n+1}$. This results in
\begin{equation}
J_g \triangleq \frac{d g(x^{n+1})}{d x^{n+1}} = \frac{3\gamma k^2}{4}(x^{n+1})^2 + \frac{\gamma k^2 x^{n-1}}{2}x^{n+1} + 1 + \frac{\gamma k^2 (x^{n-1})^2}{4}.
\end{equation}
This is the equation of a parabola that is always positive, $\forall$ $x^{n+1}$, and hence the cubic described by \eqref{eq:DuffImpl} is always monotonically increasing. As a consequence, it will cross the $x$ axis only once, and the update is unique.

Solution via Newton-Raphson proceeds as follows. For an equation of the kind \eqref{eq:ImplSchemeDef}, at each time step, a first guess $x^\star_0$ is selected as an approximation to the actual update $x^{n+1}$. Then, $p$ iterations are performed, and each iteration is 
\begin{equation}
x^{\star}_{p+1} = x^{\star}_{p} - J_g^{-1}(x^{\star}_{p})\left(g(x^{\star}_{p}) - f(x^n,x^{n-1})\right),
\end{equation}
where $J_g$ is the derivative of $g$ (i.e. the Jacobian in the vector case). One halts the sequence when a suitable tolerance threshold is met, or when the maximum number of allowed iterations is reached, and one sets $x^{n+1}=x^\star_{p+1}$.



\subsection{Numerical Experiments}

An experiment, comparing the performances of schemes \eqref{eq:subnum}, is shown in Fig. \ref{fig:CubicBasic}. Here, nonlinear oscillators with increasing values of $\gamma > 0$ are shown, whilst the ratio $|\gamma|/\omega_0$ remains small. All the schemes show energy conservation to machine accuracy, including scheme \eqref{eq:schemesDuffingA} for which the numerical energy can only be obtained as a discrete sum from the beginning of time. It is remarked that scheme \eqref{eq:schemesDuffingA} becomes in fact unstable for high $\gamma$ values, and instability is here a manifestation of the unboundedness of the numerical energy. In the absence of a stability condition, explicit schemes may become unstable. The implicit schemes \eqref{eq:schemesDuffingB} and \eqref{eq:schemesDuffingC} do reamin stable, since the nonlinear potential energy is always non-negative. Fig. \ref{fig:CubicNR} shows the number of iterations for the Newton-Raphson routine used to solve the fully implicit scheme: one needs around 5 iterations for convergence. 
\begin{figure}
\includegraphics[width= 0.99\linewidth, clip, trim={3cm 4cm 3cm 0cm}]{Figures/CubicBasic.png}
\caption{Numerical simulations of hardening Duffing equation. For all simulations, the parameters are selected as: $\omega_0 = \sqrt{200}$, $x_0 = 8.7$, $v_0=0$. The sample rate is $f_s=100$ Hz. The hardening parameters $\gamma$ are given on top. Top panel corresponds to scheme \eqref{eq:schemesDuffingA}; middle to \eqref{eq:schemesDuffingB}; bottom to \eqref{eq:schemesDuffingC}. Scheme \eqref{eq:schemesDuffingA}, despite having a conserved energy, becomes unstable for $\gamma = 180.$}\label{fig:CubicBasic}
\end{figure}
Clearly, this results in a loss of efficiency, as visible in the right panel of Fig. \ref{fig:CubicNR}: for higher values of the nonlinearity, the compute times are five to six times longer for the fully-implicit scheme, than it is for the linearly-implicit scheme. 
\begin{figure}
\includegraphics[width= \linewidth,clip,trim={3cm 0.0cm 3cm 0.5cm}]{Figures/iterNRcubic.png}
\caption{Newton-Raphson iterations of fully implicit scheme \eqref{eq:schemesDuffingC} (left) and compute time ratio of linearly implict scheme \eqref{eq:schemesDuffingB} vs fully-implicit scheme (right). Parameters are the same as Fig. \ref{fig:CubicBasic}. For the left panel, circles (o) correspond to $\gamma =30$, crosses (+) to $\gamma = 100$, stars ($\star$) to $\gamma = 180$. Tolerance threshold for Newton-Raphson is set to $10^{-9}$.}\label{fig:CubicNR}
\end{figure}


\subsection{A Fourth-order Accurate Scheme}

Modified equation techniques can be employed in order to construct higher-order schemes. Just like the linear case, one needs to approximate $\dtt^p x^n$, for $p\geq 2$, as a function of difference operators of width 2, centered around $n$. It may be convenient to adopt \eqref{eq:DuffNoLoss} as a starting point. Hence, deriving twice with respect to time 
\begin{equation}\label{eq:DuffTemp4}
\frac{d^4 x}{dt^4} = - \frac{d^2}{dt^2}\left( \omega_0^2 x + \gamma x^3\right) = -(\omega_0^2+3\gamma x^2) \frac{d^2x}{dt^2} -6\gamma x \left(\frac{dx}{dt}\right)^2
\end{equation}
Considering the Taylor expansion of the second difference operator $\dtt$, as per \eqref{eq:dttExpasion}, and using \eqref{eq:DuffTemp4}, we have
\begin{equation}
\dtt x^n \approx \frac{d^2 x(t_n)}{dt^2} + \frac{k^2}{12}\frac{d^4 x (t_n)}{dt^4} =\left(1 - \frac{k^2}{12}(\omega_0^2+3\gamma x^2) \right) \frac{d^2 x(t_n)}{dt^2} - \frac{k^2}{2} \gamma x(t_n) \left(\frac{dx(t_n)}{dt}\right)^2.
\end{equation}
For the averaging operator, one has
\begin{equation}
\mtd = 1 + \frac{k^2}{12}\frac{d^2}{dt^2} + O(k^4),
\end{equation}
such that a fourth-order accurate approximation to \eqref{eq:DuffNoLoss} can be constructed as
\begin{equation}\label{eq:DuffLinearlyImpl4}
\left(1 + \frac{k^2}{12} \left(\omega_0^2-3\gamma x_0^2\right) \right) \dtt x^n = -\omega_0^2 x^n - \gamma (x^n)^2 \mtd x^n - \frac{k^2}{2}\gamma x^n (\dtm x^n)(\dtp x^n).
\end{equation}
This yields a linearly-implicit, fourth-order accurate approximation to \eqref{eq:DuffNoLoss}, which conserves the same energy as \eqref{eq:DuffLinearlyImpl} up to $O(k^2)$. Fig. \ref{fig:duff4Err} shows the expected error trends. 
\begin{figure}
\includegraphics[width=\linewidth, clip, trim ={1cm 0.0cm 1cm 0cm}]{Figures/Duffing4order.pdf}
\caption{Error of linearly implict schemes \eqref{eq:DuffLinearlyImpl} (dashed) and \eqref{eq:DuffLinearlyImpl4} (solid). Left: error at $t=0.4$ s as a function of sample rate. Right: error as a function of $t$ for $f_s=20$ kHz. The oscillator has $\omega_0 = \sqrt{200}$, $\gamma = 180$, and is started with $x_0= 3.7$, $v_0=0$. }\label{fig:duff4Err}
\end{figure}



\section{Forced, Lossy Duffing Equation}\label{sec:ForcedDuffing}


The Duffing equation \eqref{eq:DuffNoLoss} can be modified so to include a viscous loss term, and an external, harmonic forcing. This is 
\begin{equation}\label{eq:DuffForced}
\frac{d^2 x}{dt^2} + \omega_0^2 x = - 2c \frac{dx}{dt} - \gamma x^3 + F \cos(\omega t).
\end{equation}
Here, $F \in \mathbb{R}$ is a real amplitude. The time evolution of the forcing is here assumed to be a pure cosine, without loss of generality. 

%The factor of 2 multiplying the cosine is so to make it the amplitude consistent with the linear case, \eqref{eq:harmoForceLinear}, where the complex exponential form was used.

\subsection{Primary Resonance}
First, we are going to look at the behaviour of the system when the frequency of the forcing is slightly detuned from $\omega_0$. One writes
\begin{equation}\label{eq:omDefDuff}
\omega^2 = \omega_0^2 + \epsilon \sigma,
\end{equation}
where $\sigma = O(1)$ is a \emph{detuning} parameter, and where $\epsilon$ is a small number (not to be confused with the LTE $\varepsilon^n$ introduced in \eqref{eq:LTEdef}!) In practice, here we are interested in the primary resonance of the system, that is, resonances in the neighbourhood of the linear resonant peak at $\omega_0$. 



We are going to find an approximate solution for $x(t)$ using the method of multiple scales. For that, it is convenient to re-write the above equation as
\begin{equation}\label{eq:DuffForcedEps}
\frac{d^2 x}{dt^2} + \omega^2 x = \epsilon \left( \sigma x -2\underbrace{c/\epsilon}_{\mu} \frac{d x}{dt} - \underbrace{\gamma/\epsilon}_{\delta} x^3 + \underbrace{{F}/{\epsilon}}_{f}\cos(\omega t)\right).
\end{equation}
Then, multiple scales are introduced as in Sec. \ref{sec:MultScales}, i.e. $\tau_0 = t$, $\tau_1 = \epsilon t$, ...., and the solution $x(t)$ is itself written as a function of the multiple scales, as
\begin{equation}
x(\tau_0,\tau_1) = y_0(\tau_0, \tau_1) + \epsilon y_1(\tau_0, \tau_1).
\end{equation}
The forcing phase is  written as $\omega t = \omega \tau_0$.
Substituting these in \eqref{eq:DuffForcedEps}, and equating equal powers of $\epsilon$, gives
\begin{subequations}
    \begin{align}
          \frac{\partial^2 y_0}{\partial \tau_0^2} + \omega^2 y_0 &= 0, \label{eq:MultScalesForced1}\\
          \frac{\partial^2 y_1}{\partial\tau_0^2} + \omega^2 y_1 &= \sigma y_0 - 2\frac{\partial^2 y_0}{\partial \tau_0 \partial \tau_1} - 2\mu \frac{\partial y_0}{\partial \tau_0}-\delta y_0^3+  f \cos(\omega\tau_0). \label{eq:MultScalesForced2}
    \end{align}
\end{subequations}
The first equation is solved by
\begin{equation}
y_0 = A (\tau_1) e^{j\omega \tau_0} + \text{cc},
\end{equation}
where $\text{cc}$ is the complex conjugate. Substituting this into \eqref{eq:MultScalesForced2} gives
\begin{equation}
\frac{\partial^2 y_1}{\partial\tau_0^2} + \omega^2 y_1 = \left(\sigma A - 2j\omega\frac{dA}{d\tau_1} - 2j\mu\omega A - 3\delta |A|^2 A + \frac{f}{2}\right)e^{j\omega \tau_0} - \delta A^3e^{3j\omega\tau_0} + \text{cc},
\end{equation}
where $\text{cc}$ indicates  appropriate complex conjugate terms. The term within the round brakets on the right-hand side must be set to zero, so to avoid the presence of secular terms in the solution. Hence,
\begin{equation}\label{eq:DuffAmplEq}
-\sigma A + 2j\omega\frac{dA}{d\tau_1} + 2j\mu\omega A + 3\delta |A|^2 A = \frac{f}{2}  .
\end{equation}
This may be solved by considering $A(\tau_1) = \frac{X(\tau_1)}{2} e^{j\alpha(\tau_1)}$, with $X,\alpha \in \mathbb{R}$. 
% Substituting in the above, one gets
% \begin{equation}
% j\omega_0\left(\frac{d \alpha}{d\tau_1} + j\alpha \left(\sigma-\frac{d \zeta}{d\tau_1}\right)\right) + j\mu \omega_0 \alpha + \frac{3}{8}\delta \alpha^3 = f e^{j\zeta},
% \end{equation}
% where $\zeta = \sigma \tau_1 - \beta$. Separating the real and imaginary parts gives
% \begin{subequations}\label{eq:TimeEvoAmplitudeDuff}
% \begin{align}
% \frac{d \alpha}{d\tau_1} &= -\mu \alpha + \frac{f}{\omega_0}\sin(\zeta), \\
% \alpha \frac{d \zeta}{d\tau_1} &= -\frac{3 \delta \alpha^3}{8\omega_0} + \alpha \sigma + \frac{f}{\omega_0} \cos(\zeta).
% \end{align}
% \end{subequations}
The approximate solution for $x(t)$ is then
\begin{equation}
x(t) = X(\epsilon t) \cos(\omega t + \alpha(\epsilon t)) + O(\epsilon).
\end{equation} 


\subsection{Steady-state solutions}

Here, we are interested in the response of the system in the steady state, i.e. when the contribution coming from the initial conditions has completely died out. Mathematically, this is expressed by requiring that the time derivative of $A$ with respect to $\tau_1$ vanish, i.e. $X$ and $\alpha$ are constants. In that case, one has that the steady state amplitude in \eqref{eq:DuffAmplEq} satisfies
\begin{equation}\label{eq:steadyStForcedDuff}
X \left(-\sigma + 2j\mu\omega + \frac{3}{4}\delta |X|^2 \right) = f e^{-j\alpha}.
\end{equation}
Taking absolute values on both sides gives:
\begin{equation}
\left|\frac{f}{X}\right|^2 = 4\omega^2 \mu^2 + \left(-\sigma + \frac{3}{4}\delta|X|^2\right)^2, \quad \rightarrow \quad \left|\frac{F}{X}\right|^2 = 4\omega^2 c^2 + \left(-(\omega^2-\omega_0^2) + \frac{3}{4}\gamma|X|^2\right)^2.
\end{equation}
First, we remark that this is consistent with \eqref{eq:TransXF} obtained when $\gamma$ is small (linear vibrations). When $\gamma$ is not infinitesimally small, the function $\omega(|X|)$ is obtained from the above using $4\omega^2 c^2 = 4\omega_0^2 c^2 + O(\epsilon)$, thus
\begin{equation}\label{eq:OmegaX}
\omega^2 = \omega_0^2 + \frac{3}{4}\gamma |X|^2 \pm \sqrt{\left|\frac{F}{X}\right|^2 - 4\omega_0^2 c^2} + O(\epsilon).
\end{equation}
The equation above suggests that the locus of the maxima of the $|X|(\omega)$ curve is given by $\frac{3}{4}\gamma|X|^2+\omega_0^2-\omega^2 = 0$. This curve is called the \emph{backbone curve}. The phase $\alpha$ can be estimated from \eqref{eq:steadyStForcedDuff}, by taking the ratio of the imaginary and real parts of the left-hand side. This gives
\begin{equation}
\tan \alpha = -\frac{2c\omega}{\frac{3}{4}\gamma|X|^2-\omega^2+\omega_0^2},
\end{equation}
and where $\omega(|X|)$ is given by \eqref{eq:OmegaX}. In Fig. \ref{fig:DuffBckb1}, various values of $\gamma$ and $F$ are used to plot the frequency-amplitude curves. It is seen that the amplitude curves present up to three concurrent solutions for the same value of the forcing frequency. In this case, the initial conditions dictate on which branch the solution takes place. Of the three concurrent solution, one is unstable, such that a small perturbation of the initial conditions will cause the solution to jump on either the upper or lower branch. Note that hysteresis takes place: when slowly varying the input frequency, the solution will be found on either the upper or lower branch, according to the initial conditions. 
Note that, in Fig. \ref{fig:DuffBckb1}, the phase response is also multi-valued. In Fig. \ref{fig:DuffBckb2} the same plots are presented, but in this case keeping $\gamma$ and $F$ fixed, and varying the loss coefficient. Finally, in Fig. \ref{fig:DuffFD}, a finite difference scheme is used to test the prediction from the perturbation theory. Note that the jumps and the hysteretic cycles are correctly represented.
\begin{figure}
\includegraphics[width=\linewidth]{Figures/DuffingBackBone.png}
\caption{Amplitude and phase responses of the forced, lossy Duffing oscillator. Here, $\omega_0=100$, $c=0.3$. Four values of $\gamma$ are selected as [-500,-250,250,500]. For each value of $\gamma$, the forcing amplitudes are selected as $F = [10,20,30,40,50]$. Left: amplitude response; dashed lines correspond to the backbone curves. Right: phase response. Note that multivalued solutions are present in both responses: the initial conditions in this nonlinear case dictate which solution is selected.}\label{fig:DuffBckb1}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{Figures/DuffingBackBoneLoss.png}
\caption{Amplitude and phase responses of the forced, lossy Duffing oscillator. Here, $\omega_0=100$, $\gamma=-500$, $F=50$. Three values of $c$ are selected as [0.3,0.4,0.5]. Left: amplitude response; dashed lines correspond to the backbone curves. Right: phase response.}\label{fig:DuffBckb2}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{Figures/XFduffing.png}
\caption{Nonlinear transfer functions of the Duffing oscillator.  
Here, $\omega_0=100$, $\gamma=-500$, $c=0.3$ and $F = [10,20,30,40,50]$. The transfer functions here have a shape depending on the forcing amplitude, unlike the linear case \eqref{eq:TransXF} pictured in Fig. \ref{fig:LinearTransFunctPlots}, and indicated as a dashed line in both panels.}\label{fig:XFduffing}
\end{figure}
\begin{figure}
\includegraphics[width=\linewidth]{Figures/HisteresisMatlab.png}
\caption{Amplitude response of the forced, lossy Duffing oscillator. Comparison between analytic solution \eqref{eq:OmegaX} and numerical solution, using a finite difference scheme.  
Here, $\omega_0=100$, $\gamma=-250$, $F=50$, $c=0.3$. The numerical scheme is run at a sample rate of 10kHz. The maximum amplitude of the forced response is recorded, and each simulation runs for $T=50$ s, in order to attain the steady state. The forcing frequency is first increased upward, from $0.8\omega_0$, with a step of 0.02 rad/s; then it is decreased with the same step, starting from $1.1\omega_0$. Note that the scheme displays the jumps, and the hysteresis, correctly predicted by the perturbation theory.}\label{fig:DuffFD}
\end{figure}





\section{Damped Oscillators}


In this section, single scalar equations of the form 
\begin{equation}\label{eq:LossyOsci}
\frac{d^2 x}{dt^2}+\omega_0^2 x = -\epsilon f\left(\frac{dx}{dt}\right)
\end{equation}
will be investigated. Here, $f$ is (generally) a nonlinear function in $dx/dt$, and $\epsilon > 0$ is a small parameter. 


\subsection{Positive damping}

Functions of such that $f\frac{dx}{dt}\geq 0$ will be investigated first. In this case, multiplying \eqref{eq:LossyOsci} by $dx/dt$ gives
\begin{equation}
\frac{d}{dt}\left(\frac{1}{2}\left(\frac{dx}{dt}\right)^2 +  \frac{\omega_0^2 x^2}{2} \right) = - \epsilon f\frac{dx}{dt} \leq 0,  
\end{equation}
showing that the energy (here, scaled by mass) is non-increasing over time. Hence, bound \eqref{eq:EgyBoundVel} holds here too. Cases of this kind are said to be \emph{positively damped}, in that the damping always acts to remove energy from the system.
The simple case of linear damping was treated extensively in Sec. \ref{sec:LossSHO}. Here, we are going to focus on nonlinear cases. 


In order to derive suitable approximate solutions, the method of multiple scales will be used. For that, the multiple scales $\tau_0=t$, $\tau_1 = \epsilon t$ are defined. Then, $x$ is assumed to be a function of the multiple scales, as
\begin{equation}
x(\tau_0,\tau_1) = y_0(\tau_0,\tau_1) + \epsilon y_1(\tau_0,\tau_1) + O(\epsilon^2).
\end{equation}
In order to apply the method of multiple scales, as seen in the previous sections, secular terms must be removed from the solution, so to gaurantee uniform convergence of the solution as time increases. It is convenient, then, to expand out the nonlinear function $f$ is a Fourier series, with period equal to the period of the linear part. Writing the Fourier series using the complex exponential form, as per \eqref{eq:FourierSeriesComplex}, one has
\begin{equation}\label{eq:fExpansion}
f = \sum_{m=-M}^M c_m e^{jm\omega_0 \tau_0}. 
%\quad \text{ with } f_m = \frac{\omega_0}{2\pi}\int_0^{2\pi / \omega_0} f e^{-jn\omega_0 \tau_0} \,\mathrm{d}\tau_0.
\end{equation}
Here, the coefficients $c_m$ are the same as in \eqref{eq:FourierSeriesComplex}.  In writing this form for $f$, it was assumed that $f$ is in fact periodic, with period equal to the period of the undamped oscillator. This is of course not true in general, though it holds in some limit of powers of $\epsilon$, a small parameter. Using expansion \eqref{eq:fExpansion} into \eqref{eq:LossyOsci} leads to  
\begin{subequations}
    \begin{align}
          \frac{\partial^2 y_0}{\partial \tau_0^2} + \omega_0^2 y_0 &= 0, \label{eq:MultScalesLossy1}\\
          \frac{\partial^2 y_1}{\partial\tau_0^2} + \omega_0^2 y_1 &=  - 2\frac{\partial^2 y_0}{\partial \tau_0 \partial \tau_1} - \sum_{m=-M}^M c_m e^{jm\omega_0 \tau_0}. \label{eq:MultScalesLossy2}
    \end{align}
\end{subequations}
The first equation is solved by
\begin{equation}\label{eq:tempLossya}
y_0(\tau_0,\tau_1) = A(\tau_1) e^{j\omega_0 \tau_0} + \text{cc},
\end{equation} 
where $\text{cc}$ denotes the complex conjugate. Because of the presence of the term $\omega_0^2 y_1$ on the left-hand side of \eqref{eq:MultScalesLossy2}, in the right-hand side all non-zero terms proportional to $e^{\pm j\omega_0\tau_0}$ will give rise to secular terms in the solution. Since this results in non-uniform convergence, such terms are set to zero, ultimately setting the form of $A$. Substituting  \eqref{eq:tempLossya} into \eqref{eq:MultScalesLossy2} results in
\begin{equation}
\frac{\partial^2 y_1}{\partial\tau_0^2} + \omega_0^2 y_1 = -\left(2j \omega_0\frac{dA(\tau_1)}{d\tau_1} + \frac{\omega_0}{2\pi}\int_0^{2\pi/\omega_0}f \, e^{-j\omega_0 \tau_0} \, \mathrm{d}\tau_0 \right) e^{j\omega_0 \tau_0} - \sum_{m=-M,m\neq \pm 1}^M c_m e^{jm\omega_0 \tau_0} + \text{cc}.
\end{equation}
Hence, an equation for $A$ results by setting to zero the quantity in brackets. One has{}
\begin{equation}\label{eq:LossyTempb}
\frac{dA}{d\tau_1} = \frac{j}{4\pi} \int_0^{2\pi / \omega_0} f\, e^{-j\omega_0 \tau_0}\, \mathrm{d}\tau_0.
\end{equation}
It is convenient to write $A(\tau_1) = \frac{X(\tau_1)}{2}e^{j\alpha(\tau_1)}$
% \begin{equation}
% A(\tau_1) = \frac{X(\tau_1)}{2}e^{j\alpha(\tau_1)}, 
% \end{equation}
so that
\begin{equation}
x(t) = X(\epsilon t) \cos(\omega_0 t + \alpha(\epsilon t)) + O(\epsilon).
\end{equation}
Using the complex exponential form for $A$, one gets
\begin{equation}
\frac{1}{2}\frac{dX}{d\tau_1} + j\frac{X}{2}\frac{d\alpha}{d\tau_1} = \frac{j}{4\pi} \int_0^{2\pi / \omega_0} f\, e^{-j  \left( \omega_0 \tau_0 + \alpha\right) }  \, \mathrm{d}\tau_0 = \frac{j}{4\pi\omega_0} \int_\alpha^{2\pi+\alpha} f\, e^{-j  \phi  }  \, \mathrm{d}\phi = \frac{j}{4\pi\omega_0} \int_{-\pi}^{\pi} f\, e^{-j  \phi  }  \, \mathrm{d}\phi, 
\end{equation}
where the change of integral limits in the last idendity is justified, since both $f$ and $e^{-j\phi}$ are periodic in $\phi$ with period 2$\pi$. It is noted the last equation really gives two equations, one for the real part, and one for the imaginary part. Separating out the two equations allows to get expressions for $X$ and $\alpha$. These are
\begin{subequations}\label{eq:LossyAmpPhase}
\begin{align}
\frac{dX}{d\tau_1} &= \frac{1}{2\pi\omega_0}\int_{-\pi}^{\pi} f\, \sin\phi \, \mathrm{d}\phi \\
\frac{d\alpha}{d\tau_1} &= \frac{1}{2\pi\omega_0X}\int_{-\pi}^{\pi} f\, \cos\phi \, \mathrm{d}\phi 
\end{align}
\end{subequations}


\subsubsection{Quadratic Damping}

Consider the following dissipating function
\begin{equation}\label{eq:QuadLoss1}
f\left( \frac{dx}{dt}\right) = \left| \frac{dx}{dt}\right| \frac{dx}{dt}.
\end{equation}
Since $f$ in \eqref{eq:LossyOsci} is multiplied by the small parameter $\epsilon$, we are here only interested in variations of the order of $\epsilon^0$, i.e. $\tau_0$. Hence, we neglect the slow scale $\tau_1$ and write
\begin{equation}\label{eq:QuadLoss2}
f\left(\frac{dx}{dt}\right) = f\left(\frac{dx}{d\tau_0}\right) + O(\epsilon) \approx f\left(-X\omega_0\sin\phi\right).
\end{equation}
Hence, substituting \eqref{eq:QuadLoss2} into \eqref{eq:LossyAmpPhase} gives
\begin{subequations}\label{eq:LossyAmpPhaseQuadratic}
\begin{align}
\frac{dX}{d\tau_1} &= -\frac{X|X|\omega_0}{2\pi}\int_{-\pi}^{\pi} |\sin\phi|\, \sin^2\phi \, \mathrm{d}\phi \label{eq:LossyAmpPhaseQuadratic1}\\
\frac{d\alpha}{d\tau_1} &= -\frac{|X|\omega_0}{4\pi}\int_{-\pi}^{\pi} |\sin\phi| \sin 2\phi  \, \mathrm{d}\phi  \label{eq:LossyAmpPhaseQuadratic2}
\end{align}
\end{subequations}
To solve \eqref{eq:LossyAmpPhaseQuadratic1}, one splits the integral as
\begin{equation}
\int_{-\pi}^{\pi} |\sin\phi|\, \sin^2\phi \, \mathrm{d}\phi  = -\int_{-\pi}^{0} \sin^3\phi \, \mathrm{d}\phi + \int_{0}^{\pi} \sin^3\phi \, \mathrm{d}\phi,
\end{equation}
and then antiderivatives are computed easily using the identity $\sin^3\phi = 3/4 \sin3\phi - 1/4\sin\phi$. The integrand in \eqref{eq:LossyAmpPhaseQuadratic2} is  an odd function, so the result of the integration is zero. Hence, one gets
\begin{equation}
\frac{dX}{d\tau_1} = -\frac{4X^2\omega_0}{3\pi}, \quad \alpha = \alpha_0,
\end{equation}
where it was assumed (without loss of generality), that $X$ is positive. Integrating, one gets
\begin{equation}
X = \frac{3\pi X_0}{3\pi + 4\epsilon\omega_0X_0t},
\end{equation}
where $\tau_1=\epsilon t$ was used, and where $X_0=X(t=0).$ Thus, for the quadratic damping case one has
\begin{equation}\label{eq:xtQuadraticLoss}
x(t) = \frac{3\pi X_0}{3\pi + 4\epsilon\omega_0X_0t} \cos(\omega_0 t + \alpha_0) + O(\epsilon),
\end{equation}
where $X_0$, $\alpha_0$ are determined from the initial conditions. Note that the envelope decays faster for larger values of $X_0$, as a consequence of the nonlinearity, and for smaller values of $\omega_0$.


Turning now to finite difference schemes, various options are available. Here, the following two options are given
\begin{subnumcases}{\dtt x^n + \omega_0^2 x^n = -\epsilon \dtd x^n}
|\dtd x^n|,\label{eq:FullyImplQuadraticLoss}\\
\left|\dtm x^{n}\right|.\label{eq:LinImplQuadraticLoss}
\end{subnumcases}
By Taylor-expansion arguments, it can be seen that the LTE of the first scheme is $O(k^2)$, whilst for the second scheme it is $O(k)$. The first scheme is also fully implicit, since the update appears implicitly as the argument of the absolute value. The second scheme is  linearly implicit, since the update can be computed by simple division. Turning to the energy properties, multiplying both schemes by $\dtd x^n$ and using the usual identities results in
\begin{equation}
\dtp \left(\frac{(\dtm x^n)^2}{2} + \frac{\omega_0^2 x^n x^{n-1}}{2}\right) = - \epsilon |v|(\dtd x^n)^2 \leq 0.
\end{equation}
Hence, the total discrete energy of the system is at least non-increasing, and a stability condition may be obtained from the analysis of the associated conservative system, and in this case it is \eqref{eq:StabCondSHO}. Note that, in the above, the scheme is dissipative regardless of the particular form of $v$, i.e. of whether the fully-implicit ($v= \dtd x^n$) or linearly-implicit ($v=\dtm x^n$) realisation is used. 


If one wishes to use the fully-implicit scheme, existence and unqueness of the update must be proven. To start with, the scheme is re-written as
\begin{equation}
g(y) = \frac{\epsilon}{4}|y|y + y - a = 0,
\end{equation}
where $y=x^{n+1}-x^{n-1}$, and $a = k^2(2/k \, \dtm - \omega_0^2) x^n$. One has $\lim_{y\rightarrow -\infty}g(y) = -\infty$, and $dg/dy = 1 + \epsilon/2|y|>0$ (monotonicity), and thus the scheme always has one unique solution, since $g$ is also continuous. Newton-Raphson can be employed here. Fig. \ref{fig:QuadraticLossFD} presents a case study, where it is seen that the approximate analytical solution deviates quite clearly from the numerical solutions, as the parameter $\epsilon$ gets larger.
\begin{figure}
\includegraphics[width=\linewidth]{Figures/QuadraticLoss.png}
\caption{Linear oscillator with quadratic damping. For all figures, the solid grey line is the analytic approximation \eqref{eq:xtQuadraticLoss}, solid black line is the second-order accurate scheme (fully-implicit) \eqref{eq:FullyImplQuadraticLoss}, whereas the dashed black line is the linearly implicit scheme  \eqref{eq:LinImplQuadraticLoss}. For all simulations, the sample rate is $f_s=44100$, and the initial velocity is $v_0=-0.8$ and $\omega_0 = 100$. The initial displacement $x_0$ and the value of the small parameter $\epsilon$ are as given in each panel.}\label{fig:QuadraticLossFD}
\end{figure}


\subsubsection{Coulomb damping}

For this case
\begin{equation}
f = \text{sign}\left( \frac{dx}{dt} \right)c,
\end{equation}
where $c>0$ is a constant. Using $dx/dt = -\omega_0X\sin\phi$, and again assuming $X>0$, integrals \eqref{eq:LossyAmpPhase} give
\begin{equation}
\frac{dX}{d\tau_1} = -\frac{2c}{\pi \omega_0}, \quad \frac{d \alpha}{d\tau_1} = 0,
\end{equation}
which are solved by $X=X_0-\frac{2c\epsilon  t}{\pi \omega_0}$, $\alpha = \alpha_0$. Note that in this case the rate of change of the envelope is independent of the initial  conditions, but depends on $\omega_0$: as for the previous case, the decay is faster for smaller values of $\omega_0$.


A finite difference scheme may be obtained here as
\begin{equation}
\dtt x^n + \omega_0^2 x^n = -\epsilon c \, \text{sign}(\dtd x^n), 
\end{equation}
and expressing $y = \dtd x^n$, $a = \dtm x^n - \frac{\omega_0^2 k x^n}{2}$, the scheme can be given as
\begin{equation}
g(y) = y - a + \frac{\epsilon k c}{2}\text{sign}(y)=0.
\end{equation}
Note that here $g$ has a discontinuity for $y=0$, hence the usual argument of monotonicity of $g$ cannot be used to prove the existence of a solution. Solutions to this scheme may be found considering the sign of $y$. Hence, for a solution to exist, either one of these cases must be verified:
\begin{equation}\label{eq:TempColoumb1}
y = a - \frac{\epsilon k c}{2},\, y>0 \quad \text{ or } \quad y = a + \frac{\epsilon k c}{2},\, y<0.
\end{equation}
If neither case is verified, a solution does not exist! One may be tempted to find a suitable condition on the time step $k$ such that a solution exists at all time steps. Glancing at \ref{eq:TempColoumb1}, one has that such condition is
\begin{equation}
|a|>\frac{\epsilon k c}{2},
\end{equation}
but since $a$ is state-dependent, it may not be possible to find a condition for uniqueness depending only on the systems's parameters. 


Consider then the following scheme:
\begin{equation}\label{eq:FDColoumbDamp}
\dtt x^n + \omega_0^2 x^n = - \epsilon c \frac{|\dtp x^n|-|\dtm x^n|}{k\dtt x^n}.
\end{equation}
Using Taylor expansion arguments, one can check that the LTE is here $O(k^2)$. Defining $y=k\dtt x^n$, $b = k\omega_0^2x^n$, $a = \dtm x^n$, the scheme can be written as
\begin{equation}
g(y) = y + b + \epsilon c k \frac{|y+a|-|a|}{y} = 0.
\end{equation}
Here, one has $\lim_{y\rightarrow -\infty}g(y) = -\infty$; $dg/dy = 1 + \epsilon c k \frac{\text{sign}(y+a)y - |y+a|+|a|}{y^2} > 0$, and $g$ is continuous. Thus, a solutions always exists, and is unique. Regarding the stability property of the scheme, it is remarked that the right-hand side of \eqref{eq:FDColoumbDamp} is $-\epsilon c |\dtd x^n|+O(k^2)$, and thus multiplication of \eqref{eq:FDColoumbDamp} by $\dtd x^n$ gives
\begin{equation}
\dtp \left(\frac{(\dtm x^n)^2}{2} + \frac{\omega_0^2 x^n x^{n-1}}{2} \right) = - \epsilon c |\dtd x^n| \dtd x^n +O(k^2), 
\end{equation}
and thus energy is dissipated up to (at least) a term of the order of $k^2$. Fig. \ref{fig:CoulombLossFD} illustrates an example, comparing scheme \eqref{eq:FDColoumbDamp} with the approximate analytic solution. \begin{figure}
\includegraphics[width=\linewidth]{Figures/CoulombDamp.png}
\caption{Linear oscillator with Coulomb damping. For all figures, the solid grey line is the analytic approximation, solid black line is the finite difference  scheme. For all simulations, the sample rate is $f_s=44100$, and the initial velocity is $X_0=4$, $v_0=-0.8$, $\omega_0 = 10$, $\epsilon = 0.5$. The value of $c$ is given in each panel.}\label{fig:CoulombLossFD}
\end{figure}

\subsection{Negative damping}

Some physical systems present damping profiles that are not strictly dissipative, that is, the product $f\frac{dx}{dt}$ is not strictly positive. One such case was proposed by Rayleigh, for which one has
\begin{equation}
f = \frac{dx}{dt}\left( \left( \frac{dx}{dt} \right)^2 - 1\right), 
\end{equation}
and here  strictly dissipative behaviour is observed only when $|dx/dt|>1$. When $|dx/dt|\leq 1$, the energy in the system increases. Such behaviour must yield some kind of steady state, in which the oscillator presents the same kind of motion regardless of the initial conditions. The method of multiple scales may be applied here, and \eqref{eq:LossyAmpPhase} are valid here too, since they are independent of the sign of $f\frac{dx}{dt}$. Hence, one has
\begin{equation}
\frac{dX}{d\tau_1} = \frac{1}{2\pi \omega_0}\int_{-\pi}^\pi \left(-\omega_0^3 X^3 \sin^4 \phi + \omega_0 X \sin^2 \phi \right) \,\mathrm{d}\phi.
\end{equation}
The integral can be solved via the substitution of the identities $\sin^2 \phi = (1-\cos 2\phi)/2$, $\sin^4 \phi = 3/8 + (\cos 4\phi) /8 - (\cos 2\phi) /2$, giving
\begin{equation}
\frac{dX}{d\tau_1} = \frac{1}{2\pi\omega_0}\left(- \frac{3 \omega_0^3 X^3}{8} +  \frac{\omega_0 X}{2} \right)\Bigg|_{-\pi}^{\pi} = \frac{X}{2}\left(1 - \frac{3}{4}\omega_0^2 X^2 \right).
\end{equation}
The amplitude equation is obtained via separation of variables, and integrating:
\begin{equation}
\int_{X_0}^X \frac{\mathrm{d}X}{X\left(1- \frac{3}{4}\omega_0^2 X^2 \right)} = \frac{1}{2}\log \frac{2X^2}{9X^2\omega_0^2-12} \Big|_{X_0}^X= \int_{0}^{\tau_1}\frac{\mathrm{d}\tau_1}{2}=\frac{\tau_1}{2}.
\end{equation}
Taking exponentials on both sides, and rearranging, yields
\begin{equation}
X = \frac{X_0}{\sqrt{\frac{3}{4}X_0^2\omega_0^2 + \left(1 -  \frac{3}{4}X_0^2\omega_0^2\right)e^{-\epsilon t}}},
\end{equation}
showing that the amplitude as $t\rightarrow \infty$ is $X \approx X_s = \frac{2}{\sqrt{3}\omega_0}$. This means that, if the initial amplitude is $X_0<X_s$, the amplitude will \emph{increase} over time until it reaches the saturation amplitude $X_s$. On the other hand, if the intial amplitude is $X_0>X_s$, the amplitude will decrease over time, as per the examples of the previous subsection. For the phase angle, one has 
\begin{equation}
\frac{d\alpha}{d\tau_1} = \frac{1}{2\pi \omega_0} \int_{-\pi}^\pi \left(-\omega_0^3 X^2 \sin^3 \phi + \omega_0 \sin \phi \right)\cos\phi \,\mathrm{d}\phi,
\end{equation}
and since the integrand is an odd function in $\phi$, the result is $\frac{d\alpha}{d\tau_1}=0$, or $\alpha=\alpha_0$.

As a working finite difference scheme, consider 
\begin{equation}
\dtt x^n + \omega_0^2 x^n = -\epsilon \, \dtd x^n \left((\dtd x^n)^2 - 1 \right),
\end{equation}
 which may be written as 
 \begin{equation}
 g(y) = y + a + \frac{\epsilon k}{2}\left( y^3 - y\right) = 0,
 \end{equation}
where $y = \dtd x^n$, $a = \frac{k\omega_0^2 x^n}{2}-\dtm x^n$. It is noted that $\lim_{y \rightarrow -\infty} g(y) = -\infty$, that $g$ is continuous, and that $\frac{dg}{dy} = \frac{3\epsilon k y^2}{2} + 1 - \frac{\epsilon k}{2}$. Hence, for the derivative to be positive, thus ensuring the existence and uniqueness of the solution, one must check that
\begin{equation}
k < \frac{2}{\epsilon}.
\end{equation}
This condition is to be enforced along with the ususal stability condition for the lossless oscillator, i.e. \eqref{eq:StabCondSHO}. 

Figure \ref{fig:Rayleigh} compares the analytic approximation with the finite difference scheme, again showing reasonable agreement. The phase portraits highlight the existence of a limit cycle, which occurs regardless of the initial amplitude. 

\begin{figure}
\includegraphics[width=\linewidth]{Figures/RayleighOscillator.png}
\includegraphics[width=\linewidth]{Figures/RayleighOscillatorLimitCycles.png}
\caption{Rayleigh oscillator. Top: time evolution (grey: analytic approximation; black: finite difference scheme). Bottom: phase portraits (from finite difference scheme). For all simulations, the sample rate is $f_s=44100$, the initial velocity is $v_0=0$, $\omega_0 = 5$ and $\epsilon = 0.9$. The initial displacement is $x_0 = 2 X_s$ (left panels) and $x_0 = 0.25 X_s$ (right panels).}\label{fig:Rayleigh}
\end{figure}









\chapter{Coupled Oscillators}

In this chapter we are going to study the problem of a system of coupled oscillators, in both linear and nonlinear regimes. In the linear case, extensions of the frequency-domain techniques already in use for the oscillator in isolation are possible, leading to the idea of \emph{modes of vibration}. For the nonlinear case, analytic approximations based on perturbation methods are possible, though the algebra quickly becomes unwieldy. However, time-domain simulation techniques for this multimodal case (and, particularly, energy methods) are in fact not too dissimilar from those already encountered previously, and we shall make use of them accordingly. 


\section{Two masses, three springs}
\begin{figure}
\includegraphics[width=\linewidth]{Figures/TwoMassesGeneric.pdf}
\caption{Two-mass system. For small vertical displacements, the two masses move vertically. The displacements from the rest positions are denoted $x_1$, $x_2$.}\label{fig:TwoMassesGeneric}
\end{figure}

Consider the system sketched in Fig. \ref{fig:TwoMassesGeneric}. Here, the system possesses two degrees of freedom, which may conveniently be identified with the displacements $x_1(t)$, $x_2(t)$ of the two masses $m_1$, $m_2$, from their rest position. The equations of motion for this system are as
\begin{subequations}\label{eq:TwoMass1}
\begin{align}
m_1 \frac{d^2 x_1}{dt^2} = -K_{11}x_1 - K_{12}(x_1 - x_2), \label{eq:Coupl1} \\
m_2 \frac{d^2 x_2}{dt^2} = -K_{22}x_2 + K_{12}(x_1 - x_2). \label{eq:Coupl2}
\end{align}
\end{subequations}
For the moment, it was assumed that motion is completely linear (the amplitude of vibration is small, so that nonlinear effects can be neglected). 



The frequency-domain techniques described in Sec. \ref{sec:FreqDomAn} can be extended here. To that end, it is convenient to adopt a matrix-vector formulation of the system. Though in this example the system comprises $2$ masses, this formulation proves advantageous since it can be used to describe systems comprising $N$ degrees of freedom. Furthermore, the matrix-vector formulation is the natural framework to write the discrete space-time schemes that we will encounter in later chapters. Hence, \eqref{eq:TwoMass1} is rewritten compactely as
\begin{equation}\label{eq:MultiModalMatr}
{\bf M} \, \frac{d^2{\bf x}}{dt^2} = -{\bf K} \, {\bf x}, 
\end{equation}
where clearly
\begin{equation}
{\bf M} = \begin{bmatrix}m_1 & 0 \\ 0 & m_2 \end{bmatrix}, \,\, {\bf K} = \begin{bmatrix}K_{11}+K_{12} & -K_{12} \\ -K_{12} & K_{22}+K_{12} \end{bmatrix}, \,\, {\bf x} = \begin{bmatrix}x_1  \\  x_2 \end{bmatrix}.
\end{equation}
Extension of the frequency-domain techniques of Sec. \ref{sec:FreqDomAn}, one can conveniently define a test solution of the form
\begin{equation}
{\bf x} =  {\bf X} e^{j\omega t}.
\end{equation}
Here, $ {\bf X}$ is a constant complex amplitude, $j\omega$ was substituted for $s$, since for lossless systems one has $\sigma = 0$ in $s = j\omega + \sigma$. Substituting the test solution into \eqref{eq:MultiModalMatr}, one gets
\begin{equation}\label{eq:OmegaEigen}
\left( {\bf K } - \omega^2 {\bf M} \right)  {\bf X} = 0,
\end{equation}
which shows that the frequency $\omega$ is defined as the eigenvalue of the  eigenvalue problem \eqref{eq:OmegaEigen}. Nontrivial solutions are obtained when the determinant of ${\bf K } - \omega^2 {\bf M}$ is zero. This defines the polynomial whose roots yield the eigenvalues. For this two-dimensional case, one has
\begin{equation}
 (K_{11}+K_{12} - \omega^2 m_1)(K_{22}+K_{12} - \omega^2 m_2) - K_{12}^2 = 0.
\end{equation}
Solving for $\omega^2$, one gets
\begin{equation}
\omega_\pm^2 = \frac{m_1(K_{22}+K_{12})+m_2(K_{11}+K_{12}) \pm \sqrt{\left(m_1(K_{22}+K_{12}) - m_2(K_{11}+K_{12})\right)^2 + 4 m_1 m_2 K_{12}^2 }}{2m_1 m_2}.
\end{equation}
This solution shows that both the eigenvalues $\omega^2_\pm \in \mathbb{R}$ (since the expression under the square root sign is always positive); however, there is no guarantee that $\omega^2_\pm$ are also \emph{positive}. A negative $\omega^2$ would result in exponentially growing behaviour of the test solution. Checking positivity of the roots can be quite laborious in the general case. To make things a little easier, we make the assumption $m_1=m_2=m$, for which one has
\begin{equation}
2m\omega_\pm^2 = K_{11}+K_{22}+2K_{12} \pm \sqrt{\left( K_{11}-K_{22}  \right)^2 + 4  K_{12}^2}.
\end{equation} 
The positivity of \emph{both} solutions is enforced when 
\begin{equation}\label{eq:TempIneq}
K_{11}+K_{22}+2K_{12} \geq 0, \,\, \text{ and } \,\, \left(K_{11}+K_{22}+2K_{12}\right)^2 \geq \left( K_{11}-K_{22}  \right)^2 + 4  K_{12}^2.
\end{equation}
Solving for $K_{12}$, one has
\begin{subnumcases}{K_{12} \geq -\frac{K_{11}+K_{22}}{2} \,\, \text{ and } \,\, \label{eq:IneqK12}}
 K_{12} \geq -\frac{K_{11}K_{22}}{K_{11}+K_{22}} \,\, \text{ if } \,\, K_{11}+K_{22} > 0, \label{eq:IneqK12a}\\
 K_{12} \leq -\frac{K_{11}K_{22}}{K_{11}+K_{22}} \,\, \text{ if } \,\, K_{11}+K_{22} < 0. \label{eq:IneqK12b}
\end{subnumcases}
If instead $K_{11}+K_{22}=0$, at least one eigenvalue is surely negative, as seen immediately from \eqref{eq:TempIneq}. As an example, consider the case $K_{11}=K_{22}=1$. The conditions above give $K_{12}\geq -1/2$: this is an interesting case, since one may allow the coupling to have negative stiffness, whilst guaranteeing oscillating solutions overall. As a second example, consider $K_{11}=K_{22}=-1$: here, there is no range allowable for $K_{12}$. 










Once the eigenvalues are computed, one may compute the eigenvector $ {\bf X}$ from \eqref{eq:OmegaEigen}. Assuming for instance $ { X}_1 = a_{\pm}$ (where $a_\pm$ are just useful normalisation constants), one has
\begin{equation}
 X_2 = a_{\pm}\frac{K_{11} + K_{12} - \omega^2_\pm m_1}{K_{12}},
\end{equation}
such that the general solution is given by
\begin{equation}
{\bf x} = a_+\begin{bmatrix} 1 \\ \frac{K_{11} + K_{12} - \omega^2_+ m_1}{K_{12}}\end{bmatrix}\left(A_+ e^{j\omega_+ t} + A_- e^{- j\omega_+ t} \right) + a_-\begin{bmatrix} 1 \\ \frac{K_{11} + K_{12} - \omega^2_- m_1}{K_{12}}\end{bmatrix}\left(B_+ e^{j\omega_- t} + B_- e^{- j\omega_- t} \right),
\end{equation}
where $A_\pm$, $B_\pm$ are four complex constants depending on the intial conditions ${\bf x}(t=0)$, $\frac{d{\bf x}(t=0) }{dt}$. The formula above is revealing: we showed that the motion of the system can be written as the the sum of two harmonic motions, independent of each other, one with frequency $\omega_+$, the other with frequency $\omega_-$. 






As an example, consider the case $K_{11},K_{22},K_{12},m_1,m_2=1$. In this case, one has $\omega_+ = \sqrt{3}$, $\omega_- = 1$. Then
\begin{equation}\label{eq:TwoMassesDecomposed}
{\bf x} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1\end{bmatrix}\left(A_+ e^{j\sqrt{3} t} + A_- e^{- j\sqrt{3} t} \right) - \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1\end{bmatrix}\left(B_+ e^{j  t} + B_- e^{- j t} \right),
\end{equation}
Here the normalisation constants $a_+$, $a_-$ where chosen so that the norm of the eigenvectors is 1. (Remember that these constants are entirely arbitrary). 
If one chooses ${\bf x}(t=0)=[1,-1]^\intercal$, $d{\bf x}(t=0)/dt=[0,0]^\intercal$, the constants are set as $A_+=A_-=1/\sqrt{2}$, $B_+ = B_- = 0$, giving ultimately
\begin{equation}\label{eq:Mode2}
{\bf x} = \begin{bmatrix} 1 \\ -1\end{bmatrix}\cos\left(\sqrt{3}t\right).
\end{equation}
This shows that a two-mode system may collapse to a single mode, when the system is started in that mode. There is no trace of the other mode of vibration! One may of course start the system in the other mode, and observe it oscillating in that mode only. Figs. \ref{fig:TwoMassesMode1} and \ref{fig:TwoMassesMode2} show the motion of the masses when the system is started in either one of the two modes. 
\begin{figure}[hbt]
\centering
\includegraphics[width=0.75\linewidth,clip, trim={2cm 2cm 2cm 1cm}]{Figures/TwoMassesMode1.png}
\caption{Two-mass system. Visualisation of the first mode of vibration.}\label{fig:TwoMassesMode1}
\end{figure}
\begin{figure}[hbt]
\centering
\includegraphics[width=0.75\linewidth,clip, trim={2cm 2cm 2cm 1cm}]{Figures/TwoMassesMode2.png}
\caption{Two-mass system. Visualisation of the second mode of vibration, corresponding to \eqref{eq:Mode2}.}\label{fig:TwoMassesMode2}
\end{figure}



\subsection{Energy considerations}

It is remarked that the allowable ranges for the stiffness constants $K_{ij}$ are such that the potential energy of the system  is \emph{non-negative}, so that one may bound the growth of the solutions in some manner. The total energy of the system can be found by multiplying \eqref{eq:Coupl1} by $\frac{dx_1}{dt}$, and \eqref{eq:Coupl2} by $\frac{dx_2}{dt}$, and summing. Using the usual identities, this gives
\begin{equation}\label{eq:EnBalCnt2Masses}
\frac{d}{dt}\left( \frac{m_1}{2}\left( \frac{dx_1}{dt} \right)^2 + \frac{m_2}{2}\left( \frac{dx_2}{dt} \right)^2 + \frac{K_{11} x_1^2}{2} +  \frac{K_{22} x_2^2}{2} + \frac{K_{12} (x_1-x_2)^2}{2}\right) \triangleq \frac{dH(t)}{dt} = 0,
\end{equation}
showing that $H(t) = H(t=0) = H_0$, that is, energy is conserved. The form of the energy comprises the energy of the two harmonic oscillators in isolation, plus the coupling energy, proportional to $K_{12}$. The coupling is a function of the relative distance between the masses, so at any time such that $x_1-x_2=0$, the coupling force is null. It is convenient to write the energy in matrix form, as
\begin{equation}
H(t) = \frac{1}{2}\left(\frac{d{\bf x}^\intercal}{dt} {\bf M} \frac{d{\bf x}}{dt} + {\bf x}^\intercal {\bf K} {\bf x}\right).
\end{equation}
Since both ${\bf K}$, ${\bf M}$ are positive-definite\footnote{Remember that a positive-definite $N\times N$ symmetric matrix ${\bf A}$ is a matrix with real entries such that ${\bf x}^\intercal {\bf A} {\bf x} > 0$, $\forall {\bf x} \neq {\bf 0} \in \mathbb{R}^N$. }, one has
\begin{equation}
0 \leq \frac{d{\bf x}^\intercal}{dt} {\bf M} \frac{d{\bf x}}{dt} \leq H_0, \quad 0 \leq {\bf x}^\intercal {\bf K} {\bf x} \leq H_0,
\end{equation}
that is, the norms of the solution remain bounded by some energy constant incorporating the intial conditions. This is, in essence, the idea of stability for this vector case, generalising  bound \eqref{eq:EgyBoundVel} (and the analogous bound on $x$) of the single scalar case. 

\subsection{Eigenvalue decomposition}


The discussion above suggests that the motion system \eqref{eq:MultiModalMatr} may in fact be decomposed onto linearly independent blocks, called the \emph{modes}.  Since the mass matrix is usually a diagonal matrix with positive entries, it is convenient to rewrite the system as
\begin{equation}\label{eq:EigenDecTwoMasses1}
\frac{d^2{\bf x}}{dt^2} = -{ \bf M}^{-1}{\bf K} \, {\bf x}, 
\end{equation}
where here ${{\bf M}^{-1}\bf K}$ is a positive-definite matrix. One is then able to write
\begin{equation}\label{eq:eigendempos}
{\bf M}^{-1}{\bf K} = {\bf P} \, {\bf \Omega}^2 \, {\bf P}^{\intercal},
\end{equation}
where ${\bf P}$ is a matrix comprising the column eigenvectors of ${\bf M}^{-1}{\bf K}$, and where ${\bf \Omega}$ is a diagonal matrix containing the (positive) eigenvalues (that is, the resonant frequencies of the system). Since ${\bf M}^{-1}{\bf K}$ is positive-definite, one has that the matrix ${\bf P}$ is in fact \emph{orthonormal}, that is, ${\bf P}^{-1}={\bf P}^\intercal$. Hence, multiplying \eqref{eq:EigenDecTwoMasses1} on the left by ${\bf P}^\intercal$ gives
\begin{equation}\label{eq:diagonalEigen}
\frac{d^2{\bf u}}{dt^2} = -{\bf \Omega}^2 \, {\bf u}, 
\end{equation}
with ${\bf u} = {\bf P}^{\intercal}{\bf x}$. This is a completely diagonal system, where the  degrees of freedom are independent of each other. Coming back again to the example \eqref{eq:TwoMassesDecomposed}, here one has
\begin{equation}
{\bf P} =  \frac{1}{\sqrt{2}}\begin{bmatrix}-1 & -1 \\ -1 & 1 \end{bmatrix}, \,\, {\bf \Omega} =  \begin{bmatrix}1 & 0 \\ 0 & \sqrt{3} \end{bmatrix}.
\end{equation}
One may decide to work with the diagonal system \eqref{eq:diagonalEigen}, and then switch back to the ``physical'' coordinates $\bf x$, using ${\bf x} = {\bf P}\,{\bf u}$. Here, it is immediate to verify that ${\bf P} \, {\bf P}^{\intercal} = {\bf P}^\intercal \, {\bf P} = {\bf I}$, where ${\bf I}$ is the identity matrix. It is also immediate to check that \eqref{eq:eigendempos} is verified. 




\subsection{Loss and Forcing}

System \eqref{eq:TwoMass1} may be generalised so to include losses and external forcing. The system reads
\begin{subequations}\label{eq:TwoMass2}
\begin{align}
m_1 \frac{d^2 x_1}{dt^2} &= -K_{11}x_1 - K_{12}(x_1 - x_2) -2m_1c_{11}\frac{dx_{1}}{dt} + m_1 F_1 f(t), \label{eq:TwoMass2a} \\
m_2 \frac{d^2 x_2}{dt^2} &= -K_{22}x_2 + K_{12}(x_1 - x_2) - 2m_2 c_{22} + m_2 F_2 f(t) . \label{eq:TwoMass2b}
\end{align}
\end{subequations}
Here, the $c$'s coefficients are loss coefficients (measured in s$^{-1}$), and $F$ is a force per unit mass. (These units are consistent with the case of the single mass, in \eqref{eq:SHOForced}). The system may be written as
\begin{equation}\label{eq:tempTwoModes1}
\frac{d^2{\bf x}}{dt^2} = -{\bf M}^{-1}{\bf K} \, {\bf x} - 2 {\bf C} \, \frac{d{\bf x}}{dt} + {\bf F}\, f(t).
\end{equation}
Here, $f(t)$ is any suitable input time signal. Energy analysis may be performed by left-multiplying the system by $(\dtd {\bf x})^\intercal {\bf M}$, leading to
\begin{equation}
\frac{1}{2}\frac{d}{dt}\left(\frac{d{\bf x}^\intercal}{dt} {\bf M} \frac{d{\bf x}}{dt} + {\bf x}^\intercal {\bf K} {\bf x}\right) = -Q(t) + P(t),
\end{equation}
where $Q = 2(\dtd {\bf x})^\intercal \,{\bf M}{\bf C} \, \dtd {\bf x} \geq 0$ is the dissipated power, and where $P = (\dtd {\bf x})^\intercal \, {\bf M}{\bf F} \, f(t)$ is the injected power.









Conveniently, we may want to study the case where the input is a time-harmonic signal, such as $f(t) = e^{j\omega t}$. Motion will undergo an initial transient, before settling into the steady state. There, the system will oscillate at the same frequency as the forcing. Hence, in the steady-state, one has
\begin{equation}
{\bf x} = {\bf X}\, e^{j\omega t},
\end{equation}
and inserting this expression in \eqref{eq:tempTwoModes1} one gets
\begin{equation}
{\bf X} = \left( -\omega^2 {\bf I} + {\bf M}^{-1}{\bf K} + 2 j \omega {\bf C} \right)^{-1}{\bf F},
\end{equation}
that is the generalisation to the vector case of \eqref{eq:TransXF}. Here, one may compute the transfer functions between any of the two inputs and ouputs. It may be convenient to define the transfer functions are matrices, so that, say, the mechanical admittance is
\begin{equation}
{\bf Y} = \begin{bmatrix} Y_{11} & Y_{12} \\ Y_{21} & Y_{22} \end{bmatrix},
\end{equation}
where $Y_{ij} = \frac{j \omega X_i}{ F_j}$. Analogously, the impedance is defined as $Zij = \frac{F_i}{j \omega X_j}$. Fig \ref{fig:TransFunctionsTwoMass} presents the admittance and impedance plots for the test case $K_{11},K_{22},K_{12},m_1,m_2=1$.



\begin{figure}[hbt]
\centering
\includegraphics[width=0.85\linewidth]{Figures/TwoMassTransfFunctions.png}
\caption{Transfer functions for the two-mass system, with $K_{11},K_{22},K_{12},m_1,m_2=1$. Here, the loss matrix is ${\bf C} = \text{diag}([0.02,0.01])$, and the forcing vector is ${\bf F} = [1,0]^\intercal$. For the admittance $Y$, dots (.) is $Y_{11}$ and hats (\^{}) is $Y_{21}$. For the impedance, dots (.) is $Z_{11}$, and cicles (o) is $Z_{12}$. }\label{fig:TransFunctionsTwoMass}
\end{figure}

\subsection{An Explicit Finite Difference Scheme}


A discrete-time version of \eqref{eq:MultiModalMatr} can be obtained considering
\begin{subequations}\label{eq:TwoMassFD1}
\begin{align}
m_1 \, \dtt x_1^n = -K_{11}x_1^n - K_{12}(x_1^n - x_2^n), \label{eq:CouplFD1} \\
m_2 \, \dtt x_2^n = -K_{22}x_2^n + K_{12}(x_1^n - x_2^n), \label{eq:CouplFD2}
\end{align}
\end{subequations} 
which may be written compactly as
\begin{equation}\label{eq:TwoMassFD1Compact}
{\bf M}\, \dtt {\bf x}^n = - {\bf K}\, {\bf x}^n
\end{equation}
Expanding out the difference operators, one gets
\begin{equation}\label{eq:updateExplicitScheme}
{\bf M}\, {\bf x}^{n+1} = \left(2{\bf M}-k^2{\bf K}\right){\bf x}^n - {\bf M}{\bf x}^{n-1}.
\end{equation}
Since ${\bf M}$ is fully diagonal, this scheme is \emph{explicit}, and the update may be computed by merely multiplying both sides by the diagonal matrix ${\bf M}^{-1}$, and by performing the trivial matrix-vector operations on the right-hand side. 


Stability analysis may be performed via energy arguments. To that end, multiply \eqref{eq:CouplFD1} by $\dtd x^n_1$, and \eqref{eq:CouplFD2} by $\dtd x^n_2$, and sum. Using the usual identities, one gets
\begin{equation}\label{eq:EnBalFD2Masses}
\dtp \left(\underbrace{\frac{m_1(\dtm x^n_1)^2}{2} + \frac{m_2(\dtm x^n_2)^2}{2} + \frac{K_{11}x_1^n \, \etm x_1^n}{2}  + \frac{K_{22}x_2^n \, \etm x_2^n}{2} + \frac{K_{12}(x_1^n-x_2^n) \, \etm (x_1^n-x_2^n)}{2}}_{{\mathfrak h}^{n-1/2}}\right)  = 0,
\end{equation}
which gives the discrete energy balance. Clearly, \eqref{eq:EnBalFD2Masses} discretises \eqref{eq:EnBalCnt2Masses} to second-order accuracy, however there is no guarantee that the discrete energy is in fact positive. It may be useful to rewrite the energy as a quadratic form using the matrix notation. One has
\begin{equation}
\mathfrak{h}^{n-1/2} = \frac{1}{2}\left({\dtm {\bf x}}^{\intercal}\,{\bf M}\, {\dtm {\bf x}} + \etm{\bf x}^\intercal \,{\bf K}\, { {\bf x}}\right) = \frac{1}{2}\left( \dtm {\bf x}^{\intercal}\,\left({\bf M}-\frac{k^2}{4}{\bf K}\right)\, \dtm {\bf x} + \mtm \left({\bf x}^{\intercal}\,{\bf K}\, {\bf x}\right) \right),
\end{equation}
where the last equality follows after application of the  identity $\etm {\bf u}^\intercal  {\bf A} {\bf u}= \mtm \left({\bf u}^\intercal \,{\bf A}\, {\bf u}\right) - \frac{k^2}{4}\dtm{\bf u}^\intercal \,{\bf A}\, \dtm {\bf u}$, for a positive-definite, symmetric matrix $\bf A$. Now, since $\mtm \left({\bf x}^{\intercal}\,{\bf K}\,  {\bf x} \right) \geq 0$, one has that the discrete energy above is positive definite if and only if
\begin{equation}
\text{eig}\left({\bf M}-\frac{k^2}{4}{\bf K}\right) \geq 0.
\end{equation}
Finding conditions in the general case can be quite tedious. For the case  $K_{11},K_{22},K_{12},m_1,m_2=1$, the eigenvalue equation for eigenvalue $\lambda$ is obtained as
\begin{equation}
\left(\frac{k^2}{2} + \lambda - 1\right)^2 - \frac{k^2}{16}  \geq 0.
\end{equation}
The quadratic has two solution, $\lambda_+ = 1 - \frac{k^2}{4}$, $\lambda_- = 1 - \frac{3 k^2}{4}$, and they are both positive if and only if $\lambda_+ > 0$, i.e. 
\begin{equation}\label{eq:TwoMassesStabCond}
k \leq \frac{2}{\sqrt{3}},
\end{equation}
which may be interpreted as a stability condition here. 


\subsection{Numerical Eigenvalues}


Frequency-domain analysis may be performed here too, generalising the results of Sec. \ref{sec:FreqDomSHO}. To that end, one re-writes the scheme \eqref{eq:TwoMassFD1} as
\begin{equation}
\dtt {\bf x}^n = - {\bf M}^{-1}{\bf K}\, {\bf x}^n.
\end{equation}
Then, as suggested in Sec. \ref{sec:FDtransformations}, one substitutes the test solution ${\bf x}^n = \hat{\bf x}e^{j\omega kn}$, and remembering \eqref{eq:DTFTdtt} one gets
\begin{equation}
-\frac{4}{k^2}\sin^2\left( \frac{\omega k}{2}\right)\hat{\bf x} = - {\bf M}^{-1}{\bf K}\, \hat {\bf x}^n,
\end{equation}
showing that the eigenvalues of the matrix ${\bf M}^{-1}{\bf K}$ are 
\begin{equation}
\frac{4}{k^2}\sin^2\left( \frac{\omega k}{2}\right) = \omega^2 - \frac{k^2\omega^4}{12} + O(k^4).\end{equation}
Hence, the numerical eigenvalues are second-order accurate with respect to the eigenvalues of the continuos system, and  the absolute value of error grows approximately as $\omega^4$. Thus, the  eigenfrequencies will be less and less well computed as the stability limit is approached. Note that one may decompose the right-hand using the eigendecomposition \eqref{eq:eigendempos}, and, defining again  ${\bf u} = {\bf P}^{\intercal}{\bf x}$, the system becomes
\begin{equation}
\dtt {\bf u}^n = -{\bf \Omega}^2 \,{\bf u}^n,
\end{equation}
showing that motion can be completely uncoupled in the two modes of the system. Stability analysis here is immediate, since the energy is now expressed as the sum of \emph{independent} harmonic oscillators. Hence, stability condition \eqref{eq:StabCondSHO} translates here directly, as
\begin{equation}
k < \frac{2}{\text{max}\left(\text{diag}({\bf \Omega})\right)}.
\end{equation}
For the test case $K_{11},K_{22},K_{12},m_1,m_2=1$, one recovers of course \eqref{eq:TwoMassesStabCond}. 


\subsection{A Family of Finite Difference Schemes}


Scheme \eqref{eq:TwoMassFD1Compact}  is but one among many different possible realisations. Consider the following discretisation, depending on a parameter $\alpha \in [0,1]$.
\begin{equation}\label{eq:Impl2Masses}
{\bf M}\, \dtt {\bf x}^n = - {\bf K}\, (\alpha + (1-\alpha)\mtd) {\bf x}^n.
\end{equation}
Remebering that $\mtd = 1 + \frac{k^2}{2}\dtt$, one can rewrite the scheme as 
\begin{equation}
\left({\bf M} + (1-\alpha)\frac{k^2}{2}{\bf K} \right) \, {\bf x}^{n+1} = \left(2 {\bf M} - \alpha k^2 {\bf K} \right){\bf x}^n - \left({\bf M} + (1-\alpha)\frac{k^2}{2}{\bf K} \right) \, {\bf x}^{n-1}.
\end{equation}
Here, the scheme is \emph{implicit}, since the matrix multiplying the update vector ${\bf x}^{n+1}$ is now (generally) not diagonal! The update equation is in the form of a linear system. This is, of course, more laborious than the update \eqref{eq:updateExplicitScheme} for the fully-explicit scheme. Of course, setting $\alpha = 1$ in the above, one recovers the explicit scheme \eqref{eq:TwoMassFD1Compact}.
Energy analysis is performed in the usual way, i.e. by left-multiplying \eqref{eq:Impl2Masses} by $(\dtd {\bf x}^n)^\intercal$. By means of the usual identities, this leads to the discrete energy balance
\begin{equation}
 \dtp \frac{1}{2}\left( \dtm {\bf x}^{\intercal}\,{\bf M}\, \dtm {\bf x} + \alpha \etm {\bf x}^{\intercal}\,{\bf K}\,  {\bf x} + (1-\alpha) \mtm \left({\bf x}^{\intercal}\,{\bf K}\,  {\bf x}\right) \right) \triangleq  \dtp {\mathfrak h}^{n-1/2}  = 0.
\end{equation}
An interesting case is obtained when $\alpha =0$, yielding a form for the discrete energy that is non-negative \emph{in all cases}.
Hence, scheme \eqref{eq:Impl2Masses} becomes \emph{unconditionally stable} when $\alpha = 0$. One should be aware that unconditional stability is but one of the many aspects to be considered whilst designing an appropriate scheme. Whilst scheme \eqref{eq:Impl2Masses} will remain stable, regardless of the sample rate used, multiple other issues may affect the quality of the resulting simulated dynamics, particularly with respect to Fourier and aliasing considerations. Fig. \ref{fig:TwoMassOne} presents a numerical investigation of the two mass system, for the test case $K_{11},K_{22},K_{12},m_1,m_2=1$. 



A stability condition for scheme \eqref{eq:Impl2Masses} may be obtained by writing out the expression for the energy as a quadratic form in ${\bf p} = [\left({\bf x}^n\right)^{\intercal}, \left({\bf x}^{n-1}\right)^{\intercal}]^{\intercal}$, as $\mathfrak{h}^{n-1/2} = {\bf p}^{\intercal}\,{\bf A}(\alpha,k)\,{\bf p}$, where
\begin{equation}
{\bf A}(\alpha,k) = 
\begin{bmatrix}
\frac{{\bf M}}{k^2} + \frac{(1-\alpha)}{2}{\bf K} & -\frac{{\bf M}}{k^2} +\frac{\alpha}{2}{\bf K} \\ -\frac{{\bf M}}{k^2} + \frac{\alpha}{2}{\bf K} & \frac{{\bf M}}{k^2} + \frac{(1-\alpha)}{2}{\bf K}
\end{bmatrix}.
\end{equation}
Positive-definiteness is obtained if and only if 
\begin{equation}\label{eq:StabCondMasses}
\text{eig}\left({\bf A}(\alpha,k)\right) > 0.
\end{equation}
This can be interpreted a stability condition for the system.
\begin{figure}[hbt]
\centering{}
\includegraphics[width=0.85\linewidth]{Figures/TwoMassOne.png}
\caption{Free oscillations of the two-mass system. Here, $K_{11},K_{22},K_{12},m_1,m_2=1$. The sample rate is chosen as $f_s = 50$ Hz, and $\alpha = 0.5$. The initial conditions are given as $x_1(0) = 1$, $\frac{dx_1(0)}{dt} =0$, $x_2(0) = 0$, $\frac{dx_2(0)}{dt} =0$. (a): time evolution of $m_1$ (dots) and $m_2$ (circles). (b): spectra of the solutions. (c): energy components: kinetic (\^{}), potential (\text{*}), and total (solid line). (d): numerical energy  error, defined as $\Delta H = 1 - \mathfrak{h}^{n-1/2}/\mathfrak{h}^{1/2}$.}\label{fig:TwoMassOne}
\end{figure}


\subsubsection{Loss and Forcing}

Inclusion of losses and external forcing is immediate from the template given above. Considering the continuous system \eqref{eq:tempTwoModes1}, a discrete-time version is obtained as
\begin{equation}
\dtt {\bf x}^n = -{\bf M}^{-1}{\bf K}\, (\alpha + (1-\alpha)\mtd) {\bf x}^n - 2{\bf C}\dtd {\bf x}^n + {\bf F} f^n.
\end{equation}
The energy balance is obtained after left-multiplying the equation by $(\dtd {\bf x}^n)^\intercal{\bf M}$, yielding
\begin{equation}
\frac{1}{2}\dtp \left( \dtm {\bf x}^{\intercal}\,{\bf M}\, \dtm {\bf x} + \alpha \etm {\bf x}^{\intercal}\,{\bf K}\,  {\bf x} + (1-\alpha)\mtm \left({\bf x}^{\intercal}\,{\bf K}\,  {\bf x}\right)  \right)   = -Q^n+P^n,
\end{equation}
where $Q^n = 2(\dtd {\bf x}^n)^\intercal \, {\bf C} \, \dtd {\bf x}^n \geq 0$ is the dissipated power, and where $P^n = ({\dtd {\bf x}^n})^\intercal \, {\bf F} \, f^n$ is the injected power. 
Here, since ${\bf C}$ is a symmetric, positive-definite matrix, the stability analysis is unaffected, and one may still impose \eqref{eq:StabCondMasses}. Of course, the scheme above allows to study the system under general forcing conditions, and to simulate the evolution of the system including transients. 


\section{Nonlinearly coupled oscillators}

If the amplitude of the oscillations is sufficiently large, nonlinear effects come into play. As an example, we are going to consider the following case
\begin{subequations}\label{eq:TwoMass1Nlin}
\begin{align}
m_1 \frac{d^2 x_1}{dt^2} = -K_{11}x_1 - K_{12}(x_1 - x_2) - K^{(nl)}_{12}(x_1 - x_2)^3, \label{eq:Coupl1Nlin} \\
m_2 \frac{d^2 x_2}{dt^2} = -K_{22}x_2 + K_{12}(x_1 - x_2) + K^{(nl)}_{12}(x_1 - x_2)^3. \label{eq:Coupl2Nlin}
\end{align}
\end{subequations}
Here, the two masses are connected nonlinearly, via the cubic term. Energy analysis is again revealing: multiplying \eqref{eq:Coupl1Nlin} by $\frac{d x_1}{dt}$, and \eqref{eq:Coupl2Nlin} by $\frac{d x_2}{dt}$, and summing, leads to the energy balance
\begin{equation}
\frac{dH}{dt} = 0, \,\, \text{ where } \,\, H(t) = \frac{1}{2}\left(\frac{d{\bf x}^\intercal}{dt} {\bf M} \frac{d{\bf x}}{dt} + {\bf x}^\intercal {\bf K} {\bf x}\right) + \frac{K^{(nl)}_{12}}{4}(x_1-x_2)^4.
\end{equation}
Considering for simplicity the hardening case $K^{(nl)}_{12} \geq 0$, one has that the total energy is non-negative, and therefore stability of the nonlinear system ensues. The nonlinearity is such that it is no longer possible to decompose the system into independently vibrating modes. If the system is started in either one of the linear modes of the system, energy will eventually pass onto the second mode, and the steady state will comprise an oscillation of the two modes together exchanging energy.


An analysis of this system using perturbation techniques, such as the method of multiple scales, is possible, though the algebra quickly becomes unwiedly. Numerical methods, on the other hand, are very easily implemented using the techniques already seen for the single scalar nonlinear oscillator,  as will be seen shortly. 


\subsection{Loss and Forcing}


The linear template including losses and external inputs can be modified readily from \eqref{eq:tempTwoModes1}, yielding 
\begin{equation}\label{eq:tempTwoModes1Nlin}
\frac{d^2{\bf x}}{dt^2} = -{\bf M}^{-1}{\bf K} \, {\bf x}  - {\bf M}^{-1}{\bf K}^{(nl)}(x_1-x_2)^3 - 2 {\bf C} \, \frac{d{\bf x}}{dt} + {\bf F}\, f(t),
\end{equation}
where ${\bf K}^{(nl)} = K_{12}^{(nl)}[1,-1]^\intercal$. The energy balance in this case is
\begin{equation}
\frac{1}{2}\frac{d}{dt}\left(\frac{d{\bf x}^\intercal}{dt} {\bf M} \frac{d{\bf x}}{dt} + {\bf x}^\intercal {\bf K} {\bf x}+ \frac{K^{(nl)}_{12}}{2}(x_1-x_2)^4\right) = - Q(t) + P(t),
\end{equation}
where $Q = 2(\dtd {\bf x})^\intercal \,{\bf M}{\bf C} \, \dtd {\bf x} \geq 0$ is the dissipated power, and where $P = (\dtd {\bf x})^\intercal \, {\bf M}{\bf F} \, f(t)$ is the injected power.

\subsection{Finite Difference schemes}

A number of options are available for the integration of \eqref{eq:tempTwoModes1Nlin}. Here, we will explore the following, depending on the free parameter $\alpha \in [0,1]$:
\begin{equation}
\dtt {\bf x}^n = -{\bf M}^{-1}{\bf K}\, (\alpha + (1-\alpha)\mtd) {\bf x}^n - {\bf M}^{-1}{\bf K}^{(nl)}(x^n_1-x^n_2)^2\, \mtd(x^n_1-x^n_2)- 2{\bf C}\dtd {\bf x}^n + {\bf F} f^n.
\end{equation}
Left-multiplying the system by $(\dtd {\bf x}^n)^\intercal{\bf M}$, one gets the discrete energy balance
\begin{equation}
\frac{1}{2}\dtp \left( \dtm {\bf x}^{\intercal}\,{\bf M}\, \dtm {\bf x} + \alpha \etm {\bf x}^{\intercal}\,{\bf K}\,  {\bf x} + (1-\alpha) \mtm \left({\bf x}^{\intercal}\,{\bf K}\,  {\bf x}\right) + \frac{K^{(nl)}_{12}}{2}{(x^n_1-x^n_2)^2\etm(x^n_1-x^n_2)^2}  \right)   = -Q^n+P^n,
\end{equation}
where $Q^n = 2(\dtd {\bf x}^n)^\intercal \, {\bf C} \, \dtd {\bf x}^n$ is the dissipated power, and where $P^n = ({\dtd {\bf x}^n})^\intercal \, {\bf F} \, f^n$ is the injected power. The parameter $\alpha$ may again be selected according to spectral considerations, that is, in order to reduce numerical dispersion. The stability condition \eqref{eq:StabCondMasses} holds here too. 




\chapter{Strings}


This chapter introduces the analysis of continuous systems. The case of a taut string will be considered firts, described by the one-dimensional wave equation. While simple, this equation serves as a model for various other systems, including wave propagation in acoustic tubes, and is generally regarded as the first-ever example of a partial differential equation. Outside of its historical importance, the wave equation can be approached by a vast amount of analytical and numerical techniques. Exact solutions exists in the form of travelling wavefronts (the D'Alembert solution), as well as in the form of standing wave patterns (the modes). Equivalence of these solutions is proven by Fourier theory. Numerically, the solution may be computed using direct numerical simulation, such as finite differences. A discretisation of the D'Alembert solution forms the core of digital waveguide theory, probably the most popular physical modelling technique to date. The solution of the spatial eigenvalue problem using the spatial modes allows to describe the system using a set of uncoupled oscillators, and the numerical techniques used in previous chapters can be directly applied. 


\section{Equation of motion}





We begin by assuming a lossless, infinitely thin string lying under tension $T(x,t)$ along the $x$-axis, whose vertical displacement is denoted $y$, as illustrated in Figure \ref{fig:infinite_string}. 
\begin{figure}[hbt]
\begin{center}
\includegraphics[width=6.5cm]{Figures/infinite_string.pdf}
\caption{A infinitely long string lies oriented lengthways along the $x$-axis, with its motion confined to the transverse $y$-axis.}
\label{fig:infinite_string}
\end{center}
\end{figure}
\begin{figure}[hbt]
\begin{center}
\includegraphics[width=0.7\linewidth]{Figures/stringElement.pdf}
\caption{A infinitesimal section of the infinite string. }
\label{fig:infinitesimal_string}
\end{center}
\end{figure}
We are now going to derive the dynamic equilibrium equations for the element of string depicted in Figure \ref{fig:infinitesimal_string}. For the moment, we are assuming that the displacement $y$, the deflection angle $\theta$ and the tension $T$ are all functions of the spatial coordinate $x$. Restricting the attention to the vertical direction, the infinitesimal vertical force $df$ acting on the string element is given by the difference of the force at the right end and the force at the left end, hence
\begin{equation}\label{eq:balance_eq}
df = T(x+dx,t) \sin\left( \theta(x+dx,t) \right) - T(x,t) \sin\left( \theta(x,t) \right) = dx \frac{\partial}{\partial x}(T \sin\theta)
\end{equation}
In dynamic equilibrium, the infinitesimal force is equal to the infinitesimal mass times the acceleration. The infinitesimal mass is given by the volume density $\rho$ times the volume of the string's element, $dm = ds \rho A$, where $A$ is the cross section of the string (supposed constant here), and where $ds$ is the infinitesimal arclength, $ds = dx \sqrt{1+(\partial y / \partial x)^2}$. Hence
\begin{equation}\label{eq:inf_mass}
df = ds \rho A \frac{\partial^2 y}{\partial t^2}
\end{equation}
We can now eliminate $df$ in  \eqref{eq:balance_eq} using \eqref{eq:inf_mass}, to get
\begin{equation}\label{eq:WEtheta}
\rho A \frac{\partial^2 y}{\partial t^2}ds = \frac{\partial}{\partial x}(T \sin\theta)dx
\end{equation}
What we need now are explicit expressions for the tension $T$ and the angle $\theta$ as a function of the  coordinate $x$. From Figure \ref{fig:infinitesimal_string}, the angle is obtained geometrically as
\begin{equation}
\tan\theta = \frac{\partial y}{\partial x} \quad \rightarrow \quad \sin\theta = \frac{\frac{\partial y}{\partial x}}{\sqrt{1 + (\frac{\partial y}{\partial x})^2}} 
\end{equation}
In the limit of small displacements, both the deflection angle and the displacement are small, so that $\sin \theta  \approx \partial y / \partial x$, and $dx \approx ds$. One may also assume that, under such conditions, the tension is uniform, i.e. $T(x,t)=T_0$. Thus, using these in \eqref{eq:WEtheta} yields
\begin{equation}\label{eq:WE}
 \frac{\partial^2 y}{\partial t^2} = c^2 \frac{\partial^2 y}{\partial x^2}
\end{equation}
where we have conveniently defined the wave velocity $c = \sqrt{T_0 /\rho A}$,





This is an example of a \emph{partial differential equation} (often abbreviated as PDE). Such equations involve \emph{partial} derivatives, and relate the rate of change in time of a given quantity (here, displacement), to its spatial variation. Equation \eqref{eq:WE} is often referred to as the \emph{simple wave equation}, or just as \emph{the wave equation}.
One first observation about the wave equation is that acceleration is proportional to the second spatial derivative. Remember that the second spatial derivative is a measure of the curvature of the function: hence, if the curvature is positive at a given location along the string, then force is pointing upward, and if the curvature is negative, then the force is pointing downward.  We are now going to introduce some of the standard techniques for the solution of equations such as \eqref{eq:WE}. 




\section{Solutions to the 1D wave equation}

\emph{Separation of variables} is among the techniques often employed in the analysis of PDEs. For that, we assume that
\begin{equation}\label{eq:sepVar}
y(x,t) = X(x)T(t)
\end{equation}  
Although there is no specific reason why a solution would have this form, we will assume that this is the case, and we will check at the end whether the solution obtained via separation is actually meaningful. Inserting \eqref{eq:sepVar} into \eqref{eq:WE}, and dividing both sides by $XT$, one gets
\begin{equation}
\frac{T^{\prime\prime}}{T} = c^2 \frac{X^{\prime\prime}}{X} 
\end{equation}
Note that, in the equation above, primes indicate derivatives with respect to the function's own argument (i.e. $t$ for $T$, and $x$ for $X$.) Notice also that the left-hand side depends only on $t$, and the right-hand side depends only on $x$: hence, we may conveniently write
 \begin{subnumcases}{ \label{eq:TXsyst}  }
\frac{T^{\prime\prime}}{T}  = s^2   \label{eq:TXsyst1} \\
c^2\frac{X^{\prime\prime}}{X}  = s^2 \label{eq:TXsyst2}
\end{subnumcases}
where $s$ is a constant that does not depend on $t$ or $x$. The interest of using this system is that the two equations are now in the form of \emph{ordinary differential equations} (ODE) of the kind already encountered whilst studying the  oscillator.
We can then solve \eqref{eq:TXsyst1} using the exponential technique. This gives
\begin{equation}
T(t) = A_+ e^{st} + A_- e^{-st}, \quad A_+,A_- \in \mathbb{C}
\end{equation}
Clearly, for this solution to be oscillating and not exponentially growing or decaying one must set $s \in j\mathbb{R}$ and, in fact, one may define
\begin{equation}
s = j \omega, \quad \omega \in \mathbb{R}
\end{equation}
We are now in the position of interpreting \eqref{eq:TXsyst1}: this is just the equation of a simple harmonic oscillator in time, with radian frequency given by $\omega$. The fact that the temporal component oscillates at such frequency ensures that the spatial component is also oscillating: this is encaspsulated in \eqref{eq:TXsyst}. This results in the \emph{dispersion relation} for the wave equation:
\begin{equation}\label{eq:DispRelWECnt}
\omega = c \gamma,
\end{equation}
where $\gamma$ is the spatial frequency, measured in rad/m, and called the \emph{wavenumber}. 
The solution to \eqref{eq:TXsyst2} is obtained analogously. One has
\begin{equation}
X(x) = B_+ e^{j\gamma x} + B_- e^{-j\gamma x}, \quad B_+,B_- \in \mathbb{C}
\end{equation}
Thus, inserting these in \eqref{eq:sepVar} gives
\begin{equation}\label{eq:genSolWE}
y(x,t) = \left( A_+ e^{j\omega t} + A_- e^{-j\omega t}\right)\left( B_+ e^{j\gamma x} + B_- e^{-j\gamma x}\right) 
\end{equation}
We remark two things: the first, is that the solution $y(x,t)$ is now expressed as a combination of complex-valued functions. The second, is that such combination depends on four arbitrary constants $A_+,A_-,B_+,B_-$. We make the observation that these constant can be adjusted so that the general solution \eqref{eq:genSolWE} satisfies the \emph{initial conditions} in time, plus the \emph{boundary conditions} at the string's ends. We will come back to these conditions later. For the moment, we will assume that the solution  \eqref{eq:genSolWE} exists for all times, and that the string's domain is the entire real axis (so that the boundaries are located at $\pm \infty$): in this case, the four constants are completely arbitrary, and we may use them to construct \emph{real} solutions. 



\subsection{Standing sinusoid as sum of travelling sinusoids}
\begin{figure}[hbt]
\includegraphics[width=\linewidth,clip,trim={2cm 2cm 2cm 2cm}]{Figures/TravWavesSum.png}
\caption{Sum of travelling waves giving rise to a standing wave. The black wave is travelling to the left, and the grey wave to the right. The sum of the two is depicted as crosses (+), and is a standing wave. Here, $\omega = 10\pi$, $\gamma = 4\pi$: the wavelength is $\lambda = 2\pi/\gamma = 0.5$m, as visible from the plots.}\label{fig:travStd}
\end{figure}
Under some arbitrary choice of the four constants in \eqref{eq:genSolWE}, one gets
\begin{equation}\label{eq:SttoTr}
y(x,t) = \cos (\omega t )\cos (\gamma x )
\end{equation}
You may think of this function as a standing wave pattern that extends across the whole $x$-axis. You may notice that, at the locations given by $x = (2n+1)\pi /2 \gamma$, $n\in\mathbb Z$, the solution is always zero: these are the \emph{nodes} of the standing wave. On the other hand, at the locations given by $x = n\pi/\gamma$, $n\in\mathbb Z$, the solution reaches its maximum amplitude ($\pm 1$) at the times $t = m\pi/\omega$, $m\in\mathbb Z$, see also Figure \ref{fig:travStd}. The distance between two maxima is then $\lambda = 2\pi / \gamma$: this distance is called the \emph{wavelength}, and $\gamma$ is called the $\emph{wavenumber}$ (i.e. it is the spatial radian frequency). Using basic trigonometric identities, one may re-write \eqref{eq:SttoTr} as
\begin{equation}\label{eq:SumTrav}
y(x,t) = \frac{1}{2}\left(\cos (\omega t + \gamma x) + \cos (\omega t - \gamma x ) \right)
\end{equation}
Now, the term $\cos (\omega t + \gamma x)$ is a cosine function \emph{travelling to the left}; the term $\cos (\omega t - \gamma x )$ is a cosine function \emph{travelling to the right}, see also Figure \ref{fig:travStd}. The waves are travelling with speed $\pm c=\pm\sqrt{T_0/ \rho A}$. Why are these travelling waves? Just think of the points of constant phase: these are $\omega t \pm \gamma x = \text{constant}$.
Deriving the expression above one obtains
\begin{equation}
v_\phi = \frac{dx}{dt} = \pm \frac{\omega}{\gamma} = \pm c
\end{equation}
The velocity of constant phase, indicated here as $v_\phi$, is called the \emph{phase velocity}. The absolute value of the phase velocity is given by $|\omega/\gamma|$, and for the simple wave equation such ratio is always equal to $c$.

The passage from \eqref{eq:SttoTr} to \eqref{eq:SumTrav}, explained in terms of trigonometry, bears in fact some consequences from a physical point of view: we showed that a standing wave can be written as the sum of two travelling waves going opposite directions. 

\subsection{Travelling sinusoid as sum of standing sinusoids}
\begin{figure}[hbt]
\includegraphics[width=\linewidth,clip,trim={2cm 2cm 2cm 2cm}]{Figures/StdWavesSum.png}
\caption{Sum of standing waves giving rise to a travelling wave. The black wave is a standing wave of the form $\sin(\omega t)\sin(\gamma x)$, and the grey wave is a standing wave of the form $\cos(\omega t)\cos(\gamma x)$. The sum of the two is depicted as crosses (+), and is a travelling wave to the right. Here, $\omega = 10\pi$, $\gamma = 4\pi$.}\label{fig:stdTrav}
\end{figure}
The converse is also true. Starting again from Equation \eqref{eq:genSolWE}, we may arrange the four arbitrary constants so to obtain
\begin{equation}
y(x,t) = \cos(\omega t - \gamma x)
\end{equation}
This is of course the expression of a cosine travelling to the right with phase velocity $v_\phi = c$. Using again appropriate trigonometric identities, one has
\begin{equation}
y(x,t) = \cos(\omega t)\cos(\gamma x) + \sin(\omega t)\sin(\gamma x)
\end{equation}
and, of course, this expression indicates that we can write the travelling cosine as the sum of two standing waves, see also Figure \ref{fig:stdTrav}. 





\subsection{General travelling wave solutions for $y(x,t)$ for the 1D wave equation}


The simple wave equation allows solutions that have a more general character than the sinusoidal solutions found so far. First, rewrite the wave equation as
\begin{equation}\label{eq:WEfac}
\left( \frac{\partial}{\partial t} - c  \frac{\partial}{\partial x}\right)\left( \frac{\partial}{\partial t} + c  \frac{\partial}{\partial x}\right) y(x,t) = 0
\end{equation}
Second, we are now performing a change of variables. Hence, we are mapping $(x,t) \in \mathbb{R}^2$ onto $(\eta,\zeta) \in \mathbb{R}^2$ so that
\begin{equation}
\eta = x + ct, \qquad  \zeta = x - ct.
\end{equation}
We are then using the chain rule to take partial derivatives in \eqref{eq:WEfac}. Thus
\begin{subequations}
\begin{align}
\frac{\partial}{\partial t} &= \frac{\partial \eta}{\partial t}\frac{\partial }{\partial \eta} + \frac{\partial \zeta}{\partial t}\frac{\partial }{\partial \zeta} = c \frac{\partial }{\partial \eta} - c \frac{\partial }{\partial \zeta} \\
\frac{\partial}{\partial x} &= \frac{\partial \eta}{\partial x}\frac{\partial }{\partial \eta} + \frac{\partial \zeta}{\partial x}\frac{\partial }{\partial \zeta} = \frac{\partial }{\partial \eta} + \frac{\partial }{\partial \zeta} 
\end{align}
\end{subequations}
Using these into \eqref{eq:WEfac} gives the following PDE 
\begin{equation}
\frac{\partial^2y}{\partial \eta \partial \zeta}  = 0
\end{equation}
Hence, direct integration gives
\begin{equation}
y(\eta,\zeta) = f(\eta) + g(\zeta) 
\end{equation}
or,
\begin{equation}\label{eq:Dalem}
y(x,t) = f(x+ct) + g(x-ct)
\end{equation}
In view of the discussion in the previous sections, you will recognise that $f,g$ have the form of a leftward and rightward travelling wave, respectively. Notice that this result is more general than the sinusoidal solutions of the previous section: here, $f,g$ are \emph{any} function of $x+ct$ and $x-ct$. Examples of functions that are solution to the wave equation:
\begin{equation}\label{eq:WEsolsEx}
y_1(x,t) = e^{(x-ct)}, \,\,\, y_2(x,t) = \log \left( (x+ct)^2 \right), \,\,\, y_3(x,t) =  ...
\end{equation}
Beware that nonlinear combinations (i.e. products) of $f,g$ are not, in general, solutions. Examples of functions that \emph{are not} solutions to the wave equation:
\begin{equation}
p_1(x,t) = (x-ct)e^{x+ct}, \,\,\, p_2(x,t) = \log \left( (x+ct)^2 \right)\left(\cos(x-ct)\right)^2, \,\,\, p_3(x,t) = ...
\end{equation}
This property of allowing solutions such as $f(x+ct)$ and $g(x-ct)$ is not valid for other one dimensional PDEs in acoustics: the simple wave equation is special in this sense. This equation describes the motion of ideal strings, and also the vibration of air columns with cilindrical geometry. These forms for $f,g$ were first derived by D'Alembert in 1747, and are known as \emph{D'Alembert solutions}. They are at the core of early physical modelling techniques based around \emph{digital waveguides}.


Important note. Although function such as those in \eqref{eq:WEsolsEx} solve the wave equation mathematically, they might not be "energy-compatible". In practice, not only does a function need to solve \eqref{eq:WE}, but it also must comply with some requirements at the string's ends, the boundary conditions. This aspect will become clearer as we progress through the course.

\section{Energy for the 1D wave equation}


In order to derive energy expressions for the string, we might try to relate the work done by the tension forces to stretch an infinitesimal element of string, as that in Figure \ref{fig:infinitesimal_string}. The corresponding change in potential energy is given by the stretching in the deformed configuration, minus the stretching in the undeformed configuration, times the tension $T_0$. Hence
\begin{equation}
dE_p =  T_0 (ds-dx) =  T_0  \left(\sqrt{1 + \left( \frac{\partial y}{\partial x}\right)^2} - 1 \right)dx
\end{equation}
To first order, one has
\begin{equation}
dE_p \approx \frac{T_0}{2}\left(\frac{\partial y}{\partial x}\right)^2 dx
\end{equation}
From Newtonian physics of conservative systems, we know that sum of the changes of potential and kinetic energies is equal to zero, hence
\begin{equation}
dE_k + dE_p = 0 
\end{equation}
where $dE_k = \frac{\rho A}{2} \left( \frac{\partial y}{\partial t} \right)^2 dx$. Integrating this last expression between $x_l $ and $x_r$ ($x_l<x_r$) gives
\begin{equation}\label{eq:EnBal}
E_k + E_p = \int_{x_l}^{x_r} \frac{\rho A}{2} \left(\frac{\partial y}{\partial t}\right)^2 dx + \int_{x_l}^{x_r} \frac{T_0}{2} \left(\frac{\partial y}{\partial x}\right)^2 dx = E
\end{equation}
where $E$ is the total energy (a constant). Notice that the total energy is a function of time only. 



\subsection{Energy balance for an infinitely long string}



We can try to check whether the energy balance equation \eqref{eq:EnBal} actually provides a consistent definition for the total energy. To that extent, take a time derivative of \eqref{eq:EnBal}. This is
\begin{equation}
\frac{d}{dt}\left( E_k + E_p \right) = \int_{x_l}^{x_r} \rho A \left(\frac{\partial^2 y}{\partial t^2}\right)\left(\frac{\partial y}{\partial t}\right) dx + \int_{x_l}^{x_r} T_0 \left(\frac{\partial^2 y}{\partial t\partial x}\right)\left(\frac{\partial y}{\partial x}\right) dx = 0
\end{equation}
For the moment, we are only concerned with infinitely longs strings, so that $x_l = -\infty$, $x_r = \infty$. Using integration by parts to calculate the second integral, one can rewrite the energy balance as
\begin{equation}
\int_{-\infty}^{\infty} \rho A \left(\frac{\partial^2 y}{\partial t^2}\right)\left(\frac{\partial y}{\partial t}\right) dx - \int_{-\infty}^{\infty} T_0 \left(\frac{\partial^2 y}{\partial x^2}\right)\left(\frac{\partial y}{\partial t}\right) dx = - T_0 \left(\frac{\partial y}{\partial t}\right)\left(\frac{\partial y}{\partial x}\right)\bigg|_{-\infty}^{\infty}
\end{equation}
We make the assumption that the right-hand side vanishes. (In fact, this puts some limitations on the form of the solutions to the wave equation. What can you say about the functions in \eqref{eq:WEsolsEx}?)


Collecting the common factor in the integrals, we finally get
\begin{equation}
\int_{-\infty}^{\infty} \left[ \rho A \left(\frac{\partial^2 y}{\partial t^2}\right) - T_0 \left(\frac{\partial^2 y}{\partial x^2}\right) \right]\left(\frac{\partial y}{\partial t}\right) dx  = 0
\end{equation}
Comparing this last expression with \eqref{eq:WE}, we see that the integrand is in fact identically zero. Thus, we observe that the energy expressions in \eqref{eq:EnBal} are consistent with the wave equation, provided that the solutions behave as required at the boundary of the domain.



\section{Initial Conditions}
We are now going to try to prove that, regardless of the approach we wish to adopt to describe the solutions, we are going to end up with the same solution. In order to see this, we are now going to introduce \emph{initial conditions} on the string, such that we may assume that the string is at rest for $t<0$, and that some prescribed initial conditions are given at $t=0$. For simplicity, only displacement initial conditions will be studied here, so that
\begin{equation}\label{eq:ICs}
y(x,0) = y_0(x), \qquad \frac{\partial y(x,0)}{\partial t} = v_0(x).
\end{equation}
For simplicity, only displacement initial conditions will be studied in the following, for which $v_0(x) = 0$. We are now going to solve the wave equation, with these initial conditions, using both the D'Alembert solution, and the solution in terms of sinusoids (in fact, a Fourier method). 


\subsection{D'Alembert solution with prescribed initial conditions}
Using \eqref{eq:Dalem} into \eqref{eq:ICs} gives
\begin{equation}
f(x)+g(x) = y_0(x), \qquad f^\prime(x) - g^\prime(x) = 0
\end{equation}
Integrating the second condition, one gets $f(x) = g(x)$, and using this into the first condition  one gets
\begin{equation}
f(x)=g(x)=\frac{y_0(x)}{2}
\end{equation}
so that
\begin{equation}\label{eq:Tr}
y(x,t) = \frac{1}{2}\left( y_0(x-ct) + y_0(x+ct)\right)
\end{equation}
The meaning of this last equation is that any initial displacement will split into two travelling wavefronts, of  amplitude equal to one half that of the initial displacement. (If the initial velocity is not identically zero, the solution is a little more involved, but it is again separable into two travelling wavefronts.)


\subsection{Fourier solution with prescribed initial conditions}

If instead we wish to use the sinusoidal representation for the solution, note that the most general form is given by the Fourier integral
\begin{equation}\label{eq:FTy}
y(x,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat y(\gamma,t) e^{j\gamma x}d\gamma
\end{equation}
where $\hat y(\gamma,t)$ is the Fourier transform of $y(x,t)$, defined as
\begin{equation}\label{eq:yhat}
\hat y(\gamma,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}  y(x,t) e^{-j\gamma x}d x
\end{equation}
The meaning of \eqref{eq:FTy} is that the solution can be represented as a superposition of sine / cosine functions of $\gamma x$, times an appropriate complex amplitude $\hat y(\gamma,t)$. The complex amplitude can be calculated by taking a Fourier transform of the kind \eqref{eq:yhat} of the wave equation \eqref{eq:WE}. This gives
\begin{equation}
\frac{\partial^2 \hat y(\gamma,t)}{\partial t^2} = - c^2\gamma^2 \hat y(\gamma,t),
\end{equation}
which is the usual harmonic oscillator equation solved by
\begin{equation}\label{eq:yhh}
\hat y(\gamma,t) = A_+(\gamma) e^{jc\gamma t} + A_-(\gamma)-e^{-jc\gamma t} 
\end{equation}
We can now impose the initial conditions \eqref{eq:ICs} (transformed in the Fourier domain)
\begin{equation}
\hat y(\gamma,0) = \hat y_0(\gamma), \qquad \frac{\partial \hat y (\gamma,0)}{\partial t} = 0
\end{equation}
which give
\begin{equation}
A_+(\gamma) = A_-(\gamma) = \frac{\hat y_0(\gamma)}{2}
\end{equation}
Using these into \eqref{eq:yhh} and that into \eqref{eq:FTy} gives the solution 
\begin{equation}
y(x,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}\frac{\hat y_0(\gamma)}{2} \left( e^{-j c\gamma t} + e^{j c\gamma t} \right) e^{j\gamma x} d\gamma 
\end{equation}
which may be written as
\begin{equation}\label{eq:FTtoTr}
y(x,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}\frac{\hat y_0(\gamma)}{2}  e^{j\gamma (x - c  t)}d\gamma +  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}\frac{\hat y_0(\gamma)}{2}  e^{j\gamma (x + c  t)}d\gamma  = \frac{1}{2}\left( y_0(x-ct) + y_0(x+ct)\right)
\end{equation}
where the last equality is simply an application of the definition of the Fourier transform as per \eqref{eq:FTy}. Comparing \eqref{eq:FTtoTr} with \eqref{eq:Tr}, we see that the result is the same. Note as well that, in \eqref{eq:FTtoTr}, we effectively write the solution as an infinite combination (an integral) of travelling sinusoidal functions of amplitude given by the intial conditions.

\medskip \medskip 

\noindent
The net result of this discussion is that it should not matter how you decide to write out your solution: you should always end up with the same thing once initial conditions are imposed! Note that, so far, we have not yet talked of boundary conditions, as we have assumed that the string is infinite in length. The rest of today's lecture is to understand how the introduction of bounded domains imposes further restrictions on the nature of the solutions. 



\section{Energy of a semi-infinite string. Boundary conditions.}\label{sec:WaveEquationsBCs}



The nature of boundary conditions may be understood from energy arguments. Recall  that the energy of the flexible string is 
\begin{equation}
E = E_k + E_p = \int_{-\infty}^{\infty} \frac{\rho A}{2}\left( \frac{\partial y}{\partial t} \right)^2 dx + \int_{-\infty}^{\infty} \frac{T_0}{2}\left( \frac{\partial y}{\partial x} \right)^2 dx
\end{equation}
The energy balance is expressed as $dE/dt = 0$. However, these results are only valid when the string is infinite in length. Suppose now that the string is only defined for $x\geq 0$. In practice we have introduced a boundary point at $x=0.$ The energy is given by
\begin{equation}\label{eq:EnHalf}
E = E_k + E_p = \int_{0}^{\infty} \frac{\rho A}{2}\left( \frac{\partial y}{\partial t} \right)^2 dx + \int_{0}^{\infty} \frac{T_0}{2}\left( \frac{\partial y}{\partial x} \right)^2 dx
\end{equation}
However, the energy balance is now
\begin{equation}\label{eq:EnBal}
\frac{dE}{dt} = - T_0 \frac{\partial y}{\partial x }\Bigg|_{x=0}\frac{\partial y}{\partial t }\Bigg|_{x=0}
\end{equation}
As you can see, the boundary term can now influence the energy balance in such a way as to produce or disspate power! (Note that the energy balance is expressed as the product of a force times a velocity at the boundary point.) One way of ensuring that $dE/dt = 0$ (as per the string of infinite length), is then to require that either the force, or the velocity, is zero at the boundary. Thus, one may set
\begin{equation}
y\big|_{x=0} = 0 \,\,\, \forall t, \,\,\, \text{or } \,\,\, \frac{\partial y}{\partial x }\Bigg|_{x=0} = 0 \,\,\, \forall t
\end{equation}
The first condition corresponds to zero displacement / velocity, and is therefore called \emph{fixed}, or of \emph{Dirichlet} type. The second condition corresponds to vanishing force at the boundary, and is called \emph{free}, as in free of load, or of \emph{Neumann} type. 
\medskip \medskip 

\noindent
The question is, it is not immediately obvious to see why the energy balance should have the form \eqref{eq:EnBal}. The most straightforward way of deriving such energy balance is to start from the equation of motion, multiplied by the velocity, and to integrate over the string's length. Hence
\begin{equation}
\rho A \int_0^{\infty} \frac{\partial^2 y}{\partial t^2} \frac{\partial y}{\partial t} dx = T_0 \int_0^{\infty} \frac{\partial^2 y}{\partial x^2} \frac{\partial y}{\partial t} dx
\end{equation}
The left-hand side can be expressed as
\begin{equation}
\int_0^{\infty}\frac{\partial^2 y}{\partial t^2} \frac{\partial y}{\partial t} dx = \frac{1}{2}\frac{d}{dt}\int_0^{\infty} \left( \frac{\partial y}{\partial t}\right)^2 dx
\end{equation}
The right-hand side can be integrated by parts, and written as
\begin{equation}
\int_0^{\infty} \frac{\partial^2 y}{\partial x^2} \frac{\partial y}{\partial t} dx = - T_0 \frac{\partial y}{\partial x }\Bigg|_{x=0}\frac{\partial y}{\partial t }\Bigg|_{x=0} - \frac{1}{2}\frac{d}{dt}  \int_0^{\infty}\left( \frac{\partial y}{\partial x}\right)^2 dx
\end{equation}
Thus, we recover the energy balance \eqref{eq:EnBal} with energy components given by \eqref{eq:EnHalf}. 





\subsection{D'Alembert solution with prescribed boundary condition}


We now wish to describe the motion on the semi-infinite string using the D'Alembert solution. Consider first a boundary condition of \emph{fixed} type. Using this condition one gets
\begin{equation}
y(0,t) = f(-ct) + g(ct) = 0, \,\,\, \forall t
\end{equation}
This corresponds to setting $f(-ct) = -g(ct)$. An interpretation of this result is given in Figure \ref{fig:DalFixed}: essentially, the solution can be thought as the sum of the actual solution, defined for $x\geq 0$, plus a ``ghost'' solution defined over $x<0$. The ghost solution is just the flipped image of the right-travelling wave $f(x-ct)$. When the actual left-going solution enters the negative $x$ axis, the corresponding flipped ghost solution enters the positive $x$ axis. The net result is that the actual solution gets reflected at the boundary $x=0$ with a sign change. 
\begin{figure}[hbt]
\includegraphics[width=\linewidth,clip, trim={2cm 1cm 2cm 2cm}]{Figures/FixedBCTravel.png}
\caption{D'Alembert solution with fixed boundary at $x=0$. Thick black line is the actual solution for $x\geq 0$. The line with crosses (+) is the ghost solution defined for $x<0$.}\label{fig:DalFixed}
\end{figure}

\medskip \medskip 

\noindent Turning now the attention to the \emph{free} boundary condition, the mathematical condition here is
\begin{equation}
\frac{\partial y}{\partial x}(0,t)=-f^\prime(-ct)+g^\prime(ct) = 0 \,\,\, \forall t
\end{equation}
which can be integrated to give $f(-ct)=g(ct)$
\begin{figure}[hbt]
\includegraphics[width=\linewidth,clip, trim={2cm 1cm 2cm 2cm}]{Figures/FreeBCTravel.png}
\caption{D'Alembert solution with free boundary at $x=0$. Thick black line is the actual solution for $x\geq 0$. The line with crosses (+) is the ghost solution defined for $x<0$.}\label{fig:DalFree}
\end{figure}
This is sketched in Figure \ref{fig:DalFree}. Here the ghost solution has the same sign as the right-travelling wave, so that at the boundary $x=0$ the slope of the actual solution is zero (there is a stationary point there.)


\subsection{Fourier solution with prescribed boundary condition}

Suppose now that we wish to describe the solution in terms of Fourier components (i.e. sine and cosine functions.) In analogy to the case developed for the initial conditions, we are now taking a Fourier transform in the $\omega$ domain, so that the solution may be represented as
\begin{equation}\label{eq:ytt}
y(x,t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat y(x,\omega) e^{j\omega t} d\omega
\end{equation}
where the Fourier transform $\hat y(x,\omega)$ is defined as
\begin{equation}\label{eq:yhattOm}
\hat y(x,\omega) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}  y(x,t) e^{-j\omega t} d t
\end{equation}
(Note that, with a slight abuse of notation, we have identified with the symbol $\hat y$ both the Fourier transform in the $\gamma$ domain, in \eqref{eq:yhat}, and the Fourier transform in the $\omega$ domain in \eqref{eq:yhattOm}. These are of course distinct functions.) Using the transformed wave equation, we get
\begin{equation}
-\omega^2 \hat y(x,\omega) = c^2 \frac{\partial^2 \hat y(x,\omega)}{\partial x^2}
\end{equation}
which is solved, as per usual, by
\begin{equation}\label{eq:bcss}
\hat y(x,\omega) = B_+(\omega) e^{j\frac{\omega}{c}x} + B_-(\omega) e^{-j\frac{\omega}{c}x}
\end{equation}
Using the fixed boundary condition at $x=0$ gives $B_+ = -B_-$ and thus
\begin{equation}\label{eq:FixedFou1}
\hat y(x,\omega) = 2jB_+(\omega) \sin \frac{\omega x}{c} 
\end{equation}
The solution is then given by using this expression in \eqref{eq:ytt}
\begin{equation}
y(x,t)=\int_{-\infty}^{\infty} C_+(\omega) \sin\frac{\omega x}{c} e^{j\omega t} d\omega
\end{equation}
where we defined $C_+ = 2 j B_+/\sqrt{2\pi}$. Notice that, indeed, we could have taken a Fourier transform in \eqref{eq:ytt} using $e^{-j \omega t}$ instead of $e^{j \omega t}$, so that effectively a more general solution is given by
\begin{equation}\label{eq:FixedFou}
y(x,t)=\int_{-\infty}^{\infty} C_+(\omega) \sin\frac{\omega x}{c} e^{j\omega t} d\omega +  \int_{-\infty}^{\infty} C_-(\omega) \sin\frac{\omega x}{c} e^{-j\omega t} d\omega
\end{equation}
where $C_+,C_-$ may be determined from the two initial conditions. 


\medskip
\medskip


\noindent Using instead the free boundary condition in \eqref{eq:bcss}, we get $B_+ = B_-$ and
\begin{equation}\label{eq:FreeFou}
\hat y(x,\omega) = 2 B_+(\omega) \cos \frac{\omega x}{c}
\end{equation}
and thus, the general solution is
\begin{equation}\label{eq:FreeFou}
y(x,t)= \int_{-\infty}^{\infty} D_+(\omega) \cos\frac{\omega x}{c} e^{j\omega t} d\omega +  \int_{-\infty}^{\infty} D_-(\omega) \cos\frac{\omega x}{c} e^{-j\omega t} d\omega
\end{equation}
where $D_+,D_-$ are again determined by the initial conditions.
These meaning of \eqref{eq:FixedFou} is that, no matter the time evolution of the solution, the spatial distribution is such that the solution can be written as a linear combination of sine functions (that are always zero at the origin.) On the other hand, a free condition is such that the solution can be expressed as a linear combination of cosine functions, that have zero slope at the origin, as in \eqref{eq:FreeFou}.


\section{Strings of finite length}

The analysis carried out so far is very useful, as we are now ready to analyse the case of a string of finite length. A good place to start, once more, is the energy balance for the string, now defined for $0\leq x \leq L$. The energy balance is
\begin{equation}\label{eq:EnBalFull}
\frac{dE}{dt} = T_0 \frac{\partial y}{\partial x }\Bigg|_{x=L}\frac{\partial y}{\partial t }\Bigg|_{x=L} - T_0 \frac{\partial y}{\partial x }\Bigg|_{x=0}\frac{\partial y}{\partial t }\Bigg|_{x=0}
\end{equation}
The origin of this expression should now be clear to you. The idea is to perform the same mathematical steps that lead to \eqref{eq:EnBal}, but where now the limits of integration are set between $0$ and $L$. Hence
\begin{equation}\label{eq:EnFinite}
E = E_k + E_p = \int_{0}^{L} \frac{\rho A}{2}\left( \frac{\partial y}{\partial t} \right)^2 dx + \int_{0}^{L} \frac{T_0}{2}\left( \frac{\partial y}{\partial x} \right)^2 dx
\end{equation}
In order to study the motion of strings of finite length, we may again adopt either the D'Alembert or the Fourier solution. Notice that the boundary conditions are exactly symmetric at the two string's ends. So, the same interpretation of a fixed or a free boundary can be given at $x=L$. This idea of deriving boundary conditions from energy analysis is indeed very powerful, and will be used a lot during this course.


\subsection{Boundedness of the solutions}\label{sec:BoundWECnt}

Given the energy balance \eqref{eq:EnFinite}, it is immediate to see that
\begin{equation}
\int_0^L \left( \frac{\partial y}{\partial t} \right)^2 \, \mathrm{d}x \leq \frac{2E}{\rho A}, \quad \int_0^L \left( \frac{\partial y}{\partial x} \right)^2 \, \mathrm{d}x \leq \frac{2E}{T_0},
\end{equation}
expressing the bounds on the norms of the velocity and the gradient. These are generalisations to the continuous case of the bounds found for the oscillator in Section \ref{sec:BoundsSHO}. 


\subsection{D'Alembert solution for strings of finite length}
Given this interpretation for the boundary conditions, the D'Alembert solution should be pretty obvious to you now: at either end of the string, the right/left travelling wave front gets reflected exactly, with a sign change for a fixed boundary, and without a sign change for a free boundary. The left-going and right-going wavefronts at the time $t=0$ may be determined using \eqref{eq:Tr}. There is nothing complicated here. This solution is indeed powerful, both because of its simplicity, and because of the ease of implementation in the digital domain via a digital waveguide appraoch. However, it has the drawback of not being generalisable to other systems (outside of cylindrical air columns).


\subsection{Fourier solution for strings of finite length}

On the other hand, the Fourier approach has the advantdge of being, to some extent, universal: you may indeed apply this method to any linear, time-invariant system, in one or more dimenensions. The principle of application of this method has already been illustrated in the previous examples of strings of infinite length. Suppose now that the string is fixed at both ends, so that
\begin{equation}
y(0,t) = y(L,t) = 0
\end{equation}
We have already derived the expression for the solution with a fixed boundary at $x=0$. This has led to \eqref{eq:FixedFou1}. Now, from \eqref{eq:FixedFou1}, given that the string is fixed at $x=L$, we should also have
\begin{equation}\label{eq:omeM}
y(L,t) = 0 \implies \sin \frac{\omega L}{c} = 0 \implies \omega = \omega_m = \frac{\pi m c}{L}, \,\,\, m \in \mathbb{N}
\end{equation}
This is a very important result: what \eqref{eq:omeM} is saying is that only certain vibrational frequencies are now allowed, those that multiples of the fundamental frequency $\omega_1 = \pi c / L$. In other words, the effect of finite string length is to select only a discrete set of frequencies $\{ \omega_m \}$, whilst in strings of infinite length sinusoidal components of any frequency may be found. This has a consequence on how we express the general solution \eqref{eq:FixedFou}. Instead of integrals, now we have
\begin{equation}\label{eq:modal}
y(x,t) = \sum_{m=1}^{\infty } \left( C_+^{(m)} \sin \frac{\omega_m x}{c} e^{j \omega_m t} +  C_-^{(m)} \sin \frac{\omega_m x}{c} e^{-j \omega_m t} \right)
\end{equation}
where $C_+^{(m)},C_-^{(m)}$ are determined from the initial conditions. We call the expansion \eqref{eq:modal} a \emph{modal} expansion. The modes of the system are defined as a set comprising both the \emph{modal frequencies} $\{ \omega_m \}$ and the \emph{modal shapes} $\{\sin \frac{\omega_m x}{c}\}$.

\medskip \medskip 

\noindent
The form of \eqref{eq:modal} should be familiar to you: consider one point along the string, say $x_p$. Then
\begin{equation}\label{eq:Modes}
y(x_p,t) = \sum_{m=1}^{\infty } \left( \bar C_+^{(m,x_p)} e^{j \omega_m t} +  \bar C_-^{(m,x_p)}  e^{-j \omega_m t} \right)
\end{equation}
with $\bar C_+^{(m,x_p)} = C_+^{(m)} \sin \frac{\omega_m x_p}{c}$, $\bar C_-^{(m,x_p)} = C_-^{(m)} \sin \frac{\omega_m x_p}{c}$. But \eqref{eq:Modes} is just the sum of harmonic oscillators, whose amplitude is proportional to the value of the modal shape at the selected point. Hence, the vibration at each point alont the string can be thought of as resulting from the sum of harmonic oscillators, of increasing frequency. The oscillators are ``transparent'' to each other, meaning that the vibration of each oscillator is not influenced by what any other oscillator is doing. A picture of the modal shapes is given in Figure \ref{fig:fixfix}.
\begin{figure}[hbt]
\includegraphics[width = \linewidth]{Figures/FixedFixedModes.png}
\caption{Modes of a fixed-fixed string. Here, the length of the string is $L =1$ m, and the modes have been normalised so to have a maximum amplitude of 1 mm.}\label{fig:fixfix}
\end{figure}

\medskip \medskip 

\noindent
If we decide to impose different boundary conditions at either end point, we end up with different expressions for the modal shapes. Consider for instance the case of a \emph{free-free} string. We already know that a free end at $x=0$ on a semi-infite string results in a general solution of the kind \eqref{eq:FreeFou}. We now bring in a second boundary point at $x=L$, with the requirement that $\partial y / \partial x (L,t) = 0$. This condition gives
\begin{equation}
\frac{\partial}{\partial x}\cos\frac{\omega x}{c}\Bigg|_{x=L} = - \frac{\omega}{c} \sin \frac{\omega L}{c} = 0 \implies \omega = \omega_m = \frac{\pi m c}{L}, \,\,\, m \in \mathbb{N}_0
\end{equation}
These are the same frequencies that we obtained for the fixed-fixed case, with the small difference that here the solution for $m=0$ (zero frequency) gives rise to a modal shape that is \emph{not} identically zero: this special mode is a free-body mode, in which the string does not oscillate but ``floats'' is space. (This is physically meaningful, as the string is not attached at the ends and is therefore able to float. In reality, though, this kind of boundary condition cannot be realised, as the tension in the string arises from being attached at both boundaries. If either one of the boundaries is not fixed, then no tension is applied.) A picture of the modal shapes is given in Figure \ref{fig:freefree}.


\medskip \medskip 

\noindent
Another case of interest is the fixed-free case. Again, we should start from the expression of the semi-infinite string with a fixed boundary at $x=0$, \eqref{eq:FixedFou}. Now, the second condition to impose is $\partial y / \partial x (L,0) = 0$, which is
\begin{equation}
\frac{\partial}{\partial x}\sin\frac{\omega x}{c}\Bigg|_{x=L} =  \frac{\omega}{c} \cos \frac{\omega L}{c} = 0 \implies \omega = \omega_m = \frac{\pi (2m-1) c}{2L}, \,\,\, m \in \mathbb{N}
\end{equation}
The frequencies are now odd multiples of the fundamental frequency $\omega_1 = \pi c / 2 L$, which is one octave lower than the fundamental frequencies of the fixed-fixed and free-free cases. A picture of the modal shapes is given in Figure \ref{fig:fixfree}.


\begin{figure}[hbt]
\includegraphics[width = \linewidth]{Figures/FreeFreeModes.png}
\caption{Modes of a free-free string. Here, the length of the string is $L =1$ m, and the modes have been normalised so to have a maximum amplitude of 1 mm. Note that the first mode is a rigid body mode at zero frequency.}\label{fig:freefree}
\end{figure}

\medskip \medskip 

\noindent
The last case of interest, the free-fixed case, is left to you as an exercise. The interpretation of the modal expansion is that, for \emph{any} initial conditions, the final solution can be expressed as a combination of standing waves, of shape and frequency determined by the boundary conditions. Of course, as we have discussed at some length previously, the net result of such combination of standing modes need not be a standing wave: in fact, it will almost always be a travelling wavefront. 


\begin{figure}[hbt]
\includegraphics[width = \linewidth]{Figures/FixedFreeModes.png}
\caption{Modes of a fixed-free string. Here, the length of the string is $L =1$ m, and the modes have been normalised so to have a maximum amplitude of 1 mm.}\label{fig:fixfree}
\end{figure}



\subsection{Boundary condition with nonzero finite impedance}

\begin{figure}
\includegraphics[width=\linewidth]{Figures/boundary.pdf}
\caption{Mass-spring-dashpot boundary condition. The string exerts a force on the mass, $f(t) = -T_0 \partial y / \partial x (L,t)$. The equation of motion of the mass is therefore $d^2 z / dt^2 = -kz -R dz/dt + f(t)$. Here, $z(t) = y(L,t)$.}\label{fig:impBc}
\end{figure}

The boundary conditions given above are a useful idealisation. The study conducted so far has allowed us to understand the general physical properties of ideal strings. Of course, real musical strings do not behave in this idealised manner. More realistic string models will be taken into account later in the course, when  we include stiffness effects (i.e. when we consider the effects of finite string thickness.) Here, we want to analyse the effect of introducing more realistic boundary conditions at one of the string's end. Let us consider a fixed-free string and suppose that, at the right boundary, we attach some kind of bridge. A useful model for a bridge is that of a mass-spring system: in practice, the bridge is characterised by a mass $m$, a stiffness $k$, and a loss parameter $R$. This kind of boundary condition is sketched in Figure \ref{fig:impBc}.




\medskip \medskip 
\noindent 
We want to set up the equation of motion of the mass-spring system. Of course, this is in the form of an oscillator equation, i.e. 
\begin{equation}\label{eq:ImpBSc}
m \frac{d^2 z}{dt^2} = - k z - R \frac{d z}{dt} + f(t)
\end{equation}
Here, the displacement $z(t)$ of the bridge is equal to the displacement of the string's end, so that $z(t) = y(L,t)$ (the bridge is attached to the string at all times!)
The external force $f(t)$ is the force exterted by the string's end onto the mass. What is this force? We know this expression from the energy balance \eqref{eq:EnBalFull}: 
\begin{equation}\label{eq:ft}
f(t) = - T_0 \frac{\partial y}{\partial x}(L,t)
\end{equation}



Notice that the minus sign is here due to the fact that this is the force of that the string exerts \emph{onto} the mass. In practice, the boundary displacement $z(t)$ now satisfies the differential equation \eqref{eq:ImpBSc}, and is not simply prescribed. The question is whether the string plus mass-spring system satisfies an energy balance. This is, if course, the case. Considering that the string is fixed at $x=0$, the boundary term at $x=0$ vanishes in \eqref{eq:EnBalFull}. All we need to do is to check what happens at $x=L$. To that extent, take a product of \eqref{eq:ImpBSc} with $dz/dt$. This gives
\begin{equation}
m \frac{d^2 z}{dt^2}\frac{d z}{dt} = - k z\frac{d z}{dt} - R \left(\frac{d z}{dt}\right)^2 + f(t)\frac{d z}{dt}
\end{equation}
Using the usual energy identities, we get
\begin{equation}
\frac{d}{dt}\left( \underbrace{\frac{m}{2} \left(\frac{d z}{dt}\right)^2}_{e_k} + \underbrace{\frac{k z^2}{2}}_{e_p} \right) = - R \left(\frac{d z}{dt}\right)^2 + f(t)\frac{d z}{dt}
\end{equation}
We are now taking the sum of \eqref{eq:EnBalImp} with  \eqref{eq:EnBalFull}, to get
\begin{equation}
\frac{dE}{dt} = \frac{d}{dt}\left(E_k + E_p + e_k + e_p \right) = - R \left(\frac{d z}{dt}\right)^2 + f(t)\frac{d z}{dt} + T_0 \frac{\partial y}{\partial x}\Bigg|_{x=L} \frac{\partial y}{\partial t} \Bigg|_{x=L}
\end{equation}
Of course, remembering the definitions of $f(t)$ in \eqref{eq:ft} and of $z(t)$, one has 
\begin{equation}
f(t)\frac{d z}{dt} + T_0 \frac{\partial y}{\partial x}\Bigg|_{x=L} \frac{\partial y}{\partial t} \Bigg|_{x=L} = 0
\end{equation}
and thus, the energy balance for this kind of boundary is
\begin{equation}\label{eq:EnBalImp}
\frac{dE}{dt} = - R \left(\frac{d z}{dt}\right)^2 \leq 0
\end{equation}
This is a comforting (and expected!) result. The energy in the system is decreasing because of the loss at the boundary, and is perfectly conserved if $R=0$ (conservative boundary.) The added mass and spring are energy-storing devices, and therefore the total energy now includes the corresponding kinetic and potential energies. A boundary condition such as \eqref{eq:ImpBSc} has a nonzero, finite impedance, as opposed to the fixed condition (infinite impedance), or the free condition (zero impedance). We can now analyse the effect of various cases.


\begin{figure}[hbt]
\includegraphics[width=\linewidth,clip,trim={2cm 0cm 2cm 0cm}]{Figures/tanSol.png}
\caption{Graphical solution of trascendental frequency equations. Left: \eqref{eq:bc1}. Right: \eqref{eq:bc2}}\label{fig:trasc}
\end{figure}

\medskip \medskip 
\noindent 
Case a): $m=0,R=0$. Here, we are assuming that the mass and loss parameter of the bridge are very small compared to the stiffness. The boundary condition \eqref{eq:ImpBSc} in this case is
\begin{equation}\label{eq:imp1}
k z = f(t)
\end{equation}
We want to determine the modal frequencies and modal shapes of the string under this condition. Remembering that $z(t) = y(L,t)$, and remembering that the modal shapes for fixed boundary at $x=0$ are in the form of $\sin \frac{\omega x}{c}$, condition \eqref{eq:imp1} gives
\begin{equation}
k y(L,t) = -T_0 \frac{\partial y}{\partial x}(L,0) \implies k \sin \frac{\omega L}{c} = - T_0 \frac{\omega}{c} \cos \frac{\omega L}{c}
\end{equation}
First, two sanity checks. It is easy to see that, for $k=0$, we get back to the fixed-free case ($\cos \frac{\omega L}{c} = 0$) analysed before. Also, if $k\rightarrow \infty$, we obtain again the fixed-fixed condition ($\sin \frac{\omega L}{c} = 0$). If $k \neq 0$, the boundary condition gives
\begin{equation}\label{eq:bc1}
\tan \frac{\omega L}{c} = - \frac{T_0}{ck}\omega
\end{equation}
This is a trascendental equation, that can be solved using numerical root-finding algorithms. A picture of the solution to this equation is given in Figure \ref{fig:trasc}. Notice that, when $\omega \rightarrow \infty$, the equation reduces to $\tan\frac{\omega L}{c}\rightarrow \infty$ which is solved by $\omega_m = (2m-1)\pi c / 2L$, i.e. the same frequencies as the fixed-free case. In practice, at high frequencies the effects of stiffness are negligible compared to the case of free boundary. The most affected modes are at low frequencies!


\medskip \medskip 
\noindent 
Case b): $k=0,R=0$. A second case of interest is given by a bridge where the mass is much larger than the stiffness and loss parameter. The boundary condition \eqref{eq:ImpBSc} in this case is
\begin{equation}
m \frac{d^2z}{dt^2} = f(t)
\end{equation}
Thus,
\begin{equation}
-m \omega^2 y(L,t) = -T_0 \frac{\partial y}{\partial x}(L,0) \implies m  \sin \frac{\omega L}{c} =   \frac{T_0}{c\omega} \cos \frac{\omega L}{c}
\end{equation}
Another sanity check here indicates that, when $m=0$ we obtain again the solution to the fixed-free boundary; when $m\rightarrow \infty$ we obtain again the frequencies of the fixed-fixed boundary (as expected! If the mass is really small / large, the boundary is effectively free / fixed.) For all other cases, one has
\begin{equation}\label{eq:bc2}
\tan \frac{\omega L}{c} = \frac{T_0}{c m}\omega^{-1}
\end{equation}
which is another trascendental equation solvable via numerical root-finding procedures. Notice again that, for large frequencies, we obtain  $\tan\frac{\omega L}{c}= 0$ which is solved by $\omega_m = m\pi c / L$: at high frequencies the bridge appears rigid! See also Figure \ref{fig:trasc}.


\section{Viscous Loss}


Conservative wave propagation is in practice never realised, at least over time scales of the order of seconds (i.e. the timescale of a musical signal, for example.) Thus, we are now going to study the effects that losses have on wave propagation. In analogy with the oscillator, we may introduce a loss factor propotional to the velocity of the string. Hence
\begin{equation}\label{eq:WEloss}
\frac{\partial^2 y}{\partial t^2} = c^2 \frac{\partial^2 y}{\partial x^2} - 2\sigma   \frac{\partial y}{\partial t}
\end{equation}
Note that the energy analysis here leads to
\begin{equation}\label{eq:EnBalWE}
\frac{d}{dt}\left( \int_{0}^{L} \frac{1}{2}\left( \frac{\partial y}{\partial t} \right)^2 dx + \int_{0}^{L} \frac{c^2}{2}\left( \frac{\partial y}{\partial x} \right)^2 dx \right) = -2\sigma \int_0^L \left(\frac{\partial y}{\partial t}\right)^2 dx,
\end{equation}
and hence energy is not increasing whenever $\sigma \geq 0$, which will be assumed in the remainder. 

A first thing to notice here, is that the D'Alembert solution ceases to be valid when $\sigma  \neq 0$. Thus, we cannot use such solution in the current analysis. The other tools that we developed previously though (separation of variables, analysis in the Fourier domain) are still valid, and we are going to make use of them now. What we could do, is to try to separate the solution into components depending only on time and space, i.e.
\begin{equation}
y(x,t) = T(t)X(x)
\end{equation}
We could then use this expression into \eqref{eq:WEloss}, and follow the same steps as we did for the wave equation without loss, in last week's lecture. The net result is that we end up with
\begin{equation}
y(x,t) = T(t)X(x) = \left(A_+ e^{st} + A_- e^{-st} \right)\left(B_+ e^{j\gamma x} + B_- e^{-j \gamma x} \right)
\end{equation}
where the four complex constants $A_+,A_-,B_+,B_-$ are arbitrary so long as initial and boundary conditions are not specified. Above, $s = -\sigma + j \omega$. In practice, the solution is a combination of typical terms such as
\begin{equation}\label{eq:ddp}
y(x,t) = e^{st}e^{j\gamma x} 
\end{equation}
In actual fact, when analysing PDEs such as \eqref{eq:WEloss}, we may immediately try to use solutions such as \eqref{eq:ddp}. We make an \emph{assumption} that solutions have that form, and then characterise the solutions according to the particular relationship between the frequency $\omega$ and wavenumber $\gamma$. Such assumption in textbooks is often called an \emph{ansatz} (a german word.)

\medskip \medskip 
\noindent
The use of solutions such as \eqref{eq:ddp} in PDEs is one of the tools you should understand and master in this course. The reason why we want to use such expression for the solution, as noted, is to find a relation between $\omega$ and $\gamma$ known as the \emph{dispersion relation}. Doing so for \eqref{eq:WEloss} gives
\begin{equation}
s^2 + 2 \sigma s + c^2 \gamma^2 = 0 \implies s = - \sigma \pm j \sqrt{c^2\gamma^2 - \sigma^2}
\end{equation}
Thus, the dispersion relation is
\begin{equation}\label{eq:dispRelLoss}
\omega = \sqrt{c^2\gamma^2 - \sigma^2}
\end{equation}
Of course, when $\sigma=0$ one recovers the dispersion of the simple wave equation $\omega = c\gamma$. The interesting fact about \eqref{eq:dispRelLoss} is that now the system is \emph{dispersive}. Dispersion is a property of vibratory system in which waves at different frequencies travel at different speeds. The wave equation without loss is \emph{not} dispersive, because all waves travel at the same speed $c$. The wave equation with loss, on the other hand, has dispersion. Consider a simple harmonic solution
\begin{equation}
y(x,t) = \cos(\omega t - \gamma x) 
\end{equation}
Using the definition of \emph{phase velocity}  $v_\phi$ (encountered in last week's lecture), one has
\begin{equation}
v_\phi = \frac{\omega}{\gamma} = \frac{\sqrt{c^2 \gamma^2 - \sigma^2}}{\gamma} \approx c - \frac{\sigma^2}{2c\gamma^2} = c \left(1 - \frac{\sigma^2}{2(\omega^2+\sigma^2)} \right)
\end{equation}
and thus we see that the phase velocity now depends on the frequency of the wave! In particular, waves of high frequency are non-dispersive, as the limit of the dispersion relation as $\omega \rightarrow \infty$ is $c$.


\section{Dispersion. Group Velocity}

The concept of dispersion is fairly important for our study on wave propagation, because it brings about the question of what we mean by \emph{wave velocity}. For a non-dispersive system such as the simple lossless wave equation, the answer is trivial as all wavefronts travel with the same speed $c$. This allows to write solutions in the form given by D'Alembert. However, the introduction of a viscous loss term has the consequence of introducing a frequency-dependent phase velocity. So, what is the speed of a wave? 

\medskip \medskip 
\noindent
It is convenient, in dispersive systems, to not consider sines or cosines in isolation, but to consider \emph{groups} of them. This is because, from Fourier theory, a disturbance may always be decomposed onto a linear combination of sines and cosines. Thus, let such disturbance be
\begin{equation}\label{eq:gvv}
y(x,t) = \sum_{i=1}^N \cos(\omega_i t - \gamma_i x + \phi_i)
\end{equation}
(here, for simplicity, we are considering a discrete sum, though we may replace the sum with a Fourier integral as seen before.) The form of $y(x,t)$ is that of a group of waves. We are also making the assumption that such group is centered around some reference frequency $\omega_0$ and wavenumber $\gamma_0$, so that effectively
\begin{equation}
\omega_i = \omega_0 + \delta \omega_i, \quad \gamma_i = \gamma_0 + \delta \gamma_i, \quad i \in [1,N]
\end{equation} 
with $\delta \omega_i$,  $\delta \gamma_i$ being small. Thus, the change in phase $dP_i$ for a small displacement $dx$ in the time $dt$ for each of the waves is given by
\begin{equation}
dP_i = \omega_i dt - \gamma_i dx
\end{equation}
Now, this is the very important point: if the group moves ``without changing in shape too much'' then \emph{all} the phases in the sum \eqref{eq:gvv} have changed by the same amount. Thus we set $dP_i - dP_j \approx 0$, or
\begin{equation}
(\omega_i-\omega_j)dt - (\gamma_i-\gamma_j) \approx 0 \implies \frac{dx}{dt} \approx \frac{\delta \omega}{\delta \gamma}
\end{equation}
Thus, is convient to define the \emph{group velocity} 
\begin{equation}
v_{g} = \frac{d \omega }{d \gamma} 
\end{equation}
that has the interpretation of the velocity of a group of waves, centered around some mean values $\omega_0$, $\gamma_0$, that travel with mostly the same speed, so that a packet of such waves retains its shape whilst travelling through the medium.



\chapter{Numerical methods for the wave equation}


The previous chapter has introduced the formalism of the one-dimensional wave equation. In this chapter we explore the numerical methods that can be employed to solve it. While exact solutions are available in this case, these do not generalise other systems. In fact, the exact solutions serve here as a useful benchmark against which to test the numerical schemes presented. We begin the discussion of the numerical mehtods illustrating the use of finite differences. 


\section{Spatial difference operators}


Just like time can be discretised by means of a sample rate (or equivalently, a time step), the spatial domain may also be discretised using an appropriate \emph{grid spacing}. Let such spacing be uniform along the domain of interest, so that the continuous domain $\mathcal{D} = \{ x : x \in [0,L]\}$ is mapped onto the discrete domain $\mathfrak{d} = \{ mh : 0 \leq m \leq M = L/h, m \in {\mathbb N}\}$. In the same way as, in Chapter \ref{chap:Intro}, we approximated the continuous time function $x(t)$ via the discrete time series $x^n$, here we are going to approximate the continuous function $y(x)$ via the \emph{grid function} $y_m$. The identity, forward and backward shift operators are then given as
\begin{equation}
    {1}{y}_m = {y}_m, \quad \esp {y}_m = {y}_{m+1}, \quad \esm {y}_m = {y}_{m-1}.
\end{equation}
From these, one may define the difference operators, all approximating the first spatial derivative, as
\begin{subequations}
    \begin{align}
        \dsp & = \frac{(\esp - 1)}{h} \approx \frac{d}{dx},     \\ 
        \dsm & = \frac{(1 - \esm)}{h}\approx \frac{d}{dx},      \\ 
        \dsd & = \frac{(\esp - \esm)}{2h}\approx \frac{d}{dx} . 
    \end{align}
\end{subequations}
An approximation to the second time derivative is constructed from the above as
\begin{equation}
    \dss  = \dsp\dsm\approx \frac{d^2}{dx^2}.
\end{equation}
Averaging operators (all approximating the identity) are
\begin{subequations}
    \begin{align}
        \mu_{x+} & = \frac{(\esp + 1)}{2} \approx 1,    \\ 
        \mu_{x-} & = \frac{(1 + \esm)}{2} \approx 1,    \\ 
        \mu_{x\cdot}& = \frac{(\esp + \esm)}{2} \approx 1. 
    \end{align}
\end{subequations}
Taylor series arguments can be used to infer the order of the approximation. The truncation errors of the difference operators may again be found by applying them to the continuous function $y(x)$, and expanding in a Taylor series. Hence
\begin{subequations}\label{eq:ErrsSpatial}
\begin{align}\
     \frac{dy(x_m)}{dx} - \dtp y(x_m) &= O(h), \\
     \frac{dy(x_m)}{dx} - \dtm y(x_m) &= O(h), \\   
     \frac{dy(x_m)}{dx} - \dtd y(x_m) &= O(h^2),\\   
     \frac{d^2y(x_m)}{dx^2} - \dtt y(x_m) &= O(h^2),
\end{align}
\end{subequations}
and so on. 



\subsection{Frequency-domain intepretation of spatial difference operators}\label{sec:FDtransformationsSpatial}

In Section \ref{sec:FDtransformations}, the temporal difference operators were applied to the kernel of the Fourier transform. A similar idea can be employed for the spatial difference operators, where of course the Fourier transform is now
\begin{equation}
\mathcal{X}\left\{ y_m \right\} = \sum_{m=-\infty}^\infty y_m e^{-jmh\gamma},
\end{equation}
where $\gamma$ is the wavenumber. The spatial difference operators transform in a manner analougous to the temporal ones. Take the second difference:
\begin{equation}\label{eq:DTFTdss}
\mathcal{X}\left\{\dss y_m\right\} = \frac{e^{j\gamma h}-2+e^{-j\gamma h}}{h^2}\mathcal{X}\left\{y_m\right\}=\frac{2}{h^2}\left(\cos(\gamma k)-1 \right)\mathcal{X}\left\{y_m\right\} = -\frac{4}{h^2} \sin^2\left(\frac{\gamma h}{2}\right)\mathcal{X}\left\{y_m\right\}
\end{equation}
Analogously, one has
\begin{subequations}
\begin{align}
\mathcal{X}\left\{\mu_{x\cdot} y_m\right\} &= \cos(\omega k) \mathcal{X}\left\{y_m \right\}, \\
\mathcal{X}\left\{\mu_{xx} y_m\right\} &= \frac{1}{2}\left(\cos(\omega k) + 1\right)\mathcal{X}\left\{y_m \right\} \\
\mathcal{X}\left\{\dsd y_m\right\} &= \frac{j}{k}\sin(\omega k) \mathcal{X}\left\{y_m \right\}
\end{align}
\end{subequations}
These identities will prove useful in the study of the stability of the wave equation, as will be seen shortly. 





\subsection{Matrix form of difference operators}


Since $y_m$ is interpreted as a grid function, it is natural to think of the spatial difference operators as matrices acting on the vector ${\bf y} \in \mathbb{R}^{M+1}$. Consider the operator $\dsp$. For interior points, one has $\dsp y_m = \left({\bf D}^+ {\bf y}\right)_m = \frac{y_{m+1}-y_m}{h}$. This is
\begin{equation}
{\bf D}^+ {\bf y} = \frac{1}{h}\begin{bmatrix}
& & {\bf \ddots}  & \ddots & & & \\ 
& & & {\bf -1} & 1 & & &\\
& & & & {\bf -1} & 1 & & \\
& & & & & \ddots & \ddots & \\
\end{bmatrix}
\begin{bmatrix}
\vdots \\
y_m \\
y_{m+1}\\
\vdots
\end{bmatrix}.
\end{equation}
Here, the boldface indicates the elements on the main diagonal. Analogously, one can define matrices for the operator $\dsm y_m = \left({\bf D}^- {\bf y}\right)_m = \frac{y_{m}-y_{m-1}}{h}$
\begin{equation}
{\bf D}^- {\bf y} = \frac{1}{h}\begin{bmatrix}
& & \ddots  & \ddots & & & \\ 
& & & -1 & {\bf 1} & & &\\
& & & & -1 & {\bf 1} & & \\
& & & & & \ddots & \ddots & \\
\end{bmatrix}
\begin{bmatrix}
\vdots \\
y_{m-1}\\
y_{m}\\
\vdots
\end{bmatrix}.
\end{equation}
The centered derivative is given by $\dsd y_m = \left({\bf D} {\bf y}\right)_m = \frac{y_{m+1} - y_{m-1}}{2h}$, or
\begin{equation}
{\bf D} {\bf y} = \frac{1}{2 h}\begin{bmatrix}
& & \ddots  & \ddots & \ddots & & & &\\ 
& & & -1 & {\bf 0} & 1 & & & &\\
& & & & -1 & {\bf 0 } & 1 &  & &\\
& & & & & -1 & {\bf 0 } & 1 &  & \\
& & & & & & \ddots & \ddots &  \ddots & \\
\end{bmatrix}
\begin{bmatrix}
\vdots \\
y_{m-1}\\
y_{m}\\
y_{m+1} \\
\vdots
\end{bmatrix}.
\end{equation}
The second derivative is given by $\dss y_m = \left({\bf D}^2 {\bf y}\right)_m = \frac{y_{m+1} -2y_{m} + y_{m-1}}{h^2}$, so that
\begin{equation}
{\bf D}^2 {\bf y} = \frac{1}{h^2}\begin{bmatrix}
& & \ddots  & \ddots & \ddots & & & &\\ 
& & & 1 & {\bf -2} & 1 & & & &\\
& & & & 1 & {\bf -2 } & 1 &  & &\\
& & & & & 1 & {\bf -2 } & 1 &  & \\
& & & & & & \ddots & \ddots &  \ddots & \\
\end{bmatrix}
\begin{bmatrix}
\vdots \\
y_{m-1}\\
y_{m}\\
y_{m+1} \\
\vdots
\end{bmatrix}.
\end{equation}
The averaging operators can be defined analogously. Denoting them by the letter ${\bf W}$, one has
\begin{subequations}
\begin{align}
{\bf W}^+  &= \frac{1}{2}\begin{bmatrix}
& & {\bf \ddots}  & \ddots & & & \\ 
& & & {\bf 1} & 1 & & &\\
& & & & {\bf 1} & 1 & & \\
& & & & & \ddots & \ddots & \\
\end{bmatrix}, \\
{\bf W}^-  &= \frac{1}{2}\begin{bmatrix}
& & \ddots  & \ddots & & & \\ 
& & & 1 & {\bf 1} & & &\\
& & & & 1 & {\bf 1} & & \\
& & & & & \ddots & \ddots & \\
\end{bmatrix}, \\
{\bf W} &= \frac{1}{2 }\begin{bmatrix}
& & \ddots  & \ddots & \ddots & & & &\\ 
& & & 1 & {\bf 0} & 1 & & & &\\
& & & & 1 & {\bf 0 } & 1 &  & &\\
& & & & & 1 & {\bf 0 } & 1 &  & \\
& & & & & & \ddots & \ddots &  \ddots & \\
\end{bmatrix}.
\end{align}
\end{subequations}


\subsection{Numerical Boundary Conditions}\label{sec:WEQNmatrices}


While the forms of the matrices above hold for central points, the points near the boundary deserve special treatment. One needs of course to discretise the prescribed boundary conditions. We are considering now the two types of boundary conditions encountered in Section \ref{sec:WaveEquationsBCs}: Dirichlet ($y=0$) and Neumann $\left(\frac{dy}{dx}=0\right)$. 

\subsubsection{Fixed boundary conditions}
For conditions of Dirichlet type, one can apply $y_0=y_M=0$. Clearly one need not store the end points, since these are identically zero. Then, one may define
\begin{equation}
{\bf D}^-_d  = \frac{1}{h}\begin{bmatrix}
{\bf 1} &  &   &   \\ 
-1& {\bf 1} & & &    \\
&-1 & {\bf 1} & &    \\
& & \ddots & \ddots &    \\
& &  & -1&   
\end{bmatrix},
\end{equation}
and note that this matrix is \emph{rectangular}, with dimensions $M \times M-1$. The subscript $d$ is for \emph{Dirichlet}. The elements on the main diagonal were again written in boldface. The matrix ${\bf D}^{+}$ can be defined simply as
\begin{equation}
{\bf D}^+_{d} = -({\bf D}^-_d)^\intercal
\end{equation}
or
\begin{equation}
{\bf D}^+_d  = \frac{1}{h}\begin{bmatrix}
{\bf -1} &  1 &   &   &\\ 
& {\bf -1} & 1 & &  &  \\
& & {\bf -1} & 1&   & \\
& & & \ddots &  \ddots & \\
& & &  & {\bf -1}&   1
\end{bmatrix},
\end{equation}
and is a rectangular matrix of dimensions $M-1 \times M$. The second difference operator is then given by composing the first difference matrices, as
\begin{equation}\label{eq:SecondDiffMat}
{\bf D}^2_d = {\bf D}^+_d \,\, {\bf D}^-_d
\end{equation}
Note that this composition is obvisously \emph{not} symmetric! That is, the product ${\bf D}^-_d \,\, {\bf D}^+_d$ is not the same as \eqref{eq:SecondDiffMat}, since the resulting matrix has different dimensions. The correct composition is given by \eqref{eq:SecondDiffMat}, since the resulting matrix is $M-1 \times M-1$. Explicitly
\begin{equation}\label{eq:D2Diri}
{\bf D}^2_d = \frac{1}{h^2}\begin{bmatrix}
{\bf -2}& 1&   &  &  &  \\ 
-1& {\bf -2} & 1 &  &  &\\
& \ddots & \ddots & \ddots &  &      \\
& &  1 & {\bf -2}& 1   \\
& &   & 1 & {\bf -2}   \\
\end{bmatrix}.
\end{equation}

\subsubsection{Free boundary conditions (Non-centred)}
Consider now an approximation to the Neumann condition as $\delta_{x+}y_0 = \delta_{x-}y_M=0$. This gives $y_0=y_1$ and $y_M=y_{M-1}$. Again, one need not store the end points $y_0$, $y_M$, since their value is the same as the inner points $y_1$, $y_{M-1}$. Then, one can again define rectangular matrices as the forward and backward difference operators. For the backward difference, take
\begin{equation}
{\bf D}^-_{n1}  = \frac{1}{h}\begin{bmatrix}
{\bf 0} &  &   &   \\ 
-1& {\bf 1} & & &    \\
&-1 & {\bf 1} & &    \\
& & \ddots & \ddots &    \\
& &  & 0&   
\end{bmatrix},
\end{equation}
This is a $M \times M-1$ matrix, whose first and last rows are filled with zeros. Again, boldaface symbols are on the main diagonal. The subscript ${n1}$ indicates \emph{first-order Neumann} conditions. Just like previously, the forward difference operator is obtained as
\begin{equation}
{\bf D}^+_{n1} = -({\bf D}^-_{n1})^\intercal,
\end{equation}
or
\begin{equation}
{\bf D}^+_{n1} = \frac{1}{h}\begin{bmatrix}
{\bf 0} &  1 &   &   &\\ 
& {\bf -1} & 1 & &  &  \\
& & {\bf -1} & 1&   & \\
& & & \ddots &  \ddots & \\
& & &  & {\bf -1}&   0
\end{bmatrix},
\end{equation}
and is a $M-1\times M$ matrix. The second difference operator is given by
\begin{equation}\label{eq:SecondDiffMatNeumann}
{\bf D}^2_{n1} = {\bf D}^+_{n1} \,\, {\bf D}^-_{n1},
\end{equation}
or
\begin{equation}\label{eq:D2Neum1}
{\bf D}^2_{n1} = \frac{1}{h^2}\begin{bmatrix}
{\bf -1}& 1&   &  &  &  \\ 
-1& {\bf -2} & 1 &  &  &\\
& \ddots & \ddots & \ddots &  &      \\
& &  1 & {\bf -2}& 1   \\
& &   & 1 & {\bf -1}   \\
\end{bmatrix},
\end{equation}
and is $M-1 \times M-1$.



\subsubsection{Free boundary conditions (Centred)}

A second-order accurate implementation of the Neumann condition results from the application of $\dsd y_0=\dsd y_M=0$. In this case, it results that $y_{-1}=y_1$, $y_{M-1}=y_{M+1}$, and hence, application of the second-difference operator at the boundary points results in
\begin{equation}\label{eq:D2Neum2}
{\bf D}^2_{n2} = \frac{1}{h^2}\begin{bmatrix}
{\bf -2}& 2&   &  &  &  \\ 
-1& {\bf -2} & 1 &  &  &\\
& \ddots & \ddots & \ddots &  &      \\
& &  1 & {\bf -2}& 1   \\
& &   & 2 & {\bf -2}   \\
\end{bmatrix},
\end{equation}
and the matrix is $M+1 \times M+1$. Note that this matrix is \emph{not} symmetric, and it is therefore not possible to write it as the product of a matrix times its transpose. Note, however, that the matrix can be symmetrised by applying a linear transformation in the form of a positive-definite diagonal matrix. To that end, consider the $M+1\times M+1$ diagonal matrix ${\bf T} = \text{diag}([\frac{1}{2},1,1,...,1,1,\frac{1}{2}])$. Then
\begin{equation}\label{eq:LinTransfn2}
{\bf T}\,{\bf D}^{2}_{n2} = \frac{1}{h^2}\begin{bmatrix}
{\bf -1}& 1&   &  &  &  \\ 
-1& {\bf -2} & 1 &  &  &\\
& \ddots & \ddots & \ddots &  &      \\
& &  1 & {\bf -2}& 1   \\
& &   & 1 & {\bf -1}   \\
\end{bmatrix},
\end{equation}
that is, \eqref{eq:D2Neum1}, except the dimensions of the matrix are here $M+1 \times M+1$.



\subsection{Eigenvalues and eigenvectors of second difference matrices}

The analysis is Section  \ref{sec:FDtransformationsSpatial} revealed that an eigenfunction of the $\dss$ operator is $e^{-jm h \gamma}$, with eigenvalue $-\frac{4}{h^2}\sin^2\left( \frac{\gamma h}{2}\right)$. Of course, a second eigenfunction is given by $e^{jm h \gamma}$, so that the eigenfunctions of the second difference matrix can  be written as
\begin{equation}
{\bf D}^2 \hat {\bf y} = -\frac{4}{h^2}\sin^2\left( \frac{\gamma h}{2}\right)\hat {\bf y},
\end{equation}
where $\hat {\bf y} = A_+ \hat {\bf y}^+ + A_- \hat {\bf y}^- $ and where $\hat { y}^+_m = e^{jmh\gamma}$, $\hat { y}^-_m = e^{-jmh\gamma}$. Here, ${\bf D}^2$ is either one of \eqref{eq:D2Diri}, \eqref{eq:D2Neum1} or \eqref{eq:D2Neum2}. In practice, the quantised wavenumbers $\gamma$ and the corresponding eigenvectors are obtained after application of the boundary conditions. 


\subsubsection{Eigenvalues of ${\bf D}^2_d$}

Application of ${\hat{y}}_{m=0}={\hat{y}}_{m=M}=0$ results in the following system of equations for $A_+$, $A_-$
\begin{equation}\label{eq:eigenD2d}
\begin{bmatrix}
1 & 1 \\
e^{jMh\gamma} & e^{-jMh\gamma}
\end{bmatrix}
\begin{bmatrix}
A_+ \\
A_-
\end{bmatrix}=
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\end{equation}
which has non-trivial solutions if and only if the determinant is set to zero, i.e. when
\begin{equation}\label{eq:waveNrD2d}
\sin ({Mh\gamma}) = 0  \implies  \gamma_p = \frac{p \pi}{M h}, \,\, p = 1,...,M-1.
\end{equation}
The eigenvectors are obtained by solving either one of \eqref{eq:eigenD2d}. This gives $A_+=-A_-$, and thus
\begin{equation}\label{eq:EigenVecsD2d}
{\hat y}^p_m = A_+ \sin \frac{mp\pi}{M },\,\, m = 1,..,M-1.
\end{equation}
In the above $\hat{ y}^p_m$ denotes the $m^{th}$ component of the $p^{th}$ eigenvector. The constant $A_+$ can be set to normalise the eigenvectors conveniently, as shown below. 
The eigenvectors can be ordered columnwise in a matrix,
\begin{equation}
{\bf Y}_d = \begin{bmatrix}\hat {\bf y}^1 & \hat{\bf y}^2 & ... & \hat{\bf y}^{M-1}\end{bmatrix},
\end{equation}
so that, from the spectral theorem, a decomposition of the second-difference matrix is obtained as
\begin{equation}\label{eq:DecompD2d}
{\bf D}^{2}_d = -{\bf Y}_d \, {\bf \Lambda}_d \, {\bf Y}^\intercal_d,
\end{equation}
where ${{\bf \Lambda}_d}$ is a diagonal matrix, whose diagonal elements $\lambda_p$ are  $\frac{4}{h^2}\sin^2 \left( \frac{\gamma_p h}{2}\right)$. Here, 
\begin{equation}\label{eq:BndLambdaD2d}
0 < \lambda_p < 4/h^2.
\end{equation}
Note that, by virtue of the bounds on the eigenvalues, this matrix is clearly positive-definite. 
When the eigenvectors are normalised using $A_+ = \sqrt{2/M}$, the matrix ${\bf Y}_d$ is orthonormal, and thus ${\bf Y}_d {\bf Y}_d^{\intercal}={\bf Y}_d^\intercal {\bf Y}_d = {\bf I}$.


\subsubsection{Eigenvalues of ${\bf D}^2_{n1}$}

Application of $\dsp \hat y_0 = \dsm \hat y_M = 0$ gives, after a little algebra, the following system of equations
\begin{equation}\label{eq:eigenD2n1}
\begin{bmatrix}
e^{jh\gamma} & -1 \\
e^{j(M-1)h\gamma} & -e^{-jMh\gamma}
\end{bmatrix}
\begin{bmatrix}
A_+ \\
A_-
\end{bmatrix}=
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\end{equation}
Setting the determinant to zero gives
\begin{equation}\label{eq:EigenvaluesD2n1}
\sin ({(M-1)h\gamma}) = 0  \implies  \gamma_p = \frac{p \pi}{(M-1) h}, \,\, p = 0,...,M-2.
\end{equation}
Note that the range for $p$ is now shifted, so to include the zero eigenvalue. This must be included: looking at \eqref{eq:D2Neum1}, the vector $\hat {\bf y} = [1,1,...,1]$ is necessarily an eigenvector, with eigenvalue equal to zero. The eigenvectors are obtanained solving either one of \eqref{eq:eigenD2n1}. This gives
\begin{equation}
\hat y^p_m=A_+\cos\frac{p \pi \left( m-\frac{1}{2}\right)}{M-1}, \,\, m = 1,..,M-1.
\end{equation}
As before, the eigenvectors can be arranged in columns, 
\begin{equation}
{\bf Y}_{n1} = \begin{bmatrix}\hat {\bf y}^1 & \hat{\bf y}^2 & ... & \hat{\bf y}^{M-1}\end{bmatrix},
\end{equation}
so that, from the spectral theorem, a decomposition of the second-difference matrix is obtained as
\begin{equation}\label{eq:DecompD2n1}
{\bf D}^{2}_{n1} = -{\bf Y}_{n1} \, {\bf \Lambda}_{n1} \, {\bf Y}^\intercal_{n1}.
\end{equation}
where ${{\bf \Lambda}_{n1}}$ is a diagonal matrix, whose diagonal elements $\lambda_p$ are  equal to $\frac{4}{h^2}\sin^2 \left( \frac{\gamma_p h}{2}\right)$. Here,
\begin{equation}\label{eq:BndLambdaD2n1}
0 \leq \lambda_p < 4/h^2.
\end{equation}
When the eigenvectors are normalised using $A_+ = \sqrt{2/(M-1)}$, the matrix ${\bf Y}_{n1}$ is orthonormal, and thus ${\bf Y}_{n1} {\bf Y}_{n1}^{\intercal}={\bf Y}_{n1}^\intercal {\bf Y}_{n1} = {\bf I}$.


\subsubsection{Eigenvalues of ${\bf D}^2_{n2}$}


Here, applying the boundary conditions $\dsd y_0 = \dsd y_M = 0$ gives the following set of equations
\begin{equation}\label{eq:eigenD2n2}
\begin{bmatrix}
1 & -1 \\
e^{jMh\gamma} & -e^{-jMh\gamma}
\end{bmatrix}
\begin{bmatrix}
A_+ \\
A_-
\end{bmatrix}=
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\end{equation}
and, upon setting the determinant to zero, the quantised wavenumbers are such that
\begin{equation}\label{eq:EigenvaluesD2n2}
\sin ({Mh\gamma}) = 0  \implies  \gamma_p = \frac{p \pi}{M h}, \,\, p = 0,...,M.
\end{equation}
These are the same as \eqref{eq:waveNrD2d}, except for the range of $p$, which again must include the zero eigenvalue since the vector $\hat {\bf y}=[1,1,..,1]^\intercal$ is necessarily an eigenvector. The eigenvectors are then given by solving either one of \eqref{eq:eigenD2n2}, yielding
\begin{equation}
\hat y^p_m=A_+\cos\frac{m p \pi}{M}, \,\, m = 0,..,M.
\end{equation}
The eigenvectors can be ordered columnwise in a matrix,
\begin{equation}
{\bf Y}_{n2} = \begin{bmatrix}\hat {\bf y}^1 & \hat{\bf y}^2 & ... & \hat{\bf y}^{M-1}\end{bmatrix},
\end{equation}
so that, from the spectral theorem, a decomposition of the second-difference matrix is obtained as
\begin{equation}\label{eq:DecompD2n2}
{\bf D}^{2}_{n2} = -{\bf Y}_{n2} \, {\bf \Lambda}_{n2} \, {\bf Y}^{-1}_{n2},
\end{equation}
where ${{\bf \Lambda}_{n2}}$ is a diagonal matrix, whose diagonal elements $\lambda_p$ are equal to $\frac{4}{h^2}\sin^2 \left( \frac{\gamma_p h}{2}\right)$. Here, 
\begin{equation}\label{eq:BndLambdaD2n2}
0 \leq \lambda_p \leq 4/h^2.
\end{equation}
Note that now, since ${\bf D}^2_{n2}$ is \emph{not} symmetric, the matrix ${\bf Y}_{n2}$ is not orthogonal. 







\section{An explicit finite difference scheme}

As a working finite difference scheme discretising \eqref{eq:WE}, consider
\begin{equation}\label{eq:WEFDexp}
\dtt {\bf y}^n = c^2 {\bf D}^2 {\bf y}^n.
\end{equation}
Here, we are not bothered with the particular form of the second-difference matrix. It may be stem from application of either Neumann or Dirichlet boundary conditions, so the subscript is left blank. 
% We are only assuming that the second difference matrix can be decomposed as 
% \begin{equation}\label{eq:D2decomp}
% {\bf D}^2 = {\bf D}^+ \,\, {\bf D}^-, \,\, \text{ with }\,\, {\bf D}^+ = -({\bf D}^-)^\intercal
% \end{equation} 
% just like in \eqref{eq:SecondDiffMat} and \eqref{eq:SecondDiffMatNeumann}. 
Expanding out the second time difference in \eqref{eq:WEFDexp}, one gets
\begin{equation}
{\bf y}^{n+1} = \left(2{\bf I}+c^2k^2{\bf D}^2 \right){\bf y}^n - {\bf y}^{n-1},
\end{equation}
giving an explicit update. While simple, the properties of this scheme are still unknown, particularly with respect to stability and accuracy. Just like the choice of the system's parameters poses an upper bound on the choice of the time step in the case of the harmonic oscillator, here the choice of the time step and the grid spacing is limited by a stability condition. 


\subsection{Stability via Energy Analysis}
 
An idea of stability from distributed systems is encapsulated in the continuous energy balance given in Section \ref{sec:BoundWECnt}. A similar idea can be employed in the discrete case. First, write
\begin{equation}
{\bf D}^2 = -{\bf Y} \, {\bf \Lambda} \, {\bf Y}^{-1},
\end{equation}
for a positive-semidefinite diagonal matrix ${\bf \Lambda}$, which holds true for the Dirichlet, first- and second-order Neumann conditions, as in \eqref{eq:DecompD2d}, \eqref{eq:DecompD2n1}, \eqref{eq:DecompD2n2}. Then, multiply \eqref{eq:WEFDexp} on the left by ${\bf Y}^{-1}$, to get
\begin{equation}
\dtt {\bf q}^n = -c^2 {\bf \Lambda} {\bf q}^n, \,\, \text{where }{\bf q}^n = {\bf Y}^{-1}{\bf y}^n.
\end{equation}
Multiplying the equation above on the left by $\dtd {\bf q}^\intercal$, and using the usual identities, one gets
\begin{equation}
\dtp {\mathfrak h}^{n-1/2} = 0,
\end{equation}
where
\begin{equation}\label{eq:EnWaveEqnFD}
{\mathfrak h}^{n-1/2} = \frac{(\dtm {\bf q}^n)^\intercal \dtm {\bf q}^n}{2} + \frac{c^2(\etm {\bf q}^n)^\intercal {\bf \Lambda} {\bf q}^n}{2}.
\end{equation}
This expresses an energy balance in terms of the modified state variable $\bf q$. While conserved, the discrete energy is not necessarily positive, because the potential term is of indefinite sign. It may be useful to re-write the discrete energy as a quadratic form for the vector ${\bf p} = \frac{1}{\sqrt{2}}[({\bf q}^n)^\intercal, ({\bf q}^{n-1})^\intercal]^\intercal$. This gives
\begin{equation}
{\mathfrak h}^{n-1/2} = {\bf p}^\intercal \begin{bmatrix}\frac{\bf I}{k^2} & -\frac{\bf I}{k^2}+\frac{c^2}{2}{\bf \Lambda} \\ -\frac{\bf I}{k^2}+\frac{c^2}{2}{\bf \Lambda} & \frac{\bf I}{k^2} \end{bmatrix}{\bf p}.
\end{equation}
Positive-definiteness may be established after inspection of the Schur complement, that is, the matrix is positive definite if and only if its Schur complement is:
\begin{equation}
\frac{\bf I}{k^2} - k^2 \left( -\frac{\bf I}{k^2}+\frac{c^2}{2}{\bf \Lambda}\right)^2 = -\frac{c^2k^2}{2}{\bf \Lambda}\left(-\frac{2{\bf I}}{k^2} + \frac{c^2}{2}{\bf \Lambda} \right) \geq 0.
\end{equation}
Now, since we established that ${\bf \Lambda}$ is positive definite, with largest eigenvalue $4/h^2$, the condition above results in establishing when $-\frac{2{\bf I}}{k^2} + \frac{c^2}{2}{\bf \Lambda}\leq 0$, that is:
\begin{equation}\label{eq:CFL}
h\geq ck.
\end{equation}
The condition  above takes the name of \emph{Courant-Friedrichs-Lewy}, or CFL, condition. It is a stability condition for scheme  \eqref{eq:WEFDexp}. In practice, given a wave velocity $c$ and a time step $k$, the grid spacing cannot be chosen to be smaller than bound \eqref{eq:CFL}.



While \eqref{eq:EnWaveEqnFD} expresses a discrete energy balance, it is not immediate to see how this is related to the continuous energy balance \eqref{eq:EnBalFull}. This difficulty is only apparent: a natural discretisation of \eqref{eq:EnBalFull} is already encoded in \eqref{eq:EnWaveEqnFD}, when the physical state ${\bf y}^n$ is substituted back in. First, start with the Dirichlet and non-centred Neumann conditions. For these, one has ${\bf Y}^\intercal = {\bf Y}^{-1}$, and thus \eqref{eq:EnWaveEqnFD} can be expressed as
\begin{equation}
{\mathfrak h}^{n-1/2} = \frac{(\dtm {\bf y}^n)^\intercal \dtm {\bf y}^n}{2} + \frac{c^2(\etm {\bf {\bf D}^-{\bf y}}^n)^\intercal \, {\bf D}^-{\bf y}^n}{2},
\end{equation}
where the fact that ${\bf D}^2 = {\bf D}^+{\bf D}^-$ was used, as per \eqref{eq:SecondDiffMat} and \eqref{eq:SecondDiffMatNeumann}. For centered conditions, however, the second-difference matrix is not symmetric, and ${\bf Y}$ is not orthogonal, meaning that \eqref{eq:EnWaveEqnFD} cannot be directly converted to a discrete counterpart of \eqref{eq:EnBalFull}. To solve this issue, one may employ the linear transformation \eqref{eq:LinTransfn2} on the wave equation \eqref{eq:WEFDexp}, to get 
\begin{equation}
\dtt {\bf T}\,{\bf y}^n = c^2 {\bf T}\,{\bf D}^2 \,{\bf y}^n,
\end{equation}
Remember that now the matrix ${\bf T}\,{\bf D}^2$ is symmetric, and can be decomposed as ${\bf D}^2 = {\bf D}^+{\bf D}^-$. Multiplying the equation above on the left by ${\bf y}^\intercal$ gives the energy
\begin{equation}
{\mathfrak h}^{n-1/2} = \frac{(\dtm {\bf y}^n)^\intercal {\bf T} \dtm {\bf y}^n}{2} + \frac{c^2(\etm {\bf {\bf D}^-{\bf y}}^n)^\intercal \, {\bf D}^-{\bf y}^n}{2},
\end{equation}
and of course the stability analysis of this modified energy gives the same CFL condition as \eqref{eq:CFL}.



\subsection{Accuracy}


The question of accuracy for scheme \eqref{eq:WEFDexp} is an interesting one. First, it may be useful to derive the \emph{numerical dispersion relation}. Remember that, for the continuous system, the dispersion relation is given by \eqref{eq:DispRelWECnt}, in which the temporal frequencies are proportional to the wavenumbers, with constant of proportionality given by the wave speed $c$. Now, re-write scheme \eqref{eq:WEFDexp} in operator form, and transform in the frequency domain:
\begin{equation}\label{eq:WEfdOperator}
\dtt y^n_m = c^2 \dss y^n_m \implies -\frac{4}{k^2}\sin^2 \left( \frac{\omega k}{2}\right) = -\frac{4 c^2}{h^2}\sin^2 \left( \frac{\gamma h}{2}\right).{}
\end{equation}
When the stability condition \eqref{eq:CFL} is satisfied with equality, the numerical dispersion relation above gives
\begin{equation}\label{eq:NumDispRelWE}
\omega = c \gamma,
\end{equation}
that is, the same as the continuous system! In practice, the system is dispersionless. This is certainly too strong a statement, since the dispersion relation above was obtained in the case of an infinite grid, without boundaries. For all practical applications, the grid is finite and the wavenumbers and frequencies are quantised. It is the accuracy of these quantised frequencies and wavenumbers that must be checked. From the previous section, we know that the continuous boundary conditions can be discretised in a number of ways, leading to different expressions for the quantised wavenumbers. First, consider the numerical Dirichlet conditions. The wavenumbers are given by \eqref{eq:waveNrD2d}. When one chooses $h$ to be exactly equal to $L/M$, then the numerical wavenumbers are the same as the continuous ones, up to $m=M-1$, and hence the numerical frequencies are obtained from \eqref{eq:NumDispRelWE}, as
\begin{equation}
\omega_p = \frac{pc\pi}{Mh}, \,\, p = 1,...,M-1,
\end{equation}
and these are the same as the eigenfrequencies of the continuous system, \eqref{eq:omeM}. Note that, of course, the eigenvectors of the ${\bf D}^2_{d}$ matrix, given in \eqref{eq:EigenVecsD2d} are a sampled version of the eigenvectors of the continuous system. So, for Dirchlet conditions, satisfying the stability condition \eqref{eq:CFL} and using the matrix ${\bf D}^2_{d}$ given in \eqref{eq:D2Diri} does indeed result in an exact scheme in the frequency domain! In the time domain, things are slightly more complicated, due to the discretisation of the initial conditions, discussed below. This is nonetheless a powerful result, and indeed one rarely found in other systems. 

For Neumann conditions, the first-order accurate discretisation ${\bf D}^2_{n1}$ gives the quantised wavenumbers \eqref{eq:EigenvaluesD2n1}. When one chooses $h = L/M$, the quantised wavenumbers are not exactly the same as the continuous one, so here an approximation is introduced. However, the eigenvalues of the matrix ${\bf D}^2_{n2}$, in \eqref{eq:EigenvaluesD2n2}, are again exact, and the eigenvectors are sampled versions of the continuous eigenvectors.


The order of accuracy of scheme \eqref{eq:WEFDexp} may as well be approached in terms of the Taylor series of its operators, given in \eqref{eq:WEfdOperator}. We want to get the local truncation error of the scheme. To that end, assume now that $y(t,x)$ is in fact the true solution of the continuous wave equation \eqref{eq:WE}. Then, apply the difference operators to $y(t,x)$. This gives the definition of the local truncation error (LTE), as follows:
\begin{equation}
\tau^n_m \triangleq (\dtt - c^2 \dss)y(t=kn,x=mh).
\end{equation}
Taylor-expanding the operators, one gets
\begin{equation}
\left(\frac{\partial^2}{\partial t^2} - c^2 \frac{\partial^2}{\partial x^2} + \frac{1}{12}\left(k^2 \frac{\partial^4}{\partial t^4} - c^2 h^2 \frac{\partial^4}{\partial x^4}\right) + \frac{1}{360}\left(k^4 \frac{\partial^6}{\partial t^6} - c^2 h^4 \frac{\partial^6}{\partial x^6}\right) + ... \right)y(t,x) = \tau^n_m.
\end{equation}
When the stability condition \eqref{eq:CFL} is satisfied with equality, each term in the expansion contains the factor $\frac{\partial^2}{\partial t^2} - c^2 \frac{\partial^2}{\partial x^2}$, so that a factorisation of the above results as
\begin{equation}
\left(1 + \frac{k^2}{12}\left(\frac{\partial^2}{\partial t^2} + c^2 \frac{\partial^2}{\partial x^2}\right) + \frac{k^4}{360}\left( \frac{\partial^4}{\partial t^4} + c^4 \frac{\partial^4}{\partial x^4} +c^2 \frac{\partial^4}{\partial x^2 \partial t^2} \right) + ... \right)\left(\frac{\partial^2}{\partial t^2} - c^2 \frac{\partial^2}{\partial x^2}\right) y(t,x) = \tau^n_m.
\end{equation}
But since $y(t,x)$ solves \eqref{eq:WE}, then one gets $\tau^n_m=0$, that is, the LTE is zero!  


% The global error $E^n_m$ is defined as


\subsection{Initialisation. Global error. }


The LTE computed above is exactly zero when $m$ is an inner point in the domain, and $n \geq 2$ is some time step away from the initial time steps. The global error of the finite difference scheme is defined as
\begin{equation}\label{eq:ErrWaveEqn}
E^n_m = y(kn,mh)-y^n_m,
\end{equation}
that is, the difference between the exact solution computed at the time $t=kn$, $x=mh$, and the output of the difference scheme at the corresponding time step and grid point. Clearly, the global error includes the errors made in approximating the boundary and initial conditions. The former, as seen, can be approximated exactly via the matrices ${\bf D}^2_{d}$, ${\bf D}^2_{n2}$. As for the latter, a suitable discretisation of the continuous initial conditions \eqref{eq:ICs} must be given, for some smooth initial displacement $y_0(x)$ and velocity $v_0(x)$. As per usual, one may set
\begin{equation}\label{eq:y0m}
y^0_m = y_0(x=mh),
\end{equation}
that is, one may simply sample the continuous initial shape to set the value of the grid function ${\bf y}^0$. The grid function ${\bf y}^1$ is obtained approximating the initial condition on the velocity. In analogy with \eqref{eq:HigherOrderICsOscillator}, obtained for the oscillator, one may higher-order initial conditions may be obtained here by expanding out the second time difference operator $\dtt$. 
\begin{subequations}\label{eq:HigherOrderICsWE}
\begin{align}
\dtp y^0_m &= v_0(x=mh) \quad \text{first order} \label{eq:HigherOrderICsWE1}\\
\left(\dtp -\frac{k}{2}\dtt \right) y^0_m &= v_0(x=mh) \quad \text{second order} \label{eq:HigherOrderICsWE2}\\
\left(\dtp -\frac{k}{2}\dtt - \frac{k^2}{6}\dtp \dtt \right) y^0_m &= v_0(x=mh) \quad \text{third order}\label{eq:HigherOrderICsWE3}\\
\left(\dtp -\frac{k}{2}\dtt - \frac{k^2}{6}\dtp \dtt - \frac{k^3}{24}\dtt^2 \right) y^0_m &= v_0(x=mh) \quad \text{fourth order}\label{eq:HigherOrderICsWE4}
\end{align}
\end{subequations}
In the expressions above, substituting $\dtt y^0_m = c^2 \dss y^0_m$ gives a way to compute $y^1_m$, knowing $y^0_m$ and $v_0$. While this idea is simple in theory, things turn out to be a little more complicated than this. To figure out what is going on, it may be useful in fact to plot the error curves as a function of $k$, the time step. In order to do so, we are going to compute the numerical error of the scheme against the exact solution. The initial conditions are given as follows
\begin{equation}
y_0(x)  = 
\left\{ 
\begin{array}{ll}
1 - \cos \left(\frac{2\pi x}{L/2}\right)& \text{if }0\leq x \leq L/2, \\
0 & \text{elsewhere,}
\end{array}
\right.
\end{equation}
and $v_0(x)=0$. Under such intial conditions, an exact solution is given by the D'Alembert soluition \eqref{eq:Tr}. Boundary conditions of fixed type are chosen, so that the wave reflects with a change of sign at the boundary. In practice, a discrete counterpart of \eqref{eq:Tr} is needed, in the form of a digital waveguide. For that, divide the length of the string in $M$ subintervals. Define $h=L/M$, $k = h/c$. 
Define the left-travelling wave front as $l_{m}^n$, and the right-travelling wave front as $r^n_m$. For $n=1$, one has
\begin{equation}
l^1_m = r^1_m = \frac{y^0_m}{2},\quad m=1,...,M-1.
\end{equation}
 where $y^0_m$ is defined in \eqref{eq:y0m}. Then, at the timestep $n\geq 2$, the travelling wavefronts are updated as follows
\begin{equation}
l^n_m = l^{n-1}_{m+1}  \,\, (m < M-1), \quad
l^n_{M-1} = r^{n-2}_{M-1}, \quad
r^n_m = r^{n-1}_{m-1}, \,\, (m > 1), \quad
r^n_1 = l^{n-2}_{1}.
\end{equation}
This discretises the wave equation exactly. Then the ouput of the finite difference scheme, with initial conditions as per \eqref{eq:HigherOrderICsWE}, is then compared against the output of the exact solution, and the error \eqref{eq:ErrWaveEqn} is stored for various timesteps $k$. It is convenient to use an even number $M$ of subintervals, and to check the output of the wave equation at $m=M/2 +1$ (the central point of the grid). The reference sample is \texttt{Ts=floor(te/k)}, where $t_e$ is the duration in seconds of the simulation. The results are presented in Fig. \ref{fig:ErrorCurvesWE}. 
\begin{figure}[hbt]
\centering
\includegraphics[width=\linewidth]{Figures/ErrorWEeqn.eps}
\caption{Error curves for scheme \eqref{eq:WEFDexp}, under various initial conditions. (a): \eqref{eq:HigherOrderICsWE1}; (b): \eqref{eq:HigherOrderICsWE2}; (c): \eqref{eq:HigherOrderICsWE3}; (d): \eqref{eq:HigherOrderICsWE4}. The error is computed as per \eqref{eq:ErrWaveEqn}, where $m = M/2 + 1$ (M is always chosen to be even); and $n = \text{floor}(0.0165/k)$. Then, $h=L/M$, and $k = h/c$, where $L=1$, and $c=315$.}\label{fig:ErrorCurvesWE}
\end{figure}
The error plots are revealing: for \eqref{eq:HigherOrderICsWE1} and \eqref{eq:HigherOrderICsWE3}, the error curves attain the expected trends, but for \eqref{eq:HigherOrderICsWE2} and \eqref{eq:HigherOrderICsWE4}, they do not! In fact, \eqref{eq:HigherOrderICsWE2} reveals that the error is down to machine accuracy: in other words, the initialisation is \emph{exact} in this case. This is understood by a simple arguments: it is immediate to verify that, under the condition that $h=ck$, \eqref{eq:HigherOrderICsWE2} gives $y^1_m = \frac{y^0_{m+1}+y^0_{m-1}}{2}$, that is, the discrete D'Alembert solution. This holds true for inner points, but, as we know from the previous section, the boundary conditions are also discretised exactly in this case via the matrix ${\bf D}^2_d$, and hence the scheme overall is exact! 

The interpretation of Fig. \ref{fig:ErrorCurvesWE}(d) is a little more obscure. Note that, in order to implement the corresponding initial condition \eqref{eq:HigherOrderICsWE4}, one needs to apply the discrete difference $\dss$ twice or, equivalently, form a matrix ${\bf D}^4_d$. Here, the choice was to use the product of ${\bf D}^2_d$ with itself: ${\bf D}^4_d = {\bf D}^2_d \, {\bf D}^2_d$. This is the problem: ${\bf D}^4_d$ is not of sufficiently high accuracy to increase the accuracy of the scheme overall, which hence reamins third-order. A higher-order discretisation of $\dss^2$ needs to be implemented, though this possibility will not be explored further here. 


\section{Loss and forcing}


The wave equation including loss and source terms can be given as
\begin{equation}
\frac{\partial^2 y}{\partial t^2} = c^2 \frac{\partial^2 y}{\partial x^2}-2 \sigma \frac{\partial y}{\partial t} + p(x,t).
\end{equation}
Here, $\sigma \geq 0$ (measured in s$^{-1}$) is a viscous-loss, as in \eqref{eq:WEloss}, and $p$ (measured in m/s$^2$) is a source term. For all practical purposes, one may set $p(x,t) = \frac{\eta(x)}{\rho A} f(t)$, that is, the source is separable in $x$ and $t$, with $f$ being a force signal measured in N. The energy balance for this equation, an extension of \eqref{eq:EnBalWE}, reads
\begin{equation}\label{eq:EnBalanceWEQNloss}
\frac{d}{dt}\left( \int_{0}^{L} \frac{1}{2}\left( \frac{\partial y}{\partial t} \right)^2 dx + \int_{0}^{L} \frac{c^2}{2}\left( \frac{\partial y}{\partial x} \right)^2 dx \right) = -2\sigma \int_0^L \left(\frac{\partial y}{\partial t}\right)^2 \, dx + \frac{f(t)}{\rho A}\int_0^L \eta \frac{\partial y}{\partial t}\, dx.
\end{equation}
Here, conservative boundary conditions are assumed, for instance of fixed type, so that the corresponding boundary integrals vanish in the energy balance.


\subsection{Finite difference schemes}

An approximation using finite differences is obtained immediately after defining the time-dependent vector ${\bf y}^n$, approximating the solution $y(t,x)$ on the grid. Then, take
\begin{equation}
\dtt {\bf y}^n = c^2 {\bf D}^2_d {\bf y}^n - 2\sigma \dtd {\bf y}^n + \frac{f^n}{\rho A}{\boldsymbol \eta}.
\end{equation}
Here, the grid function ${\boldsymbol \eta}$ is a discrete approximation to the continuous distribution $\eta(x)$, and $f^n$ is a time series sampling the input $f(t)$ at the current time step. Regardless of the particular discretisation for $\eta$, notice that the discrete energy balance is obtained immediately after multiplication of the left by $\dtd {\bf y}^\intercal$, giving
\begin{equation}
\dtp \left(\frac{\dtm ({\bf y}^n)^\intercal \, \dtm {\bf y}^n}{2} + \frac{c^2(\etm {\bf {\bf D}^-{\bf y}}^n)^\intercal \, {\bf D}^-{\bf y}^n}{2} \right) = -2\sigma (\dtd {\bf y}^n)^\intercal \dtd {\bf y}^n +  \frac{f^n}{\rho A}(\dtd {\bf y}^n)^\intercal {\boldsymbol \eta},
\end{equation}
that is a discrete counterpart of \eqref{eq:EnBalanceWEQNloss}. 


\subsection{Spreading and interpolation}
In many practical applications, the distribution of the load is concentrated in a small area around the loading point $x_p$, so that $\eta(x) \approx \delta(x-x_p)$. In this case, $\boldsymbol \eta$ is obtained using a spreading operator of sufficient accuracy. Spreading and interpolation are closely-related concepts, in that the former consists in yielding a grid function, starting from the knowledge of a given continuous function; the former instead produces a continuous function, starting from the sampled values of a grid function. There are many different kinds of interpolants. One such popular ones is due to Lagrange, and it uses a basis of polynomials. Suppose we want to interpolate the value of a point $x_p$, such that 
\begin{equation}
m_p = \text{floor}(x_p/h), \quad \alpha = x_p/h - m_p,
\end{equation}
In practice $0\leq m_p\leq M$ is the grid point to the left of $x_p$, and $0\leq \alpha \leq h$ represent the remainder of the flooring operation. Then, consider the following arrays ${\bf r}$ of coefficients, of length $M+1$:
\begin{equation}\label{eq:LagrangeArrays}
\begin{bmatrix}
\vdots\\
r_{mp} \\
\vdots
\end{bmatrix} = \frac{1}{h}\begin{bmatrix}
\vdots\\
1 \\
\vdots
\end{bmatrix},\quad 
\begin{bmatrix}
\vdots\\
r_{mp-1} \\
r_{mp} \\
r_{mp+1} \\
\vdots
\end{bmatrix} = \frac{1}{h}\begin{bmatrix}
\vdots\\
\frac{1-\alpha}{2} \\
0 \\
\frac{1+\alpha}{2} \\
\vdots
\end{bmatrix},\quad
\begin{bmatrix}
\vdots\\
r_{mp-1} \\
r_{mp} \\
r_{mp+1} \\
\vdots
\end{bmatrix} = \frac{1}{h}\begin{bmatrix}
\vdots\\
\frac{\alpha(\alpha-1)}{2} \\
(1+\alpha)(1-\alpha) \\
\frac{\alpha(\alpha+1)}{2} \\
\vdots
\end{bmatrix},\quad
\begin{bmatrix}
\vdots\\
r_{mp-1} \\
r_{mp} \\
r_{mp+1} \\
r_{mp+2} \\
\vdots
\end{bmatrix} = \frac{1}{h}\begin{bmatrix}
\vdots\\
-\frac{\alpha(\alpha-1)(\alpha-2)}{6} \\
\frac{(\alpha+1)(\alpha-1)(\alpha-2)}{2} \\
-\frac{\alpha(\alpha+1)(\alpha-2)}{2} \\
\frac{\alpha(\alpha+1)(\alpha-1)}{6}\\
\vdots
\end{bmatrix}
\end{equation}
\begin{figure}[hbt]
\centering
\includegraphics[width=\linewidth]{Figures/ErrorsSpreading.eps}
\caption{Errors of the discrete spreading operators \eqref{eq:LagrangeArrays}. Here, the error $E$, as per \eqref{eq:ErrSpreading}, is computed starting from the continuous function $y(x) = \sin (\pi x)$, at $x_p=0.289$, and where the grid function ${\bf y}$ contains the sampled values of $y$ at $x=mh$, where $h=1/M$. The plots are obtained using a number of values for $M$. The panels, from $(a)$ to $(d)$, include higher-order approximations to the Delta function, as given in \eqref{eq:LagrangeArrays}.}\label{fig:ErrsLagrange}
\end{figure}
Lagrange interpolation is simply given by
\begin{equation}
y(x_p) = h{\bf r}^\intercal {\bf y},
\end{equation}
where $\bf r$ is any one of the arrays given in \eqref{eq:LagrangeArrays}. Of course, the arrays yields coefficients such that the order of the approximation increases. The error is defined simply as
\begin{equation}\label{eq:ErrSpreading}
E = y(x_p)-h{\bf r}^\intercal {\bf y}.
\end{equation}
In particular, \eqref{eq:LagrangeArrays} gives approximations of order one to four. Spreading, on the other hand, is somewhat the reversed operation. Note that the equation above can be written in an equivalent form as
\begin{equation}
y(x_p) \triangleq \int_0^L y(x) \delta(x-x_p)\, dx = h{\bf r}^\intercal {\bf y},
\end{equation}
in pratice yielding an approximation to the Dirac delta function as the array $\bf r$. In Fig. \ref{fig:ErrsLagrange}, the accuracy of the spreading operator is checked in a numerical experiment, yielding the expected error trends. 







\end{document}